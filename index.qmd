---
title: "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013–2014)"
subtitle: "Capstone Report"
author:
  - "Namita Mishra"
  - "Autumn Wilcox"
advisor: "Dr. Ashraf Cohen"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: Capstone Projects in Data Science
bibliography: references.bib
self-contained: true
execute:
  warning: false
  message: false
editor:
  markdown:
    wrap: 72
---

Slides: [slides.html](slides.html){target="_blank"} (Edit `slides.qmd`.)

## Introduction

Diabetes mellitus (DM) is a major public health challenge, and identifying key risk factors—such as obesity, age, sex, and race/ethnicity—is essential for prevention and targeted intervention. Logistic regression is widely used to estimate associations between such risk factors and binary outcomes, including the presence or absence of diabetes. However, classical maximum likelihood estimation (MLE) can yield unstable results in small samples or in the presence of missing data, quasi-separation, or complete separation. Moreover, healthcare data (e.g., DNA sequences, imaging, patient-reported outcomes, electronic health records, and longitudinal health measurements) are often complex, making standard analytical approaches insufficient [@zeger2020].

Bayesian hierarchical models, implemented via Markov Chain Monte Carlo (MCMC), provide a framework that integrates prior knowledge and accounts for hierarchical data structures. These models have been successfully applied in predicting patient health status across diseases such as pneumonia, prostate cancer, and mental disorders [@zeger2020]. Compared to frequentist approaches, Bayesian inference naturally quantifies uncertainty and accommodates complex covariate structures, though it remains limited by parametric assumptions.

Recent work has extended Bayesian methods to disease diagnostics. For example, Bayesian inference has been used to evaluate diagnostic test data from the National Health and Nutrition Examination Survey (NHANES), comparing parametric and nonparametric approaches. These methods provide posterior probabilities that improve disease classification, especially where conventional dichotomous thresholds fail to capture heterogeneity in populations [@chatzimichail2023]. Similarly, Bayesian clinical reasoning models have been applied to cardiovascular and metabolic risk prediction, emulating clinician decision-making by incorporating demographic, metabolic, and conventional risk factors [@liu2013].

Bayesian regression also addresses methodological challenges such as missing data. Multiple imputation combined with Bayesian modeling has been applied in clinical research, generating robust estimates under assumptions of missing at random (MAR), missing not at random (MNAR), or missing completely at random (MCAR) [@austin2021]. These applications highlight the versatility of Bayesian approaches in healthcare, particularly when traditional models are undermined by data limitations.

The broader Bayesian literature emphasizes the importance of prior specification, model checking, and variable selection. Vande Schoot et al. highlight the role of informative, weakly informative, and diffuse priors, noting that prior elicitation can draw from experts, data-based approaches, or maximum likelihood estimates [@vandeschoot2021]. Priors not only regularize estimates but also improve performance in small samples. Tutorials demonstrate how packages such as brms and blavaan in R, combined with MCMC, allow estimation of posterior distributions and facilitate empirical Bayesian analysis [@klauenberg2015].

In meta-analytic settings, Bayesian hierarchical regression has been used to augment data with results from prior studies. This approach incorporates both exchangeable and unexchangeable predictors, enabling explicit testing of heterogeneity across studies [@deleeuw2012]. Other works extend Bayesian methods to psychology and neuroscience (e.g., EEG studies [@baldwin2017]), intuitive reasoning frameworks [@kruschke2017], and Bayesian deep learning in healthcare applications such as imaging [@abdullah2022].

**Our question:** Using NHANES 2013–2014, what is the association between BMI category, age, sex, and race/ethnicity and a diabetes-related outcome (DIQ240), and does a Bayesian approach yield more stable inference than frequentist baselines under missingness and potential separation?

## Data & Preparation

- **Source:** NHANES (CDC/NCHS) 2013–2014 [@nchs1999].
- **Files:** `BMX_H` (body measures), `DEMO_H` (demographics),
  `DIQ_H` (diabetes questionnaire).
- **Outcome:** `DIQ240` - a proxy indicator of diabetes status.
- **Predictors:**
  - `BMDBMIC` (BMI category: underweight, normal, overweight, obese).
  - `RIDAGEYR` (age, continuous).
  - `RIAGENDR` (sex: male/female).
  - `RIDRETH1` (race/ethnicity: 5-level categorical).
- **Survey Design Variables:** `WTMEC2YR`, `SDMVPSU`, `SDMVSTRA`.

```{r}
# Load packages for this report
library(tidyverse)
library(knitr)
library(survey)

# Global plotting theme and palette (colorblind-friendly)
library(ggplot2)
library(viridis)   # for viridis palettes
library(bayesplot) # for Bayesian diagnostics

theme_set(theme_minimal(base_size = 13))

# Custom viridis colors for ggplot (3 colors for EDA)
custom_colors <- viridis(3, option = "D")

# Set bayesplot scheme (must be length 1 or 6)
color_scheme_set("viridis")   # built-in colorblind-friendly scheme

# Build merged dataset if missing (uses R/data_prep.R from the repo)
if (!file.exists("data/merged_2013_2014.rds")) {
  source("R/data_prep.R")
}

# Load merged NHANES data created by R/data_prep.R
merged_data <- readRDS("data/merged_2013_2014.rds")

# Quick peek
knitr::kable(head(merged_data))
```

### Basic Exploration 

```{r}
# Safe tabulations (keep NA visible)
table(merged_data$BMDBMIC, useNA = "ifany")
table(merged_data$DIQ240,  useNA = "ifany")

# Age distribution
ggplot(merged_data, aes(x = RIDAGEYR)) +
  geom_histogram(binwidth = 5, boundary = 0, closed = "left",
                 fill = custom_colors[1], color = "white") +
  labs(title = "Age distribution (NHANES 2013–2014)",
       x = "Age (years)", y = "Count")

# BMI category counts
merged_data %>%
  mutate(BMDBMIC = factor(BMDBMIC, exclude = NULL)) %>%
  count(BMDBMIC) %>%
  ggplot(aes(x = BMDBMIC, y = n, fill = BMDBMIC)) +
  geom_col(color = "black") +
  scale_fill_viridis_d(option = "D") +
  labs(title = "Counts by BMDBMIC (BMI category code)",
       x = "BMDBMIC (code; NA common for adults)", y = "Count")

# DIQ240 outcome distribution
merged_data %>%
  mutate(DIQ240 = factor(DIQ240, exclude = NULL)) %>%
  count(DIQ240) %>%
  ggplot(aes(x = DIQ240, y = n, fill = DIQ240)) +
  geom_col(color = "black") +
  scale_fill_viridis_d(option = "D") +
  labs(title = "Outcome Distribution: DIQ240",
       x = "DIQ240 (proxy indicator of diabetes status)",
       y = "Count")
```

### Survey Design

``` {r}
# Survey design setup (weights/strata/PSU)
library(survey)
nhanes_design <- svydesign(
  id = ~SDMVPSU,
  strata = ~SDMVSTRA,
  weights = ~WTMEC2YR,
  nest = TRUE,
  data = merged_data
)

# Example: weighted mean age
svymean(~RIDAGEYR, nhanes_design, na.rm = TRUE)
```

## Methods  

We applied four complementary approaches to the NHANES 2013–2014 data:  

1. **Frequentist Logistic Regression (MLE):**  
   - Baseline model predicting diabetes (`DIQ240`) from BMI, age, sex, and race/ethnicity.  
   - Limitation: listwise deletion reduced the analytic sample to only ~14 complete cases, causing quasi-separation and unstable coefficients.  

2. **Firth Penalized Logistic Regression:**  
   - Frequentist alternative using Jeffreys prior for bias correction [@dangelo2025].  
   - Produces finite coefficients under separation, but does not yield posterior distributions. 

3. **Multiple Imputation by Chained Equations (MICE):**  
   - To avoid the severe sample reduction, we applied predictive mean matching to impute missing values, generating five complete datasets [@vanbuuren2012].  
   - This retained ≈ 9,813 observations, mitigated quasi-separation artifacts, and model fit was assessed using Hosmer–Lemeshow tests (p < 0.001).  
   
4. **Bayesian Logistic Regression:**  
   - Modeled `DIQ240` as a function of BMI, age, sex, and race/ethnicity using `brms`.  
   - Priors: weakly informative Normal(0, 2.5) for coefficients and Normal(0, 5) for intercept [@gelman2008; @gosho2025].  
   - Informative priors were also considered, e.g., for male diabetes odds [@ali2024].  
   - Models were fit with 4 chains × 2000 iterations each.  
   - Diagnostics included Rhat, effective sample size, trace plots, and posterior predictive checks.  

## Modeling (skeleton code)

```{r}
library(mice)

# Multiple imputation
vars <- c("BMDBMIC", "RIDAGEYR", "RIAGENDR", "RIDRETH1", "DIQ240")
analytic_data <- merged_data[, vars]

imputed <- mice(analytic_data, m=5, method="pmm", seed=123)
Imputed_data1 <- complete(imputed, 1)

# Frequentist models
glm_fit <- glm(DIQ240 ~ BMDBMIC + RIDAGEYR + RIAGENDR + RIDRETH1,
               data=Imputed_data1, family=binomial())

library(logistf)
firth_fit <- logistf(DIQ240 ~ BMDBMIC + RIDAGEYR + RIAGENDR + RIDRETH1,
                     data=merged_data)

# Bayesian model
library(brms)
priors <- c(
  set_prior("normal(0,2.5)", class="b"),
  set_prior("normal(0,5)", class="Intercept")
)

bayes_fit <- brm(
  DIQ240 ~ BMDBMIC + RIDAGEYR + RIAGENDR + RIDRETH1,
  data=Imputed_data1,
  family=bernoulli(link="logit"),
  prior=priors,
  chains=4, iter=2000, seed=123,
  control=list(adapt_delta=0.95)
)

# Diagnostics (placeholders)

# Posterior predictive check
pp_check(bayes_fit)

# Trace plots
plot(bayes_fit, combo = c("dens", "trace"))

# Summarize model fit
summary(bayes_fit)
```

## Results (to be completed)

- **MLE:** unstable due to quasi-separation, extreme coefficients.
- **Firth regression:** finite coefficients but based on only 14 complete cases.
- **MICE + MLR:** stable results across ≈ 9,813 observations; Hosmer-Lemeshow test suggested misfit (p < 0.001).
- **Bayesian model:** stable posterior estimates, interpretable odds ratios with credible intervals, and uncertainty fully captured. 

### Comparing Models  

- **MLE:** collapsed under listwise deletion, yielding only 14 complete cases with extreme, unstable coefficients.  
- **Firth Regression:** produced finite coefficients under separation, but was still limited by the tiny complete-case sample size.  
- **MICE + MLR:** restored the full N ≈ 9,813; improved stability, but Hosmer–Lemeshow tests suggested misfit (p < 0.001).  
- **Bayesian Logistic Regression:** incorporated prior information, produced stable posterior estimates with interpretable odds ratios and credible intervals, and handled quasi-separation more effectively.  

**Takeaway:** Frequentist methods either collapsed (MLE) or were limited (Firth), while MICE improved sample retention but not fit. The Bayesian model provided the most robust framework, with uncertainty fully captured in the posterior distributions.

Formal goodness-of-fit comparisons (e.g., Hosmer-Lemeshow for MLR, posterior predictive checks for Bayesian models) will be reported in the final analysis. 

## Discussion & Conclusion

- MICE imputation plus Bayesian logistic regression provides more reliable inference than MLE or Firth regression.
- Weakly-informative priors regularize coefficients, and informative priors (Ali, 2024) help contextualize results.
- The Bayesian approach improves transparency and robustness under quasi-separation and missingness.
- Limitations: subjectivity in priors, computational intensity, and reliance on proxy outcome (`DIQ240`).


## References
