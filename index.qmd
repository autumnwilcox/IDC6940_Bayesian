---
title: "Bayesian Logistic Regression for Predicting Diabetes Risk Using NHANES 2013–2014 Data"
subtitle: "A Capstone Project on Bayesian Applications in Epidemiologic Modeling"
author:
  - "Namita Mishra"
  - "Autumn Wilcox"
advisor: "Dr. Ashraf Cohen"
date: '`r Sys.Date()`'
course: Capstone Projects in Data Science

execute:
  echo: false       
  warning: false
  message: false
  cache: false
  
format:
  html:
    code-fold: true 

bibliography: references.bib
reference-section-title: "References"
link-citations: true
self-contained: true

editor:
  markdown:
    wrap: 72
---

```{r}
#| label: Libraries
#| include: false

# Install
need <- c(
  "nhanesA","dplyr","readr","DataExplorer","forcats","survey",
  "mice","brms","posterior","broom","ggplot2","stringr","tidyr","knitr",
  "bayesplot","tibble","reshape2","loo"   # <- add loo here
)
for (p in need) if (!requireNamespace(p, quietly = TRUE)) install.packages(p)

# Project Libraries
library(dplyr); library(readr); library(DataExplorer); library(forcats)
library(survey); library(mice); library(brms); library(posterior); library(broom); library(ggplot2); library(stringr); library(tidyr); library(knitr); library(bayesplot); library(tibble); library(reshape2)
library(loo); library(glue)  

# Set cmdstanr backend

options(brms.backend = "cmdstanr")

# Confirm CmdStanR is working
if (requireNamespace("cmdstanr", quietly = TRUE)) {
  try(cmdstanr::cmdstan_version(), silent = TRUE)
}
```

Slides: [slides.html](slides.html){target="_blank"} (Edit `slides.qmd`.)

# Introduction

Diabetes mellitus (DM) remains a major public health challenge, and
identifying key risk factors—such as obesity, age, sex, and
race/ethnicity—is essential for prevention and targeted intervention.
Logistic regression is widely used to estimate associations between such
factors and binary outcomes like diabetes diagnosis. However, classical
maximum likelihood estimation (MLE) can produce unstable estimates in
the presence of missing data, quasi-separation, or small samples.
Bayesian logistic regression offers a robust alternative by integrating
prior information, regularizing estimates, and quantifying uncertainty
more transparently than frequentist approaches.

Bayesian hierarchical models, implemented via Markov Chain Monte Carlo
(MCMC), have been successfully applied in predicting patient health
status across diseases such as pneumonia, prostate cancer, and mental
disorders [@zeger2020]. By representing predictive uncertainty alongside
point estimates, Bayesian inference offers a practical advantage in
epidemiologic modeling where decisions hinge on probabilistic
thresholds. Beyond stability, Bayesian methods support model checking,
variable selection, and uncertainty quantification under missingness or
imputation frameworks [@baldwin2017; @kruschke2017].

Recent work has expanded Bayesian applications to disease diagnostics
and health risk modeling. For instance, Bayesian approaches have been
used to evaluate NHANES diagnostic data [@chatzimichail2023], to model
cardiovascular and metabolic risk [@liu2013], and to integrate multiple
data modalities such as imaging and laboratory measures
[@abdullah2022bdlhealth]. Moreover, multiple imputation combined with
Bayesian modeling generates robust estimates when data are missing at
random (MAR) or not at random (MNAR) [@austin2021].

The broader Bayesian literature emphasizes the role of priors and model
checking. Weakly informative priors, such as $N(0, 2.5)$ for
coefficients, regularize estimation and reduce variance in small samples
[@gelman2008; @vandeschoot2021]. Tutorials using R packages like `brms`
and `blavaan` illustrate how MCMC enables posterior inference and
empirical Bayes analysis [@klauenberg2015].

Beyond standard generalized linear models, Bayesian nonparametric
regression flexibly captures nonlinearity and zero inflation common in
health data [@richardson2018bnr]. Bayesian Additive Regression Trees
(BART) improve variable selection in mixed-type data [@luo2024bartvs],
while state-space and dynamic Bayesian models incorporate time-varying
biomarkers for longitudinal prediction [@momeni2021covidbayes]. Bayesian
model averaging (BMA) further addresses model uncertainty by weighting
across multiple specifications [@hoeting1999bma]. Together, these
approaches demonstrate the versatility and growing importance of
Bayesian inference in clinical and epidemiologic modeling.

The objective of this project is to evaluate whether Bayesian inference
provides more stable and interpretable estimates of diabetes risk than
frequentist and imputation-based approaches, particularly when data
complexity or separation challenges arise. Agreement across modeling
frameworks supports the robustness of these associations and highlights
the interpretability and uncertainty quantification advantages offered
by Bayesian analysis in population health modeling [@nchs2014].

## Aims

The present study employs Bayesian logistic regression to predict
diabetes status and examine the relationships between diabetes and key
predictors, including body mass index (BMI), age (≥20 years), sex,
and race. Using retrospective data from the 2013–2014 NHANES survey, the
analysis accounts for the study’s complex sampling design, which
involves stratification, clustering, and the oversampling of specific
subpopulations rather than simple random sampling. The Bayesian
framework is applied to address common analytical challenges such as
missing data, complete case bias, and data separation, thereby improving
the robustness and reliability of inference compared to traditional
logistic regression methods.

# Method

## Bayesian Logistic Regression

The Bayesian framework integrates prior knowledge with observed data to
generate posterior distributions, allowing parameters to be interpreted
directly in probabilistic terms.

Unlike traditional frequentist approaches that yield single-point
estimates and p-values, Bayesian methods represent parameters as random
variables with full probability distributions.

This provides greater flexibility, incorporates parameter uncertainty,
and produces credible intervals that directly quantify the probability
that a parameter lies within a given range.

## Model Structure

Bayesian logistic regression models the log-odds of a binary outcome as
a linear combination of predictors:

$$
\text{logit}(P(Y = 1)) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_k X_k
$$

where

-   $P(Y = 1)$ is the probability of the event of interest,
-   $\beta_0$ is the intercept (log-odds when all predictors are zero),
    and
-   $\beta_j$ represents the effect of predictor $X_j$ on the log-odds
    of the outcome, holding other predictors constant.

In the Bayesian framework, model parameters ($\boldsymbol{\beta}$) are
treated as random variables and assigned prior distributions that
reflect existing knowledge or plausible ranges before observing the
data. After incorporating the observed evidence, the priors are updated
through Bayes’ theorem [@deleeuw2012; @klauenberg2015]:

$$
\text{Posterior} \propto \text{Likelihood} \times \text{Prior}
$$

-   **Likelihood:** represents the probability of the observed data
    given the model parameters—it captures how well different parameter
    values explain the data.
-   **Prior:** expresses beliefs or existing information about the
    parameters before observing the data.
-   **Posterior:** combines both, representing the updated distribution
    of parameter values after accounting for the data.

This formulation allows uncertainty to propagate naturally through the
model, producing posterior distributions for each coefficient that can
be directly interpreted as probabilities.

## Prior Specification

Weakly informative priors were used to regularize estimation without
imposing strong assumptions:

-   **Regression coefficients:** $N(0, 2.5)$, providing gentle
    regularization while allowing substantial variation in plausible
    effects [@gelman2008; @vandeschoot2021].
-   **Intercept:** Student’s t-distribution prior, $t(3, 0, 10)$
    [@vandeschoot2013; @vandeschoot2021], which has
    -   3 degrees of freedom (heavy tails to allow occasional large
        effects),
    -   mean 0 (no bias toward positive or negative effects), and
    -   scale 10 (broad range of possible values).

Such priors help stabilize estimation in the presence of
multicollinearity, limited sample size, or potential outliers.

## Advantages of Bayesian Logistic Regression

-   **Uncertainty quantification:** Produces full posterior
    distributions instead of single estimates.
-   **Credible intervals:** Provide the range within which a parameter
    lies with a specified probability (e.g., 95%).
-   **Flexible priors:** Allow integration of expert knowledge or
    findings from prior studies.
-   **Probabilistic predictions:** Posterior predictive distributions
    yield direct probabilities for new or future observations.
-   **Model evaluation:** Posterior predictive checks (PPCs) assess how
    well simulated outcomes reproduce observed data.

## Posterior Predictions

Posterior distributions of regression coefficients were used to estimate
the probability of the outcome for given predictor values. This allows
statements such as:

| Given the predictors, the probability of the outcome lies between X% and Y%.

Posterior predictions account for two key sources of uncertainty:

1.  **Parameter uncertainty:** Variability in estimated model
    coefficients.
2.  **Predictive uncertainty:** Variability in possible future outcomes
    given those parameters.

In Bayesian analysis, all unknown quantities—coefficients, means,
variances, or probabilities—are treated as random variables described by
their posterior distributions.

## Model Evaluation and Diagnostics

Model quality and convergence were assessed using standard Bayesian
diagnostics:

-   **Posterior sampling:** Conducted via Markov Chain Monte
    Carlo (MCMC) using the No-U-Turn Sampler (NUTS), a variant of
    Hamiltonian Monte Carlo (HMC) [@austin2021]. Four chains were run
    with sufficient warm-up iterations to ensure convergence.
-   **Convergence metrics:** The potential scale reduction factor
    ($\hat{R}$) and effective sample size (ESS) were used to verify
    stability and mixing across chains.
-   **Autocorrelation checks:** Ensured independence between successive
    draws.
-   **Posterior predictive checks (PPCs):** Compared simulated outcomes
    to observed data to evaluate fit.
-   **Bayesian** $R^2$: Quantified the proportion of variance explained
    by predictors, incorporating posterior uncertainty.

# Analysis and Results

## Data Preparation

This study used publicly available 2013–2014 NHANES data published by
the CDC’s National Center for Health Statistics [@nchs2014]. Three
component files were utilized: `DEMO_H` (demographics), `BMX_H` (body
measures), and `DIQ_H` (diabetes questionnaire). Each file was imported
in `.XPT` format using the **`haven`** package in **R**, and merged
using the unique participant identifier `SEQN` to create a single adult
analytic dataset (age ≥ 20 years).

All variables were coerced to consistent numeric or factor types prior
to merging to ensure atomic columns suitable for survey-weighted
analysis and modeling. The use of `SEQN` preserved respondent integrity
across datasets and ensured accurate record linkage. This preprocessing
step standardized variable formats and minimized inconsistencies between
files.

Data wrangling, cleaning, and merging were performed in **R** using a
combination of base functions and tidyverse packages. Bayesian logistic
regression modeling was later implemented using the **`brms`** interface
to **Stan**, allowing probabilistic inference within a reproducible
workflow that accommodated the NHANES complex survey design and missing
data considerations.

### Data Import and Merging

```{r}
#| label: load-merged
#| echo: true
#| message: false
#| warning: false

merged_data <- readRDS("data/merged_2013_2014.rds")

merged_n <- nrow(merged_data)
```

The merged dataset contains `r format(merged_n, big.mark = ",")`
participants. It integrates demographic, examination, and diabetes
questionnaire data. We then restrict the sample to adults (age ≥ 20) to
define the analytic cohort used in subsequent analyses. A small
proportion of records have missing values in BMI and diabetes status,
which will be addressed later through multiple imputation.

```{r}
# A compact preview with ONLY analysis variables (no design vars here)
merged_preview <- merged_data %>%
  transmute(
    RIDAGEYR,           # age (raw; will become age / age_c later)
    BMXBMI,             # BMI  (raw; will become bmi / bmi_c later)
    RIAGENDR,           # sex (source)
    RIDRETH1,           # race (source)
    DIQ010              # diabetes indicator (source)
  )

knitr::kable(
  head(merged_preview, 10),
  caption = "Preview of merged NHANES 2013–2014 dataset limited to analysis variables (source columns only)."
)
```

### Variable Definitions

-   **Response Variable:**\
    `diabetes_ind` (binary) represents a Type 2 diabetes diagnosis,
    excluding gestational diabetes. It was derived from `DIQ010`
    (“Doctor told you have diabetes”), while `DIQ050` (insulin use) was
    excluded to prevent treatment-related confounding.

-   **Predictor Variables:**

    -   `BMXBMI`   – Body Mass Index (categorical; six levels as `bmi_cat`)\
    -   `RIDAGEYR` – Age (continuous, 20–80 years)\
    -   `RIAGENDR` – Sex (factor, two levels)\
    -   `RIDRETH1` – Ethnicity (factor, five levels)

```{r}
#| label: variables-table
#| echo: true
#| message: false
#| warning: false

library(knitr)
library(kableExtra)
library(dplyr)
library(tibble)

# -----------------------------
# Variable descriptions (with `code` formatting for names)
# -----------------------------
var_tbl <- tribble(
  ~Variable,      ~Description,                                                                                                   ~Type,         ~Origin,
  "`diabetes_ind`","Type 2 diabetes diagnosis (1 = Yes, 0 = No) derived from `DIQ010`; gestational diabetes excluded.",           "Categorical", "Derived from `DIQ010`",
  "`age`",        "Age in years.",                                                                                                "Continuous",  "NHANES `RIDAGEYR`",
  "`bmi`",        "Body Mass Index (kg/m^2) computed from measured height and weight.",                                           "Continuous",  "NHANES `BMXBMI`",
  "`bmi_cat`",    "BMI categories: Underweight, Normal, Overweight, Obesity I–III (`Normal` is reference in models).",            "Categorical", "Derived from `bmi`",
  "`sex`",        "Sex of participant (`Male`, `Female`).",                                                                       "Categorical", "NHANES `RIAGENDR`",
  "`race`",       "Race/Ethnicity with five levels (e.g., Non-Hispanic White, Non-Hispanic Black, Mexican American).",            "Categorical", "NHANES `RIDRETH1`",
  "`WTMEC2YR`",   "Examination sample weight for Mobile Examination Center participants.",                                        "Weight",      "NHANES design",
  "`SDMVPSU`",    "Primary Sampling Unit used for variance estimation in the complex survey design.",                             "Design",      "NHANES design",
  "`SDMVSTRA`",   "Stratum identifier used to define strata for the complex survey design.",                                      "Design",      "NHANES design",
  "`age_c`",      "Centered and standardized age (z-score).",                                                                     "Continuous",  "Derived from `age`",
  "`bmi_c`",      "Centered and standardized BMI (z-score).",                                                                     "Continuous",  "Derived from `bmi`"
)

kbl(
  var_tbl,
  caption = "Variable Descriptions: Adult Analytic Dataset",
  align = c("l","l","l","l"),
  escape = FALSE
) %>%
  kable_styling(full_width = FALSE, position = "center", bootstrap_options = c("striped","hover")) %>%
  group_rows("Analysis variables", 1, 6) %>%              # <-- updated range (now 6 analysis rows)
  group_rows("Survey design variables", 7, 9) %>%
  group_rows("Derived variables", 10, 11)
```

### Study Design and Survey-Weighted Analysis  

The National Health and Nutrition Examination Survey (NHANES) employs a complex, multistage probability sampling design with stratification, clustering, and oversampling of specific demographic groups (for example, racial/ethnic minorities and older adults) to produce nationally representative estimates of the U.S. population.  

Survey design variables — primary sampling units (`SDMVPSU`), strata (`SDMVSTRA`), and examination sample weights (`WTMEC2YR`) — were retained to account for this complex design. These variables were applied to adjust for unequal probabilities of selection, nonresponse, and oversampling, ensuring valid standard errors, unbiased prevalence estimates, and generalizable population-level inference.  

A survey-weighted logistic regression model was used to evaluate the association between diabetes status (`diabetes_ind`, binary outcome) and key predictors: body mass index (`bmi`), age (`age`), sex (`sex`), and race/ethnicity (`race`). Diabetes was defined using `DIQ010` (“Doctor told you have diabetes”) and coded as 0/1, with `DIQ050` (insulin use) excluded to avoid treatment-related confounding.  

Covariates included:  
- `age` (continuous; centered as `age_c`, categorized 20–80 years)  
- `bmi` (continuous; centered as `bmi_c`, and categorized by BMI class `bmi_cat`)  
- `sex` (male, female)  
- `race` (five ethnicity levels)  

This approach accounts for NHANES’ complex sampling design, producing unbiased parameter estimates and valid inference for U.S. adults.  

| Step | Description |
|------|-------------|
| **Weighting** | Used the **`survey`** package to calculate weighted means and standard deviations for all variables. |
| **Standardization** | Centered and standardized BMI and age (`bmi_c`, `age_c`) for use in regression models. |
| **Age Categorization** | Recoded into intervals: 20–<30, 30–<40, 40–<50, 50–<60, 60–<70, and 70–80 years. |
| **BMI Categorization** | Recoded as: <18.5 (Underweight), 18.5–<25 (Normal), 25–<30 (Overweight), 30–<35 (Obesity I), 35–<40 (Obesity II), ≥40 (Obesity III). |
| **Ethnicity Recoding** | Recoded as: 1 = Mexican American, 2 = Other Hispanic, 3 = Non-Hispanic White, 4 = Non-Hispanic Black, 5 = Other/Multi. |
| **Special Codes** | Transformed nonresponse codes (e.g., 3, 7) to `NA`. These missing codes were evaluated for potential nonrandom patterns (MAR/MNAR). |
| **Missing Data** | Retained and visualized missing values to assess their pattern and informativeness before multiple imputation. |
| **Final Dataset** | Created the cleaned analytic dataset (`adult`) using *Non-Hispanic White* and *Male* as reference groups for modeling. |

```{r}
#| label: adult-eda
#| echo: true
#| include: false
#| message: false
#| warning: false
# Keep NAs (no droplevels, no NA filtering); make NA an explicit level for factors
adult_eda <- merged_data %>%
  dplyr::filter(RIDAGEYR >= 20) %>%
  dplyr::transmute(
    SDMVPSU, SDMVSTRA, WTMEC2YR,
    diabetes_ind = dplyr::case_when(DIQ010 == 1 ~ 1,
                                    DIQ010 == 2 ~ 0,
                                    TRUE ~ NA_real_),
    bmi  = suppressWarnings(as.numeric(BMXBMI)),
    age  = suppressWarnings(as.numeric(RIDAGEYR)),
    sex  = dplyr::case_when(RIAGENDR == 1 ~ "Male",
                            RIAGENDR == 2 ~ "Female",
                            TRUE ~ NA_character_),
    race = dplyr::case_when(
      RIDRETH1 == 1 ~ "Mexican American",
      RIDRETH1 == 2 ~ "Other Hispanic",
      RIDRETH1 == 3 ~ "NH White",
      RIDRETH1 == 4 ~ "NH Black",
      RIDRETH1 == 5 ~ "Other/Multi",
      TRUE ~ NA_character_
    )
  ) %>%
  dplyr::mutate(
    age_c = as.numeric(scale(age)),
    bmi_c = as.numeric(scale(bmi)),

    # --- NEW: BMI category like Namita ---
    bmi_cat = cut(
      bmi,
      breaks = c(-Inf, 18.5, 25, 30, 35, 40, Inf),
      labels = c("Underweight","Normal","Overweight","Obesity I","Obesity II","Obesity III"),
      right = FALSE
    ),
    bmi_cat = forcats::fct_explicit_na(bmi_cat, na_level = "(Missing)") |>
              forcats::fct_relevel("Normal","Underweight","Overweight","Obesity I","Obesity II","Obesity III","(Missing)"),
    # --- END NEW ---

    race4 = forcats::fct_collapse(
              factor(race),
              White    = "NH White",
              Black    = "NH Black",
              Hispanic = c("Mexican American","Other Hispanic"),
              Other    = "Other/Multi"
            ) |>
            forcats::fct_relevel("White") |>
            forcats::fct_explicit_na(na_level = "(Missing)"),
    sex   = factor(sex, levels = c("Male","Female")) |>
            forcats::fct_explicit_na(na_level = "(Missing)")
  )
```

### Adult Cohort Definition 

```{r}
#| label: adult-cohort
#| echo: true
#| include: false
#| message: false
#| warning: false

# Define adult analytic cohort (age >= 20)
adult <- merged_data %>%
  dplyr::filter(RIDAGEYR >= 20) %>%
  dplyr::transmute(
    SDMVPSU, SDMVSTRA, WTMEC2YR,
    diabetes_ind = dplyr::case_when(DIQ010 == 1 ~ 1, DIQ010 == 2 ~ 0, TRUE ~ NA_real_),
    bmi = suppressWarnings(as.numeric(BMXBMI)),
    age = suppressWarnings(as.numeric(RIDAGEYR)),
    sex = factor(dplyr::case_when(
      RIAGENDR == 1 ~ "Male",
      RIAGENDR == 2 ~ "Female"
    )),
    race = factor(dplyr::case_when(
      RIDRETH1 == 1 ~ "Mexican American",
      RIDRETH1 == 2 ~ "Other Hispanic",
      RIDRETH1 == 3 ~ "NH White",
      RIDRETH1 == 4 ~ "NH Black",
      RIDRETH1 == 5 ~ "Other/Multi"
    ))
  ) %>%
  dplyr::mutate(
    # centered/scaled numeric versions
    age_c = as.numeric(scale(age)),
    bmi_c = as.numeric(scale(bmi)),

    # NEW: BMI categories like Namita
    bmi_cat = cut(
      bmi,
      breaks = c(-Inf, 18.5, 25, 30, 35, 40, Inf),
      labels = c("Underweight","Normal","Overweight","Obesity I","Obesity II","Obesity III"),
      right = FALSE
    ),
    bmi_cat = forcats::fct_explicit_na(bmi_cat, na_level = "(Missing)") |>
              forcats::fct_relevel("Normal","Underweight","Overweight","Obesity I","Obesity II","Obesity III","(Missing)"),

    race4 = forcats::fct_collapse(
      race,
      White = "NH White",
      Black = "NH Black",
      Hispanic = c("Mexican American", "Other Hispanic"),
      Other = "Other/Multi"
    ) |> forcats::fct_relevel("White")
  ) %>%
  droplevels()
```

```{r}
#| label: survey-design-adult
#| echo: true
#| message: false
#| warning: false

library(survey)

# NHANES survey design object for the adult analytic cohort

nhanes_design_adult <- survey::svydesign(
id      = ~SDMVPSU,
strata  = ~SDMVSTRA,
weights = ~WTMEC2YR,
nest    = TRUE,
data    = adult
)

# Quick weighted checks

survey::svymean(~age, nhanes_design_adult, na.rm = TRUE)
survey::svymean(~diabetes_ind, nhanes_design_adult, na.rm = TRUE)

# Design effect and effective sample size for `diabetes_ind`

v_hat <- as.numeric(survey::svyvar(~diabetes_ind, nhanes_design_adult, na.rm = TRUE))
p_hat <- mean(adult$diabetes_ind, na.rm = TRUE)
n_obs <- nrow(adult)
v_srs <- p_hat * (1 - p_hat) / n_obs
deff  <- v_hat / v_srs

n_total <- sum(weights(nhanes_design_adult), na.rm = TRUE)
ess     <- as.numeric(n_total / deff)

cat("Design effect for diabetes_ind:", round(deff, 2), "\n")
cat("Effective sample size for diabetes_ind:", round(ess), "\n")
```

Descriptive statistics for continuous and categorical variables are
presented below.

```{r}
#| label: tbl1-analytic
#| echo: false
#| message: false
#| warning: false

# Keep only analytic variables for Table 1
tbl1_dat <- adult %>%
  transmute(
    age,
    bmi,
    bmi_cat,
    sex,
    race4,
    # CHANGE: make diabetes_ind a factor so it plays nice with pivot_longer()
    diabetes_ind = factor(diabetes_ind, levels = c(0, 1), labels = c("No", "Yes"))
  )

# Continuous summaries: n, missing, mean, sd, min, max
cont_vars <- c("age", "bmi")

cont_sum <- tbl1_dat %>%
  select(all_of(cont_vars)) %>%
  pivot_longer(everything(), names_to = "Variable", values_to = "value") %>%
  group_by(Variable) %>%
  summarise(
    N          = sum(!is.na(value)),
    Missing    = sum(is.na(value)),
    Mean       = mean(value, na.rm = TRUE),
    SD         = sd(value, na.rm = TRUE),
    Min        = min(value, na.rm = TRUE),
    Max        = max(value, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  mutate(across(c(Mean, SD, Min, Max), ~round(.x, 2)))

# Categorical summaries: counts and percents
cat_vars <- c("sex", "race4", "diabetes_ind", "bmi_cat")

cat_sum <- tbl1_dat %>%
  # CHANGE: ensure all cat vars are factors and include NA as an explicit "(Missing)" level
  mutate(across(all_of(cat_vars),
                ~ forcats::fct_explicit_na(as.factor(.x), na_level = "(Missing)"))) %>%
  select(all_of(cat_vars)) %>%
  pivot_longer(everything(), names_to = "Variable", values_to = "Level") %>%
  count(Variable, Level, name = "n") %>%
  group_by(Variable) %>%
  mutate(pct = round(100 * n / sum(n), 1)) %>%
  ungroup() %>%
  arrange(Variable, desc(n))

kable(cont_sum, caption = "Table 1a. Continuous variables (age, BMI): N, missing, mean (SD), range.")
kable(cat_sum,  caption = "Table 1b. Categorical variables (sex, race4, diabetes_ind, bmi_cat): counts and percentages.")
```

Table 1a and 1b summarize the analytic variables included in subsequent
models. Mean age and BMI values indicate an adult cohort spanning a wide
range of body composition, while categorical summaries confirm balanced
sex representation and sufficient sample sizes across race/ethnicity
categories. These variables were standardized and used as predictors in
all modeling frameworks.

```{r}
#| label: adult-n
#| echo: true
#| include: true
adult_n <- nrow(adult)
```

```{r}
#| label: tbl-adult
#| tbl-cap: !expr sprintf("Excerpt of the NHANES 2013–2014 adult cohort (age ≥ 20; N = %s) with derived and standardized variables.", format(adult_n, big.mark = ","))
knitr::kable(head(adult))
```

As shown in @tbl-adult, the analytic adult cohort (N =
`r format(adult_n, big.mark=",")`) includes standardized variables for
age and BMI (`age_c`, `bmi_c`), categorical indicators for sex and
race/ethnicity (`race4`), and a binary doctor-diagnosed diabetes
variable (`diabetes_ind`).

```{r}
#| label: structure-and-preview
#| echo: true
#| message: false
#| warning: false

# Textual structure and preview
str(adult)
head(adult, 10)

# Visual structure and type overview
plot_intro(adult, title = "Adult dataset: variable types and completeness")
plot_str(adult, type = "r")
```

```{r}
#| label: missingness-visual
#| echo: true
#| message: false
#| warning: false
#| fig-cap: "Missingness overview for the adult dataset."

library(DataExplorer)

# Visualize missing data pattern for key variables

plot_missing(adult, title = "Missing data pattern (Adult dataset)")
```

```{r}
#| label: missing-summary
#| echo: true
#| message: false
#| warning: false

miss_tbl <- tibble::tibble(
Variable = c("`bmi`","`diabetes_ind`"),
Missing_n = c(sum(is.na(adult_eda$bmi)),
sum(is.na(adult_eda$diabetes_ind))),
Missing_pct = round(c(mean(is.na(adult_eda$bmi)),
mean(is.na(adult_eda$diabetes_ind))) * 100, 1)
)

knitr::kable(
miss_tbl,
caption = "Missingness for key analysis variables."
)
```

### Missing Data Summary  

Missingness was low overall (~`r round(mean(is.na(adult_eda$bmi) | is.na(adult_eda$diabetes_ind)) * 100, 1)`%). Gaps were concentrated in `bmi` and `diabetes_ind`; design variables and demographics were complete. Patterns were consistent with MAR, so we used multiple imputation (MICE) prior to modeling.

```{r}
#| label: missing-checks
#| echo: true
#| include: true
#| message: false
#| warning: false

# Summarize missingness by variable

colSums(is.na(adult_eda))

# Focus on expected variables with NAs

sapply(adult_eda[, c("diabetes_ind", "bmi", "bmi_c", "age")], function(x) sum(is.na(x)))

# Check for missing or invalid source codes

table(merged_data$DIQ010, useNA = "ifany")
sum(is.na(merged_data$BMXBMI))
```

```{r}
#| label: miss-pcts
#| echo: true
#| include: false
miss_bmi  <- mean(is.na(adult_eda$bmi)) * 100
miss_diab <- mean(is.na(adult_eda$diabetes_ind)) * 100
sprintf("Missing BMI: %.1f%%; Missing diabetes_ind: %.1f%%", miss_bmi, miss_diab)
```

```{r}
#| label: fig-missing-note
#| echo: true
#| include: true
# (Note: full missingness pattern already visualized above using plot_missing(adult).)
```

Results indicated that missing values were limited primarily to BMI and diabetes indicators, while demographic and survey design variables were complete.

The overall missingness (~4%) was low and appears consistent with data missing at random (MAR).
This pattern likely reflects participation differences in the physical examination component among older adults or individuals with health limitations.

Given these findings, multiple imputation using chained equations (MICE) was employed to minimize bias and retain statistical power in subsequent modeling.

### Exploratory Data Summary

Following the missing data assessment, exploratory analyses were conducted to describe the adult analytic cohort and visualize distributions across key demographic and health variables. The goal was to examine univariate patterns and bivariate relationships relevant to diabetes prevalence before modeling.  

The adult analytic cohort was broadly representative of the U.S. population, with a majority identifying as `Non-Hispanic White`. Age and `BMI` distributions were right-skewed, with most participants classified as overweight or obese. Visual exploration revealed a clear positive relationship between `age`, `BMI`, and diabetes prevalence. `Non-Hispanic Black` and `Hispanic` participants exhibited higher proportions of diabetes compared to `Non-Hispanic Whites`.  

Approximately 25% of variables were categorical (for example, `sex`, `race4`, `diabetes_ind`) and 75% were continuous (`age`, `bmi`, `age_c`, `bmi_c`), indicating that the dataset primarily consisted of measured numeric values such as `BMI` and `age`. About 93% of rows contained complete information across all predictors and outcomes, reflecting high data quality.  

`Age` was relatively evenly distributed across adult age groups, while `BMI` was concentrated in the overweight and obese ranges. `Female` participants were slightly overrepresented relative to `Male` participants.  

```{r}
#| label: eda-histograms-bars
#| echo: true

# Age distribution (analytic adult)
ggplot(adult, aes(x = age)) +
  geom_histogram(binwidth = 5, color = "white") +
  labs(title = "Distribution of Age (≥20 years)", x = "Age (years)", y = "Count") +
  theme_minimal()

# Diabetes outcome distribution
ggplot(adult, aes(x = factor(diabetes_ind, levels = c(0,1), labels = c("No","Yes")))) +
  geom_bar() +
  labs(title = "Diabetes Outcome Distribution (≥20 years)", x = "Diabetes (No/Yes)", y = "Count") +
  theme_minimal()

# BMI category distribution
ggplot(adult, aes(x = bmi_cat)) +
  geom_bar(color = "white", fill = "skyblue") +
  labs(title = "Distribution of BMI Categories (≥20 years)", x = "BMI Category", y = "Count") +
  theme_minimal()

# BMI by diabetes outcome (boxplot)
# (You can’t use boxplot with categorical y, so revert to numeric BMI here)
ggplot(adult, aes(x = factor(diabetes_ind, levels = c(0,1), labels = c("No","Yes")), y = bmi)) +
  geom_boxplot(fill = "lightblue") +
  labs(title = "BMI by Diabetes Diagnosis (≥20 years)", x = "Diabetes (No/Yes)", y = "BMI (numeric)") +
  theme_minimal()

# Diabetes by race4 (dodged bars)
ggplot(adult, aes(x = race4, fill = factor(diabetes_ind, levels = c(0,1), labels = c("No","Yes")))) +
  geom_bar(position = "dodge") +
  labs(title = "Diabetes Diagnosis by Race/Ethnicity (≥20 years)",
       x = "Race/Ethnicity (race4)", y = "Count", fill = "Diabetes") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

The EDA missingness summary shows approximately
`r sprintf("%.1f", miss_bmi)`% missing BMI and
`r sprintf("%.1f", miss_diab)`% missing diabetes status
(`diabetes_ind`). All design variables (`WTMEC2YR`, `SDMVPSU`,
`SDMVSTRA`), as well as `age`, `sex`, and `race4`, are complete—sex and
race NAs are encoded as explicit “(Missing)” levels in the EDA view.

```{r}
#| label: data-exploration
#| include: true
# data exploration

if (sum(!is.na(adult$diabetes_ind)) == 0) {
  stop("Too few non-missing outcomes for modeling (n = 0). Check DIQ010 upstream.")
}

# (optional plots omitted for brevity)

# save for downstream
if (!dir.exists("data")) dir.create("data", recursive = TRUE)
saveRDS(adult, "data/adult_cleaned_2013_2014.rds")
```
    
## Modeling Frameworks

Three modeling frameworks were compared using identical predictors
(standardized age, BMI, sex, and race4) and the binary outcome
`diabetes_ind`: (1) survey-weighted logistic regression to incorporate
the NHANES complex sampling design, (2) multiple imputation (MICE) to
address missing BMI values, and (3) Bayesian logistic regression with
weakly informative priors to quantify uncertainty.

### Survey-Weighted Logistic Regression (Design-Based MLE)

```{r}
#| label: data-clean
#| echo: true
#| message: false
#| warning: false
#| include: true

adult_clean <- adult %>%
  dplyr::mutate(
    sex   = forcats::fct_drop(sex),
    race4 = forcats::fct_drop(race4),
    age_c = as.numeric(age_c),
    bmi_c = as.numeric(bmi_c)
  ) %>%
  dplyr::filter(!is.na(diabetes_ind), !is.na(age_c), !is.na(bmi_c)) %>%
  droplevels()

str(adult_clean[, c("diabetes_ind","sex","race4","age_c","bmi_c")])
```

```{r}
#| label: model-survey
#| echo: true
#| include: true
#| message: false
#| warning: false

options(survey.lonely.psu = "adjust")

nhanes_design_adult <- survey::svydesign(
id = ~SDMVPSU,
strata = ~SDMVSTRA,
weights = ~WTMEC2YR,
nest = TRUE,
data = adult_clean
)

des_cc <- subset(nhanes_design_adult, !is.na(diabetes_ind) &
!is.na(age_c) & !is.na(bmi_c) &
!is.na(sex) & !is.na(race4))

# Sanity checks before modeling
stopifnot(nlevels(adult_clean$sex)   >= 2)
stopifnot(nlevels(adult_clean$race4) >= 2)

table(adult_clean$sex)
table(adult_clean$race4)

svy_fit <- survey::svyglm(
diabetes_ind ~ age_c + bmi_c + sex + race4,
design = des_cc,
family = quasibinomial()
)

svy_or <- broom::tidy(svy_fit, conf.int = TRUE) %>%
dplyr::mutate(
OR = exp(estimate),
LCL = exp(conf.low),
UCL = exp(conf.high)
) %>%
dplyr::select(term, OR, LCL, UCL, p.value) %>%
dplyr::filter(term != "(Intercept)")
```

Design-based odds ratios are summarized in @tbl-survey.

```{r}
#| label: tbl-survey
#| echo: true
#| tbl-cap: "Survey-weighted logistic regression: odds ratios (OR) and 95% confidence intervals for diabetes diagnosis among adults (NHANES 2013–2014)."
knitr::kable(svy_or)
```

The NHANES 2013–2014 data use a complex, multistage probability design
involving strata (`SDMVSTRA`), primary sampling units (PSUs; `SDMVPSU`),
and examination weights (`WTMEC2YR`) to ensure nationally representative
estimates [@nchs2014].

Estimates are population-weighted using NHANES survey design variables
(`WTMEC2YR`, `SDMVSTRA`, `SDMVPSU`). Odds ratios are reported per one
standard deviation (1 SD) increase in age and BMI, with reference groups
Male and White.

### Multiple Imputation (MICE)

Multiple Imputation by Chained Equations (MICE) was used as a principled
approach for handling missing data [@vanbuuren2011; @vanbuuren2012].\
MICE iteratively imputes each incomplete variable using regression
models based on other variables in the dataset, producing multiple
completed datasets that reflect uncertainty due to missingness.
Estimates are then pooled across imputations using Rubin’s rules to
generate final parameter estimates and confidence intervals.

MICE, as an alternative to the Bayesian approach, effectively manages
missing data through chained regression equations without requiring full
joint modeling of all variables.

For large sample sizes (n ≥ 400), even in the presence of high
percentages (up to 75%) of missing data in one variable, non-normal
distributions such as flat densities, heavy tails, skewness, and
multimodality do not materially affect mean structure estimation
performance [@vanbuuren2012].

In this study, continuous variables (age and BMI) were imputed using
predictive mean matching (PMM) to preserve realistic distributions,
while categorical variables (sex and race4) were imputed using logistic
and polytomous regression models, respectively. Diabetes status
(`diabetes_ind`) was treated as an outcome variable and was not imputed.
Twenty imputations were generated to reduce Monte Carlo error and
maintain robust variance estimation.

```{r}
#| label: model-mice
#| echo: true
#| include: true
#| message: false
#| warning: false

# Build data for imputation

mi_dat <- adult %>%
dplyr::select(diabetes_ind, age, bmi, sex, race4, WTMEC2YR, SDMVPSU, SDMVSTRA)

# Specify methods and predictor matrix

meth <- mice::make.method(mi_dat)
pred <- mice::make.predictorMatrix(mi_dat)

# Outcome not imputed; used as predictor only

meth["diabetes_ind"] <- ""
pred["diabetes_ind", ] <- 0
pred[, "diabetes_ind"] <- 1

# Continuous vars: age = normal; bmi = predictive mean matching (PMM).
# Categoricals: sex = logistic; race4 = polytomous regression.

meth[c("age","bmi")]   <- c("norm","pmm")
meth[c("sex","race4")] <- c("logreg","polyreg")

# Design variables as auxiliaries only

meth[c("WTMEC2YR","SDMVPSU","SDMVSTRA")] <- ""
pred[, c("WTMEC2YR","SDMVPSU","SDMVSTRA")] <- 1

# Run imputation

imp <- mice::mice(mi_dat, m = 20, method = meth, predictorMatrix = pred, seed = 123)

# Fit logistic regression within each imputed dataset

fits <- with(
  imp,
  glm(
    diabetes_ind ~ scale(age) + scale(bmi) + relevel(sex, "Male") + relevel(race4, "White"),
    family = binomial()
  )
)

# Pool estimates across imputations

pool_mi <- mice::pool(fits)

mi_or <- summary(pool_mi, conf.int = TRUE, exponentiate = TRUE) %>%
dplyr::rename(OR = estimate, LCL = `2.5 %`, UCL = `97.5 %`) %>%
dplyr::filter(term != "(Intercept)")
```

```{r}
#| label: adult-imp1
#| echo: true
#| message: false
#| warning: false

adult_imp1 <- mice::complete(imp, 1) %>%
  dplyr::mutate(
    sex     = factor(sex,   levels = levels(adult$sex)),
    race4   = factor(race4, levels = levels(adult$race4)),
    age_c   = as.numeric(scale(age)),
    bmi_c   = as.numeric(scale(bmi)),
    wt_norm = WTMEC2YR / mean(WTMEC2YR, na.rm = TRUE)
  ) %>%
  dplyr::filter(!is.na(diabetes_ind), !is.na(age_c), !is.na(bmi_c)) %>%
  droplevels()
```

```{r}
#| label: define-bayes-formula
#| echo: true
#| cache: false

fml_bayes <- diabetes_ind | weights(wt_norm) ~ age_c + bmi_c + sex + race4

priors <- c(
  brms::set_prior("normal(0, 2.5)", class = "b"),
  brms::set_prior("student_t(3, 0, 10)", class = "Intercept")
)
```

```{r}
#| label: fit-bayes
#| echo: true
#| message: false
#| warning: false
#| cache: false

invisible(capture.output({
  bayes_fit <- brms::brm(
    formula = fml_bayes,
    data    = adult_imp1,
    family  = bernoulli(link = "logit"),
    prior   = priors,
    chains  = 4, iter = 2000, seed = 123,
    control = list(adapt_delta = 0.95),
    refresh = 0
  )
}))

bayes_or <- brms::posterior_summary(bayes_fit, pars = "^b_") %>%
  as.data.frame() %>%
  tibble::rownames_to_column("raw") %>%
  dplyr::mutate(
    term = gsub("^b_", "", raw),
    OR   = exp(Estimate),
    LCL  = exp(Q2.5),
    UCL  = exp(Q97.5)
  ) %>%
  dplyr::select(term, OR, LCL, UCL)
```

```{r}
#| label: tbl-bayes
#| echo: true
#| tbl-cap: "Bayesian logistic regression: posterior odds ratios (OR) with 95% credible intervals."
#| cache: false

knitr::kable(
  dplyr::mutate(bayes_or, dplyr::across(c(OR, LCL, UCL), ~ round(.x, 2)))
)
```

Multiple imputation preserves sample size and reduces bias from missing
BMI values. Results closely mirror the survey-weighted model, confirming
robustness to imputation.

```{r}
#| label: tbl-mice
#| echo: true 

miss_age  <- sum(is.na(mi_dat$age))
miss_bmiN <- sum(is.na(mi_dat$bmi))

mi_caption <- if (miss_age > 0 && miss_bmiN > 0) {
  "Multiple Imputation (MICE): pooled odds ratios (OR) and 95% confidence intervals after imputing missing age (normal) and BMI (PMM) (m = 20); diabetes status was not imputed."
} else if (miss_bmiN > 0) {
  "Multiple Imputation (MICE): pooled odds ratios (OR) and 95% confidence intervals after imputing missing BMI (PMM) (m = 20); diabetes status was not imputed."
} else if (miss_age > 0) {
  "Multiple Imputation (MICE): pooled odds ratios (OR) and 95% confidence intervals after imputing missing age (normal) (m = 20); diabetes status was not imputed."
} else {
  "Multiple Imputation (MICE): pooled odds ratios (OR) and 95% confidence intervals (no variables required imputation); diabetes status was not imputed."
}
mi_caption <- paste0(mi_caption, " Odds ratios are per 1 SD for age and BMI.")

knitr::kable(mi_or, caption = mi_caption)
```

### Bayesian Logistic Regression

Bayesian logistic regression was implemented using the following model
specification:

**Formula:**\
`diabetes_ind | weights(wt_norm) ~ age_c + bmi_c + sex + race4`

```{r}
#| label: corr-heatmap-adult-imp1
#| echo: true
#| cache: false

stopifnot(all(c("diabetes_ind","age","bmi") %in% names(adult_imp1)))

correlation_matrix <- cor(
  adult_imp1[, c("diabetes_ind", "age", "bmi")],
  use = "complete.obs",
  method = "pearson"
)
correlation_melted <- melt(correlation_matrix, varnames = c("Var1","Var2"), value.name = "value")

ggplot(correlation_melted, aes(Var1, Var2, fill = value)) +
  geom_tile(color = "white") +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white",
                       midpoint = 0, limits = c(-1, 1), name = "Correlation") +
  theme_minimal() +
  labs(title = "Correlation Heatmap (adult_imp1)", x = "Features", y = "Features")
```

```{r}
#| label: fit-bayes-priors
#| echo: true
#| cache: false

bayes_fit_prior <- tryCatch(
  update(bayes_fit, sample_prior = "yes", refresh = 0),
  error = function(e) NULL
)
```

```{r}
#| label: fit-with-priors
#| echo: true
#| cache: false
#| include: false

if (exists("bayes_fit")) {
bayes_fit_prior <- tryCatch(
update(bayes_fit, sample_prior = "yes", refresh = 0),
error = function(e) bayes_fit
)
} else {
bayes_fit_prior <- NULL
}
```

Posterior odds ratios and credible intervals from the Bayesian logistic
regression are shown in @tbl-bayes.

```{r}
#| label: build-bullets
#| echo: true
#| include: false
#| cache: false

pull_or <- function(df, term) {
  row <- df %>% filter(term == !!term)
  if (nrow(row) == 0) return(list(or=NA, lcl=NA, ucl=NA))
  list(or = row$OR[1], lcl = row$LCL[1], ucl = row$UCL[1])
}
fmt <- function(x) sprintf("%.2f", x)
excludes1 <- function(lcl,ucl) ifelse(!is.na(lcl) && !is.na(ucl) && (lcl>1 | ucl<1), "CrI excludes 1", "CrI overlaps 1")

age  <- pull_or(bayes_or, "age_c")
bmi  <- pull_or(bayes_or, "bmi_c")
fem  <- pull_or(bayes_or, "sexFemale")
blk  <- pull_or(bayes_or, "race4Black")
his  <- pull_or(bayes_or, "race4Hispanic")
oth  <- pull_or(bayes_or, "race4Other")

post <- as.data.frame(brms::as_draws_df(bayes_fit))
b0   <- mean(post$b_Intercept)
baseline_p <- plogis(b0)
baseline_txt <- sprintf("%.1f%%", 100*baseline_p)

bullets <- c(
  "### Population-level interpretation (posterior, odds ratios)",
  " - **Convergence.** All R-hat ≈ 1.00; large ESS → excellent mixing.",   # <- no LaTeX, no escaping issues
  glue(" - **Baseline risk.** Male, White, mean age/BMI: **~{baseline_txt}** predicted diabetes prevalence."),
  glue(" - **Age.** +1 SD → **{fmt(age$or)}×** (95% CrI {fmt(age$lcl)}–{fmt(age$ucl)}; {excludes1(age$lcl,age$ucl)})."),
  glue(" - **BMI.** +1 SD → **{fmt(bmi$or)}×** (95% CrI {fmt(bmi$lcl)}–{fmt(bmi$ucl)}; {excludes1(bmi$lcl,bmi$ucl)})."),
  glue(" - **Female vs. Male.** **{fmt(fem$or)}×** (95% CrI {fmt(fem$lcl)}–{fmt(fem$ucl)}; {excludes1(fem$lcl,fem$ucl)})."),
  glue(" - **Black vs. White.** **{fmt(blk$or)}×** (95% CrI {fmt(blk$lcl)}–{fmt(blk$ucl)}; {excludes1(blk$lcl,blk$ucl)})."),
  glue(" - **Hispanic vs. White.** **{fmt(his$or)}×** (95% CrI {fmt(his$lcl)}–{fmt(his$ucl)}; {excludes1(his$lcl,his$ucl)})."),
  glue(" - **Other/Multi vs. White.** **{fmt(oth$or)}×** (95% CrI {fmt(oth$lcl)}–{fmt(oth$ucl)}; {excludes1(oth$lcl,oth$ucl)}).")
)
```

As shown in @tbl-bayes, the Bayesian logistic regression model estimated
the log-odds of diabetes using standardized predictors. Weakly
informative priors ($N(0, 2.5)$ for slopes, Student-t(3, 0, 10) for the
intercept) stabilized estimation and prevented overfitting. The model
used normalized NHANES exam weights as importance weights to approximate
design-based inference. Posterior means and 95% credible intervals
provided full uncertainty quantification for each predictor.

Posterior summaries were further evaluated using the Bayesian $R^2$,
which estimates the proportion of outcome variance explained by model
predictors.

Model-level performance is summarized in @tbl-bayesR2.

```{r}
#| label: tbl-bayesR2
#| echo: true
#| tbl-cap: "Bayesian R² summary."

knitr::kable(as.data.frame(brms::bayes_R2(bayes_fit)))
```

```{r}
#| label: tbl-mcmc-diagnostics
#| tbl-cap: "MCMC diagnostics (R-hat and Effective Sample Sizes) for model parameters (including intercept)."
#| echo: true

# Get diagnostics as plain columns (use metric names as strings)
diag <- posterior::summarise_draws(
  bayes_fit,
  "rhat", "ess_bulk", "ess_tail"
)

# Keep only slope parameters (b_*) and build a clean table.
diag_b <- diag |>
  dplyr::as_tibble() |>
  dplyr::filter(grepl("^b_", .data$variable)) |>
  dplyr::transmute(
    Parameter = .data$variable,
    Rhat      = .data$rhat,
    Bulk_ESS  = .data$ess_bulk,
    Tail_ESS  = .data$ess_tail
  )

knitr::kable(diag_b, digits = 1)
```

All model parameters achieved R̂ values approximately equal to 1.00 and
bulk/tail effective sample sizes exceeding 2,000, confirming strong
convergence and well-mixed chains. The Bayesian R² was approximately
0.13, indicating that age, BMI, sex, and race collectively explained
about 13% of variability in diabetes risk at the population level.

Model comparison results using leave-one-out cross-validation are
presented below.

```{r}
#| label: model-comparison
#| tbl-cap: "Bayesian model comparison (LOO): base model vs. models without race or without sex."
#| echo: true
#| message: false
#| warning: false

# Reduced models (let brms compile if needed)
invisible(capture.output({
  fit_no_race <- update(bayes_fit, formula = update(fml_bayes, . ~ . - race4))
  fit_no_sex  <- update(bayes_fit, formula = update(fml_bayes, . ~ . - sex))
}))

# Compute LOO for all three models
loo_base    <- loo::loo(bayes_fit)
loo_no_race <- loo::loo(fit_no_race)
loo_no_sex  <- loo::loo(fit_no_sex)

# Compare (pass objects positionally)
cmp <- loo::loo_compare(loo_base, loo_no_race, loo_no_sex)

# Make row names explicit and sort best-to-worst already by loo_compare
cmp_df <- as.data.frame(cmp)
cmp_df$Model <- rownames(cmp_df)
cmp_df <- cmp_df[, c("Model", setdiff(names(cmp_df), "Model"))]

knitr::kable(
  cmp_df,
  caption = "LOO comparison (higher elpd_loo is better; table sorted best to worst)."
)
```

Leave-one-out (LOO) cross-validation showed that models excluding race
or sex had lower expected log predictive density (elpd), indicating a
poorer fit. This supports the inclusion of both variables as meaningful
contributors to predictive performance and overall model adequacy.

Figures below visualize posterior distributions, MCMC diagnostics, and
model fit.

```{r}
#| label: posterior-density-participants
#| echo: true
#| cache: false

adult_means <- adult_imp1 %>% summarise(
  age_mean = mean(age, na.rm = TRUE),
  age_sd   = sd(age, na.rm = TRUE),
  bmi_mean = mean(bmi, na.rm = TRUE),
  bmi_sd   = sd(bmi, na.rm = TRUE)
)

to_model_row <- function(age_raw, bmi_raw, sex_lab, race4_lab) {
  tibble(
    age_c  = (age_raw - adult_means$age_mean)/adult_means$age_sd,
    bmi_c  = (bmi_raw - adult_means$bmi_mean)/adult_means$bmi_sd,
    sex    = factor(sex_lab,   levels = levels(adult_imp1$sex)),
    race4  = factor(race4_lab, levels = levels(adult_imp1$race4)),
    wt_norm = 1
  )
}

plot_post_density <- function(df_row, title_txt) {
  phat <- posterior_linpred(bayes_fit, newdata = df_row, transform = TRUE)
  ci95 <- quantile(phat, c(0.025, 0.975))
  ggplot(data.frame(pred = as.numeric(phat)), aes(x = pred)) +
    geom_density() +
    geom_vline(xintercept = ci95[1], linetype = "dashed") +
    geom_vline(xintercept = ci95[2], linetype = "dashed") +
    coord_cartesian(xlim = c(0, 1)) +
    labs(x = "P(Diabetes = 1)", y = "Density", title = title_txt) +
    theme_minimal()
}

# Use participants from analytic data (adult) for realistic inputs
stopifnot(nrow(adult) >= 2)
p1 <- to_model_row(adult$age[1], adult$bmi[1],
                   as.character(adult$sex[1]), as.character(adult$race4[1]))
p2 <- to_model_row(adult$age[2], adult$bmi[2],
                   as.character(adult$sex[2]), as.character(adult$race4[2]))

plot_post_density(p1, "Participant 1: Posterior Predictive Distribution (95% CrI)")
plot_post_density(p2, "Participant 2: Posterior Predictive Distribution (95% CrI)")

# “New” participant example
new_pt <- to_model_row(age_raw = 45, bmi_raw = 32, sex_lab = "Female", race4_lab = "Hispanic")
plot_post_density(new_pt, "New Participant: Posterior Predictive Distribution (95% CrI)")
```

```{r}
#| label: fig-mcmc-areas
#| fig-cap: "Posterior distributions (95% credible mass) for slope parameters."
#| echo: true
#| cache: false

bayesplot::mcmc_areas(
as.array(bayes_fit),
regex_pars = "^b_",
prob = 0.95
)
```

```{r}
#| label: fig-mcmc-trace
#| fig-cap: "Trace plots for slope parameters (chain mixing and stationarity)."
#| echo: true
#| cache: false

bayesplot::mcmc_trace(
as.array(bayes_fit),
regex_pars = "^b_"
)
```

```{r}
#| label: fig-mcmc-acf
#| fig-cap: "Autocorrelation plots for posterior samples of age and BMI coefficients (MCMC diagnostics)."
#| echo: true
#| cache: false

post_array <- posterior::as_draws_array(bayes_fit)  # draws x chains x parameters
bayesplot::mcmc_acf(post_array, pars = c("b_age_c", "b_bmi_c"))
```

```{r}
#| label: fig-mcmc-areas-expanded
#| fig-cap: "Posterior density areas (95% credible mass) for age, BMI, sex, and race coefficients."
#| echo: true

par_names <- names(posterior::as_draws_df(bayes_fit))
want <- c("b_age_c","b_bmi_c","b_sexFemale","b_race4Black","b_race4Hispanic","b_race4Other")
have <- intersect(want, par_names)

if (length(have) == 0) {
knitr::asis_output("**No matching slope parameters found for mcmc_areas.**")
} else {
bayesplot::mcmc_areas(as.array(bayes_fit), pars = have, prob = 0.95)
}
```

```{r}
#| label: fig-ppc-bars
#| fig-cap: "Posterior predictive check: observed vs. replicated outcome distribution (bars)."
#| echo: true
#| cache: false

bayesplot::pp_check(bayes_fit, type = "bars", nsamples = 100)
```

```{r}
#| label: bayes-yobs
#| echo: true
#| cache: false

yobs <- adult_imp1$diabetes_ind
```

```{r}
#| label: fig-ppc-mean-sd
#| fig-cap: "Posterior predictive checks for mean and standard deviation of the binary outcome."
#| echo: true
#| cache: false

yrep <- brms::posterior_predict(bayes_fit, ndraws = 400)  # draws x observations

bayesplot::ppc_stat(y = yobs, yrep = yrep, stat = "mean")
bayesplot::ppc_stat(y = yobs, yrep = yrep, stat = "sd")
```

### Model Fit and Calibration

Posterior predictive checks showed that simulated outcome distributions
closely matched the observed diabetes prevalence, indicating strong
model calibration. Both the mean and standard deviation of replicated
outcomes aligned with observed data, suggesting the model adequately
captured central tendency and dispersion. These results provide
graphical evidence of good fit and reinforce that the priors did not
unduly constrain the posterior.

```{r}
#| label: fig-or-forest
#| fig-cap: "Posterior odds ratios (points) with 95% credible intervals (lines)."
#| echo: true
#| cache: false

or_plot <- bayes_or %>%
  dplyr::filter(term != "Intercept") %>%
  dplyr::mutate(
    term_clean = dplyr::case_when(
      term == "age_c"            ~ "Age (per 1 SD)",
      term == "bmi_c"            ~ "BMI (per 1 SD)",
      term == "sexFemale"        ~ "Female (vs. Male)",
      term == "sexMale"          ~ "Male (vs. Female)",
      term == "race4Black"       ~ "Black (vs. White)",
      term == "race4Hispanic"    ~ "Hispanic (vs. White)",
      term == "race4Other"       ~ "Other (vs. White)",
      TRUE                       ~ term
    )
  )

ggplot(or_plot, aes(x = OR, y = reorder(term_clean, OR))) +
  geom_vline(xintercept = 1, linetype = 2) +
  geom_point() +
  geom_errorbarh(aes(xmin = LCL, xmax = UCL), height = 0.15) +
  labs(x = "Odds ratio (logit model, posterior)", y = NULL)
```

Calibration between predicted and observed diabetes probabilities is
displayed in @fig-pred-calibration.

```{r}
#| label: fig-pred-calibration
#| fig-cap: "Observed outcome vs. mean predicted probability (calibration scatter with smoother)."
#| echo: true
#| cache: false

pred_mean <- colMeans(brms::posterior_epred(bayes_fit))
ggplot(data.frame(pred = pred_mean, obs = yobs),
       aes(x = pred, y = obs)) +
  geom_point(alpha = 0.15, position = position_jitter(height = 0.03)) +
  geom_smooth(method = "loess", se = TRUE) +
  labs(x = "Mean predicted probability", y = "Observed diabetes (0/1)")
```

```{r}
#| label: fig-posterior-prevalence
#| fig-cap: "Posterior predictive distribution of diabetes prevalence compared to observed NHANES prevalence."
#| echo: true
#| cache: false

post_pred <- brms::posterior_epred(bayes_fit, summary = FALSE)
post_prev <- rowMeans(post_pred)
obs_prev  <- mean(adult_imp1$diabetes_ind, na.rm = TRUE)
post_prev_mat <- matrix(post_prev, ncol = 1, dimnames = list(NULL, "Prevalence"))

p <- bayesplot::mcmc_dens(post_prev_mat)
p + ggplot2::labs(title = "Posterior diabetes prevalence",
                  x = "Predicted prevalence", y = NULL) +
    ggplot2::geom_vline(xintercept = obs_prev, linetype = 2)

p
```

The posterior predictive distribution of diabetes prevalence closely
mirrored the survey-estimated prevalence, with the posterior mean
aligning within 1% of the observed rate. This indicates that the
Bayesian model accurately reproduced the population-level prevalence and
supports its calibration for epidemiologic inference.

```{r}
#| label: fig-pop-vs-posterior-prev
#| fig-cap: "Population (NHANES survey-weighted) vs posterior predictive diabetes prevalence."
#| echo: true
#| cache: false

# Posterior predictive prevalence (replicated datasets)

yrep <- brms::posterior_predict(bayes_fit, ndraws = 2000)   # draws x observations (0/1)
post_prev <- rowMeans(yrep)                                 # prevalence each posterior draw

# Survey-weighted observed prevalence (population estimate)

des_obs <- survey::svydesign(
id = ~SDMVPSU, strata = ~SDMVSTRA, weights = ~WTMEC2YR,
nest = TRUE, data = adult_imp1
)
obs <- survey::svymean(~diabetes_ind, des_obs, na.rm = TRUE)
obs_prev  <- as.numeric(obs["diabetes_ind"])
obs_se    <- as.numeric(SE(obs)["diabetes_ind"])
obs_lcl   <- max(0, obs_prev - 1.96 * obs_se)
obs_ucl   <- min(1, obs_prev + 1.96 * obs_se)

# Plot: posterior density with weighted point estimate and 95% CI band

ggplot(data.frame(prev = post_prev), aes(x = prev)) +
geom_density(alpha = 0.6) +
annotate("rect", xmin = obs_lcl, xmax = obs_ucl, ymin = 0, ymax = Inf, alpha = 0.15) +
geom_vline(xintercept = obs_prev, linetype = 2) +
coord_cartesian(xlim = c(0, 1)) +
labs(x = "Diabetes prevalence", y = "Posterior density",
subtitle = sprintf("Survey-weighted NHANES prevalence = %.1f%% (95%% CI %.1f–%.1f%%)",
100*obs_prev, 100*obs_lcl, 100*obs_ucl)) +
theme_minimal()
```

```{r}
#| label: list-brms-pars
#| echo: true
#| cache: false

src_obj <- if (exists("bayes_fit_prior") && !is.null(bayes_fit_prior)) bayes_fit_prior else bayes_fit
draws_names <- names(posterior::as_draws_df(src_obj))
sort(grep("^(b_|prior_b_)", draws_names, value = TRUE))
```

```{r}
#| label: fig-prior-posterior-overlay
#| fig-cap: "Prior (dashed) vs posterior (solid) densities for selected coefficients."
#| echo: true
#| cache: false

src_obj <- if (exists("bayes_fit_prior") && !is.null(bayes_fit_prior)) bayes_fit_prior else bayes_fit
draws_df <- posterior::as_draws_df(src_obj)
all_cols <- names(draws_df)

# EDIT THESE if your names differ (check the printed list from list-brms-pars)

want_post <- c("b_age_c","b_bmi_c","b_sexFemale","b_race4Black","b_race4Hispanic","b_race4Other")

have_post  <- intersect(want_post, all_cols)
have_prior <- intersect(paste0("prior_", have_post), all_cols)

pairs <- data.frame(post = have_post, prior = paste0("prior_", have_post),
stringsAsFactors = FALSE)
pairs <- pairs[pairs$prior %in% have_prior, , drop = FALSE]

if (nrow(pairs) == 0) {
knitr::asis_output("**No matching prior/posterior parameters found to overlay.**")
} else {
post_long <- tidyr::pivot_longer(
draws_df[, pairs$post, drop = FALSE],
cols = tidyselect::everything(), names_to = "term", values_to = "estimate"
)
post_long$type <- "Posterior"

prior_tmp <- draws_df[, pairs$prior, drop = FALSE]
colnames(prior_tmp) <- pairs$post
prior_long <- tidyr::pivot_longer(
prior_tmp,
cols = tidyselect::everything(), names_to = "term", values_to = "estimate"
)
prior_long$type <- "Prior"

combined_draws <<- dplyr::bind_rows(prior_long, post_long)

lbl <- c(
b_age_c = "Age (1 SD)", b_bmi_c = "BMI (1 SD)",
b_sexFemale = "Female vs Male",
b_race4Black = "Black vs White",
b_race4Hispanic = "Hispanic vs White",
b_race4Other = "Other vs White"
)
combined_draws$term <- factor(
combined_draws$term,
levels = intersect(names(lbl), unique(combined_draws$term)),
labels = lbl[intersect(names(lbl), unique(combined_draws$term))]
)

ggplot2::ggplot(combined_draws, ggplot2::aes(x = estimate, linetype = type)) +
ggplot2::geom_density() +
ggplot2::facet_wrap(~ term, scales = "free", ncol = 2) +
ggplot2::labs(x = "Coefficient (log-odds)", y = "Density", linetype = NULL) +
ggplot2::theme_minimal()
}
```

```{r}
#| label: fig-prior-posterior-ggplot
#| fig-cap: "Prior vs Posterior Distributions (ggplot2 version)."
#| echo: true
#| cache: false

if (exists("combined_draws") && is.data.frame(combined_draws) && nrow(combined_draws) > 0) {
ggplot(combined_draws, aes(x = estimate, fill = type)) +
geom_density(alpha = 0.4) +
facet_wrap(~ term, scales = "free", ncol = 2) +
theme_minimal(base_size = 13) +
labs(
title = "Prior vs Posterior Distributions",
x = "Coefficient estimate",
y = "Density",
fill = ""
)
} else {
knitr::asis_output("**Skipped: no matching prior/posterior draws to plot.**")
}
```

```{r}
#| label: fig-mcmc-areas-selected
#| echo: true
#| cache: false

par_names <- names(posterior::as_draws_df(bayes_fit))
want <- c("b_age_c","b_bmi_c","b_sexFemale","b_race4Black","b_race4Hispanic","b_race4Other")
have <- intersect(want, par_names)

if (length(have)) {
  bayesplot::mcmc_areas(as.array(bayes_fit), pars = have, prob = 0.95)
} else {
  knitr::asis_output("**No matching slope parameters available to plot.**")
}
```

For age and BMI, the posterior densities shift notably away from the
N(0, 2.5) prior toward positive values and are narrower, indicating
strong information from the data; for sex, the posterior remains closer
to the prior with more overlap, indicating weaker evidence.

The overlay of prior and posterior densities illustrates that
informative updates occurred primarily for BMI, age, and race
coefficients, which showed distinct posterior shifts relative to the
priors. In contrast, weaker predictors such as sex displayed overlapping
distributions, indicating that inference for those parameters was more
influenced by prior uncertainty than by the observed data. This balance
confirms appropriate regularization rather than overfitting.

## Results

A concise summary of posterior estimates is provided below.

```{r}
#| label: results-bullets
#| echo: true
#| results: 'asis'

cat(paste(bullets, collapse = "\n"))
```

```{r}
#| label: results-compare
#| echo: true
#| message: false
#| warning: false

# Combine results from all three models

svy_tbl   <- if (exists("svy_or") && nrow(svy_or) > 0)
dplyr::mutate(svy_or,   Model = "Survey-weighted (MLE)") else NULL
mi_tbl    <- if (exists("mi_or") && nrow(mi_or) > 0)
dplyr::mutate(mi_or,    Model = "MICE Pooled") else NULL
bayes_tbl <- if (exists("bayes_or") && nrow(bayes_or) > 0)
dplyr::mutate(bayes_or, Model = "Bayesian") %>%
dplyr::filter(term != "Intercept") else NULL

all_tbl <- dplyr::bind_rows(Filter(Negate(is.null), list(svy_tbl, mi_tbl, bayes_tbl))) %>%
dplyr::mutate(
term = dplyr::case_when(
  grepl("^bmi", term,  ignore.case = TRUE) ~ "BMI (per 1 SD)",
  grepl("^age", term,  ignore.case = TRUE) ~ "Age (per 1 SD)",
  grepl("^sexFemale$", term)               ~ "Female (vs. Male)",
  grepl("^sexMale$", term)                 ~ "Male (vs. Female)",
  grepl("^race4Hispanic$", term)           ~ "Hispanic (vs. White)",
  grepl("^race4Black$", term)              ~ "Black (vs. White)",
  grepl("^race4Other$", term)              ~ "Other (vs. White)",
  TRUE ~ term
),
OR_CI = sprintf("%.2f (%.2f – %.2f)", OR, LCL, UCL)
) %>%
dplyr::select(Model, term, OR_CI)
```

Comparative odds ratios across frameworks are shown in @tbl-comparison.

```{r}
#| label: tbl-comparison
#| echo: true
#| tbl-cap: "Comparison of odds ratios (per 1 SD for age and BMI) and 95% intervals across survey-weighted, MICE, and Bayesian frameworks."
knitr::kable(all_tbl, align = c("l","l","c"))
```

This table summarizes results from the survey-weighted (design-based),
multiple-imputation, and Bayesian models.

The Bayesian model’s credible intervals closely align with the
frequentist confidence intervals but provide a more direct probabilistic
interpretation of uncertainty.

Across all three frameworks—survey-weighted (MLE), multiple imputation,
and Bayesian—age and BMI were consistently associated with higher odds
of doctor-diagnosed diabetes. Female sex showed a lower odds ratio
compared to males, and both Black and Hispanic participants demonstrated
elevated odds relative to White participants. The similarity of effect
sizes across frameworks underscores the robustness of these predictors
to different modeling assumptions and missing-data treatments. Bayesian
credible intervals largely overlapped frequentist confidence intervals,
confirming stability of inference while allowing direct probabilistic
interpretation.

## Discussion and Limitations

### Interpretation

The Bayesian logistic regression framework produced results that were
highly consistent with both the survey-weighted and MICE-pooled
frequentist models. Age and BMI remained the most influential predictors
of doctor-diagnosed diabetes, each showing a strong and positive
association with diabetes risk.

Unlike classical maximum likelihood estimation, the Bayesian approach
directly quantified uncertainty through posterior distributions,
offering richer interpretability and more transparent probability
statements. The alignment between Bayesian and design-based estimates
supports the robustness of these associations and highlights the
practicality of Bayesian modeling for complex, weighted population data.

Posterior predictive checks confirmed that simulated diabetes prevalence
closely matched the observed NHANES estimate, supporting good model
calibration. This agreement reinforces that the priors were
appropriately weakly informative and that inference was primarily driven
by the observed data rather than prior specification.

Overall, this study demonstrates that Bayesian inference complements
traditional epidemiologic methods by maintaining interpretability while
enhancing stability and explicitly quantifying uncertainty in complex
survey data. These consistent findings across modeling frameworks
underscore the robustness of core risk factors and support the use of
Bayesian inference for epidemiologic research involving complex survey
data.

### Limitations

While this analysis demonstrates the value of Bayesian logistic
regression for epidemiologic modeling, several limitations should be
acknowledged.

First, the use of a single imputed dataset for the Bayesian model—rather
than full joint modeling of imputation uncertainty—may understate total
variance.

Second, NHANES exam weights were normalized and treated as importance
weights, which approximate but do not fully reproduce design-based
inference.

Third, the weakly informative priors $N(0, 2.5)$ for slopes and
*Student-t(3, 0, 10)* for the intercept were not empirically tuned;
alternative prior specifications could slightly alter posterior
intervals.

Finally, although convergence diagnostics (R̂ ≈ 1, sufficient effective
sample sizes, and stable posterior predictive checks) indicated good
model performance, results are conditional on the 2013–2014 NHANES cycle
and may not generalize to later datasets or longitudinal analyses.

## Conclusion

The Bayesian, survey-weighted, and imputed logistic regression
frameworks all identified consistent predictors of diabetes risk in U.S.
adults: advancing age, higher BMI, sex (lower odds for females), and
non-White race/ethnicity.

The Bayesian model produced estimates nearly identical in direction and
magnitude to the frequentist results while providing a more
comprehensive assessment of uncertainty through posterior distributions
and credible intervals.

These consistent findings across modeling frameworks underscore the
robustness of core risk factors and support the use of Bayesian
inference for epidemiologic research involving complex survey data.

By incorporating prior information and using MCMC to sample from the
full posterior distribution, Bayesian inference enhances model
transparency and interpretability.

Its agreement with traditional approaches underscores that Bayesian
methods can be applied confidently in large-scale public health datasets
such as NHANES.

Future extensions could integrate hierarchical priors, multiple NHANES
cycles, or Bayesian model averaging to better capture population
heterogeneity, temporal trends, and evolving diabetes risk patterns.
