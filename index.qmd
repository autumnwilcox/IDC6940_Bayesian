---
title: "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013–2014)"
subtitle: "Capstone Report"
author:
  - "Namita Mishra"
  - "Autumn Wilcox"
advisor: "Dr. Ashraf Cohen"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: Capstone Projects in Data Science
bibliography: references.bib
self-contained: true
execute:
  warning: false
  message: false
editor:
  markdown:
    wrap: 72
---

Slides: [slides.html](slides.html){target="_blank"} (Edit `slides.qmd`.)

## Introduction

Diabetes mellitus (DM) is a major public health challenge, and identifying key risk factors—such as obesity, age, race, and gender—is essential for prevention and targeted intervention. Logistic regression is widely used to estimate associations between such risk factors and binary outcomes, including the presence or absence of diabetes. However, classical maximum likelihood estimation (MLE) can yield unstable results in small samples or in the presence of missing data, quasi-separation, or complete separation. Moreover, healthcare data (e.g., DNA sequences, imaging, patient-reported outcomes, electronic health records, and longitudinal health measurements) are often complex, making standard analytical approaches insufficient [@zeger2020].

Bayesian hierarchical models, implemented via Markov Chain Monte Carlo (MCMC), provide a framework that integrates prior knowledge and accounts for hierarchical data structures. These models have been successfully applied in predicting patient health status across diseases such as pneumonia, prostate cancer, and mental disorders [@zeger2020]. Compared to frequentist approaches, Bayesian inference naturally quantifies uncertainty and accommodates complex covariate structures, though it remains limited by parametric assumptions.

Recent work has extended Bayesian methods to disease diagnostics. For example, Bayesian inference has been used to evaluate diagnostic test data from the National Health and Nutrition Examination Survey (NHANES), comparing parametric and nonparametric approaches. These methods provide posterior probabilities that improve disease classification, especially where conventional dichotomous thresholds fail to capture heterogeneity in populations [@chatzimichail2023]. Similarly, Bayesian clinical reasoning models have been applied to cardiovascular risk prediction, emulating clinician decision-making by incorporating demographic, metabolic, and conventional risk factors [@liu2013].

Bayesian regression also addresses methodological challenges such as missing data. Multiple imputation combined with Bayesian modeling has been applied to clinical research settings, generating robust estimates under assumptions of missing at random (MAR), missing not at random (MNAR), or missing completely at random (MCAR) [@austin2021]. These applications highlight the versatility of Bayesian approaches in healthcare, particularly when traditional models are undermined by data limitations.

## Related Work

The broader Bayesian literature emphasizes the importance of prior specification, model checking, and variable selection. Vande Schoot et al. highlight the role of informative, weakly informative, and diffuse priors, noting that prior elicitation can draw from experts, data-based approaches, or maximum likelihood estimates [@vandeschoot2021]. Priors not only regularize estimates but also improve performance in small samples. Tutorials demonstrate how packages such as brms and blavaan in R, combined with MCMC, allow estimation of posterior distributions and facilitate empirical Bayesian analysis [@klauenberg2015].

In meta-analytic settings, Bayesian hierarchical regression has been used to augment data with results from prior studies. This approach incorporates both exchangeable and unexchangeable predictors, enabling explicit testing of heterogeneity across studies [@deleeuw2012]. Such hierarchical modeling strengthens inference in small-sample or multi-study contexts where frequentist regression falls short.

Applications extend beyond traditional regression. Baldwin and Larson illustrated Bayesian regression with EEG and anxiety data, showing how priors and posterior distributions yield richer probabilistic interpretations than frequentist results [@baldwin2017]. Kruschke and Liddell framed Bayesian reasoning as intuitive, reflecting how individuals update beliefs with new evidence [@kruschke2017]. Abdullah et al. reviewed Bayesian deep learning in healthcare, highlighting its role in uncertainty quantification for tasks such as medical imaging and disease classification [@abdullah2022].

Together, these works underscore the adaptability of Bayesian methods across domains, while also noting challenges including computational demands, subjective prior specification, and the need for careful convergence diagnostics. This literature directly motivates our project, which applies Bayesian logistic regression to NHANES survey data to explore predictors of diabetes outcomes while addressing quasi-separation, missingness, and small effective sample size.

**Our question:** Using NHANES 2013–2014, what is the association between key predictors (BMI category, age, sex, race/ethnicity) and a diabetes-related outcome, and does a Bayesian approach yield more stable inference than a frequentist baseline under missingness and potential separation?

## Data & Preparation

- **Source:** NHANES (CDC) 2013–2014.
- **Files:** `BMX_H` (body measures), `DEMO_H` (demographics),
  `DIQ_H` (diabetes questionnaire).
- **Variables:**
  - Predictors/Covariates: `BMDBMIC` (BMI category), `RIDAGEYR` (age),
    `RIAGENDR` (sex), `RIDRETH1` (race/ethnicity).
  - Survey design: `WTMEC2YR`, `SDMVPSU`, `SDMVSTRA`.
  - Outcome candidate for now: `DIQ240` (usual diabetes doctor; a
    diabetes-related marker, not a diagnosis).

```{r}
# Load packages for this report
library(tidyverse)
library(knitr)

# Build merged dataset if missing (uses R/data_prep.R from the repo)
if (!file.exists("data/merged_2013_2014.rds")) {
  source("R/data_prep.R")
}

# Load merged NHANES data created by R/data_prep.R
merged_data <- readRDS("data/merged_2013_2014.rds")

# Quick peek
knitr::kable(head(merged_data))
```

## Basic Exploration 

```{r}
# Safe tabulations (keep NA visible)
table(merged_data$BMDBMIC, useNA = "ifany")
table(merged_data$DIQ240,  useNA = "ifany")

# Age distribution
ggplot(merged_data, aes(x = RIDAGEYR)) +
  geom_histogram(binwidth = 5, boundary = 0, closed = "left") +
  labs(title = "Age distribution (NHANES 2013–2014)",
       x = "Age (years)", y = "Count") +
  theme_minimal()

# BMI category counts (codes as-is)
merged_data %>%
  mutate(BMDBMIC = factor(BMDBMIC, exclude = NULL)) %>%
  count(BMDBMIC) %>%
  ggplot(aes(x = BMDBMIC, y = n)) +
  geom_col() +
  labs(title = "Counts by BMDBMIC (BMI category code)",
       x = "BMDBMIC (code; NA common for adults)", y = "Count") +
  theme_minimal()
```

## Survey Design

``` {r}
# Survey design setup (weights/strata/PSU)
library(survey)
nhanes_design <- svydesign(
  id = ~SDMVPSU,
  strata = ~SDMVSTRA,
  weights = ~WTMEC2YR,
  nest = TRUE,
  data = merged_data
)

# Example: weighted mean age
svymean(~RIDAGEYR, nhanes_design, na.rm = TRUE)
```

## Methods

- **Baseline:** Frequentist logistic regression (MLE).
- **Main:** Bayesian logistic regression with weakly-informative priors
for stability and honest uncertainty intervals.
- **Missingness:** Prefer multiple imputation (or Bayesian models with
missing data mechanisms) over listwise deletion to retain ~9,800
observations and avoid separation artifacts. We will run prior
sensitivity checks.

## Modeling (placeholders)

```{r}
# Example skeletons (commented until outcome is finalized)

# library(rstanarm)  # or library(brms)

# Frequentist baseline:
# fit_mle <- glm(outcome ~ BMDBMIC + RIDAGEYR + RIAGENDR + RIDRETH1 + DIQ240,
#                data = merged_data, family = binomial())

# Bayesian (weakly-informative priors):
# fit_bayes <- rstanarm::stan_glm(
#   outcome ~ BMDBMIC + RIDAGEYR + RIAGENDR + RIDRETH1 + DIQ240,
#   data = merged_data, family = binomial(),
#   prior = rstanarm::normal(0, 2.5),
#   prior_intercept = rstanarm::normal(0, 5),
#   chains = 4, iter = 2000, seed = 123
# )

# Next steps:
# - finalize outcome (prefer DIQ010), run MI if needed, then fit both models
# - compare odds ratios/posteriors, AUC, calibration, and survey-weighted variants
```

## Results (to be populated)

- Posterior summaries and credible intervals for effects.
- Predictive performance vs. MLE baseline.
- Sensitivity to priors; effect of handling missingness vs. deletion.

## Discussion & Conclusion

- Bayesian logistic regression is a good fit for survey data with
missingness and potential separation, providing stable estimates and
interpretable uncertainty.
- Next: finalize the outcome (DIQ010), run MI, fit survey-aware models,
and present results with clear figures and tables.

## References
