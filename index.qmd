---
title: "Bayesian Logistic Regression for Predicting Diabetes Risk Using NHANES 2013–2014 Data"
subtitle: "A Capstone Project on Bayesian Applications in Epidemiologic Modeling"
author:
  - "Namita Mishra"
  - "Autumn Wilcox"
advisor: "Dr. Ashraf Cohen"
date: '`r Sys.Date()`'
course: Capstone Projects in Data Science

format:
  html:
    code-fold: true
    toc: true

bibliography: references.bib
reference-section-title: "References"
link-citations: true
self-contained: true

execute:
  warning: false
  message: false

editor:
  markdown:
    wrap: 72
---

```{r}
# Install
need <- c(
  "nhanesA","dplyr","readr","DataExplorer","forcats","survey",
  "mice","brms","posterior","broom","ggplot2","stringr","tidyr","knitr",
  "bayesplot","tibble","reshape2","loo"   # <- add loo here
)
for (p in need) if (!requireNamespace(p, quietly = TRUE)) install.packages(p)

# Project Libraries
library(dplyr); library(readr); library(DataExplorer); library(forcats)
library(survey); library(mice); library(brms); library(posterior); library(broom)
library(ggplot2); library(stringr); library(tidyr); library(knitr)
library(bayesplot); library(tibble); library(reshape2)
library(loo)  # <- add this

# Set cmdstanr backend

options(brms.backend = "cmdstanr")

# Optional: confirm CmdStanR is working
if (requireNamespace("cmdstanr", quietly = TRUE)) {
  try(cmdstanr::cmdstan_version(), silent = TRUE)
}
```

Slides: [slides.html](slides.html){target="_blank"} (Edit `slides.qmd`.)

# Introduction

Diabetes mellitus (DM) remains a major public health challenge, and
identifying key risk factors—such as obesity, age, sex, and
race/ethnicity—is essential for prevention and targeted intervention.
Logistic regression is widely used to estimate associations between such
factors and binary outcomes like diabetes diagnosis. However, classical
maximum likelihood estimation (MLE) can produce unstable estimates in
the presence of missing data, quasi-separation, or small samples.
Bayesian logistic regression offers a robust alternative by integrating
prior information, regularizing estimates, and quantifying uncertainty
more transparently than frequentist approaches.

This study applies Bayesian logistic regression to estimate the risk of
doctor-diagnosed diabetes among adults in the 2013–2014 National Health
and Nutrition Examination Survey (NHANES). Predictors include age, body
mass index (BMI), sex, and a coarsened race/ethnicity factor (race3)
comprising White, Black, and Hispanic groups, with low-frequency levels
combined as “Other.” Three analytic frameworks were compared: (1)
survey-weighted maximum likelihood estimation (MLE) using the NHANES
complex design, (2) multiple imputation (MICE) with predictive mean
matching and Rubin’s rules, and (3) Bayesian inference with weakly
informative priors \[N(0, 2.5)\] implemented via `brms`. The Bayesian
model incorporated normalized NHANES exam weights as importance weights,
approximating design-based inference. Across all methods, age and BMI
were positively associated with diabetes odds, female sex tended to have
lower odds than male, and Black and Hispanic adults showed higher odds
relative to White. Agreement across modeling frameworks supports the
robustness of these associations and highlights the interpretability and
uncertainty quantification advantages offered by Bayesian analysis for
population health modeling.

Bayesian hierarchical models, implemented via Markov Chain Monte Carlo
(MCMC), have been successfully applied in predicting patient health
status across diseases such as pneumonia, prostate cancer, and mental
disorders [@zeger2020]. By representing predictive uncertainty alongside
point estimates, Bayesian inference offers a practical advantage in
epidemiologic modeling where decisions hinge on probabilistic
thresholds. Beyond stability, Bayesian methods support model checking,
variable selection, and uncertainty quantification under missingness or
imputation frameworks [@baldwin2017; @kruschke2017].

Recent work has expanded Bayesian applications to disease diagnostics
and health risk modeling. For instance, Bayesian approaches have been
used to evaluate NHANES diagnostic data [@chatzimichail2023], to model
cardiovascular and metabolic risk [@liu2013], and to integrate multiple
data modalities such as imaging and laboratory measures
[@abdullah2022bdlhealth]. Moreover, multiple imputation combined with
Bayesian modeling generates robust estimates when data are missing at
random (MAR) or not at random (MNAR) [@austin2021].

The broader Bayesian literature emphasizes the role of priors and model
checking. Weakly informative priors, such as Normal(0, 2.5) for
coefficients, regularize estimation and reduce variance in small samples
[@gelman2008; @vandeschoot2021]. Tutorials using R packages like `brms`
and `blavaan` illustrate how MCMC enables posterior inference and
empirical Bayes analysis [@klauenberg2015].

Beyond standard generalized linear models, Bayesian nonparametric
regression flexibly captures nonlinearity and zero inflation common in
health data [@richardson2018bnr]. Bayesian Additive Regression Trees
(BART) improve variable selection in mixed-type data [@luo2024bartvs],
while state-space and dynamic Bayesian models incorporate time-varying
biomarkers for longitudinal prediction [@momeni2021covidbayes]. Bayesian
model averaging (BMA) further addresses model uncertainty by weighting
across multiple specifications [@hoeting1999bma]. Together, these
approaches demonstrate the versatility and growing importance of
Bayesian inference in clinical and epidemiologic modeling.

The objective of this project is to evaluate whether Bayesian inference
provides more stable and interpretable estimates of diabetes risk than
frequentist and imputation-based approaches, particularly when data
complexity or separation challenges arise. Agreement across modeling
frameworks supports the robustness of these associations and highlights
the interpretability and uncertainty quantification advantages offered
by Bayesian analysis in population health modeling [@nchs2014].

# Methods

Bayesian logistic regression extends the classical logistic model by
treating model parameters as random variables with prior probability
distributions [@deleeuw2012; @baldwin2017; @kruschke2017]. Let $Y_i$
denote a binary outcome for observation $i$ and $\mathbf{x}_i$ the
corresponding vector of predictors. The probability of the event is
modeled as

$$
p_i = \Pr(Y_i = 1 \mid \mathbf{x}_i),
\qquad
\text{logit}(p_i) = \beta_0 + \sum_{j=1}^{p} \beta_j x_{ij},
$$

where $\beta_0$ is the intercept and $\beta_j$ are regression
coefficients for predictors $x_{ij}$ [@liu2013]. The logit link
transforms probabilities to the real line, ensuring $0 < p_i < 1$.

In the Bayesian framework, prior distributions are specified for all
unknown parameters. Weakly informative priors—typically Normal$(0, 2.5)$
for regression coefficients—provide gentle regularization, constraining
extreme values without overpowering the data [@vandeschoot2021]. The
joint posterior distribution of parameters is proportional to the
product of the likelihood and the priors:

$$
p(\boldsymbol{\beta} \mid \mathbf{y}, \mathbf{X}) \propto
\mathcal{L}(\mathbf{y} \mid \mathbf{X}, \boldsymbol{\beta}) \;
p(\boldsymbol{\beta}),
$$

where $\mathcal{L}(\mathbf{y} \mid \mathbf{X}, \boldsymbol{\beta})$
denotes the binomial likelihood.

Posterior inference was performed using Markov Chain Monte Carlo (MCMC)
sampling via the No-U-Turn Sampler (NUTS), a variant of Hamiltonian
Monte Carlo (HMC). Four chains were run with sufficient warm-up
iterations to ensure convergence. Diagnostic checks included trace
plots, the potential scale-reduction factor ($\hat{R}$), and posterior
predictive checks comparing simulated and observed outcomes
[@austin2021].

Point estimates were summarized by posterior means and medians, and
uncertainty was quantified through 95% credible intervals derived from
the posterior distribution. Model adequacy was evaluated using posterior
predictive diagnostics and Bayesian $R^2$ statistics. Odds ratios (ORs)
were obtained by exponentiating the posterior samples of $\beta_j$:

$$
\text{OR}_j = \exp(\beta_j),
$$

representing the multiplicative change in the odds of the outcome
associated with a one-unit increase in the predictor. All computation
was implemented in a probabilistic programming environment using the
`brms` interface to Stan.

# Analysis and Results

## 1. Data Preparation

We analyzed NHANES 2013–2014 public-use data from the CDC’s National
Center for Health Statistics [@nchs2014]. Three component files were
merged: demographics (**DEMO_H**), body measures (**BMX_H**), and the
diabetes questionnaire (**DIQ_H**). All variables were coerced to
consistent numeric or factor types prior to merging to ensure atomic
columns suitable for survey analysis and modeling.

### 1.1 Import and Merge Datasets

```{r}
#| label: load-merged
#| echo: true
#| message: false
#| warning: false

merged_data <- readRDS("data/merged_2013_2014.rds")

# A compact preview with ONLY analysis variables (no design vars here)
merged_preview <- merged_data %>%
  transmute(
    RIDAGEYR,           # age (raw; will become age / age_c later)
    BMXBMI,             # BMI  (raw; will become bmi / bmi_c later)
    RIAGENDR,           # sex (source)
    RIDRETH1,           # race (source)
    DIQ010              # diabetes indicator (source)
  )

knitr::kable(
  head(merged_preview, 10),
  caption = "Preview of merged NHANES 2013–2014 dataset limited to analysis variables (source columns only)."
)
```

```{r}
#| label: merged-n
#| echo: false
merged_n <- nrow(merged_data)
```

> The merged dataset contains `r format(merged_n, big.mark=",")`
> participants.

The merged NHANES 2013–2014 dataset integrates demographic, examination,
and diabetes questionnaire data. We next restrict to **adults (age ≥
20)**, yielding the analytic cohort used below. Several key variables
(BMI and diabetes indicator) contain modest missingness that we address
later via multiple imputation.

### 1.2 Adult Cohort Definition

```{r}
#| label: adult-eda
#| echo: true
#| message: false
#| warning: false
# Keep NAs (no droplevels, no NA filtering); make NA an explicit level for factors
adult_eda <- merged_data %>%
  dplyr::filter(RIDAGEYR >= 20) %>%
  dplyr::transmute(
    SDMVPSU, SDMVSTRA, WTMEC2YR,
    diabetes_ind = dplyr::case_when(DIQ010 == 1 ~ 1,
                                    DIQ010 == 2 ~ 0,
                                    TRUE ~ NA_real_),
    bmi  = suppressWarnings(as.numeric(BMXBMI)),
    age  = suppressWarnings(as.numeric(RIDAGEYR)),
    sex  = dplyr::case_when(RIAGENDR == 1 ~ "Male",
                            RIAGENDR == 2 ~ "Female",
                            TRUE ~ NA_character_),
    race = dplyr::case_when(
      RIDRETH1 == 1 ~ "Mexican American",
      RIDRETH1 == 2 ~ "Other Hispanic",
      RIDRETH1 == 3 ~ "NH White",
      RIDRETH1 == 4 ~ "NH Black",
      RIDRETH1 == 5 ~ "Other/Multi",
      TRUE ~ NA_character_
    )
  ) %>%
  dplyr::mutate(
    age_c = as.numeric(scale(age)),
    bmi_c = as.numeric(scale(bmi)),
    race3 = forcats::fct_collapse(
              factor(race),
              White    = "NH White",
              Black    = "NH Black",
              Hispanic = c("Mexican American","Other Hispanic"),
              Other    = "Other/Multi"
            ) |>
            forcats::fct_relevel("White") |>
            forcats::fct_explicit_na(na_level = "(Missing)"),
    sex   = factor(sex, levels = c("Male","Female")) |>
            forcats::fct_explicit_na(na_level = "(Missing)")
  )
```

```{r}
#| label: missing-checks
#| echo: true
#| message: false
#| warning: false

# How many NAs in each column?
colSums(is.na(adult_eda))

# Focus on the key variables we expect NAs in
sapply(adult_eda[, c("diabetes_ind","bmi","bmi_c","age")], function(x) sum(is.na(x)))

# Do the source columns actually contain NAs / non-(1,2) codes?
table(merged_data$DIQ010, useNA = "ifany")
sum(is.na(merged_data$BMXBMI))
```

```{r}
#| label: miss-pcts-print
#| echo: true
miss_bmi <- mean(is.na(adult_eda$bmi)) * 100
miss_diab <- mean(is.na(adult_eda$diabetes_ind)) * 100
sprintf("Missing BMI: %.1f%%; Missing diabetes_ind: %.1f%%", miss_bmi, miss_diab)
```

```{r}
#| label: adult-cohort
#| echo: true
#| message: false
#| warning: false

# Define adult analytic cohort (age >= 20)
adult <- merged_data %>%
  filter(RIDAGEYR >= 20) %>%
  transmute(
    SDMVPSU, SDMVSTRA, WTMEC2YR,
    diabetes_ind = case_when(DIQ010 == 1 ~ 1, DIQ010 == 2 ~ 0, TRUE ~ NA_real_),
    bmi = as.numeric(BMXBMI),
    age = as.numeric(RIDAGEYR),
    sex = factor(case_when(
      RIAGENDR == 1 ~ "Male",
      RIAGENDR == 2 ~ "Female"
    )),
    race = factor(case_when(
      RIDRETH1 == 1 ~ "Mexican American",
      RIDRETH1 == 2 ~ "Other Hispanic",
      RIDRETH1 == 3 ~ "NH White",
      RIDRETH1 == 4 ~ "NH Black",
      RIDRETH1 == 5 ~ "Other/Multi"
    ))
  ) %>%
  mutate(
    age_c = scale(age),
    bmi_c = scale(bmi),
    race3 = forcats::fct_collapse(
      race,
      White = "NH White",
      Black = "NH Black",
      Hispanic = c("Mexican American", "Other Hispanic"),
      Other = "Other/Multi"
    ) |> forcats::fct_relevel("White")
  ) %>%
  droplevels()
```

```{r}
#| label: tbl1-analytic
#| echo: false
#| message: false
#| warning: false
#| tbl-cap: "Table 1. Analytic variables for the NHANES 2013–2014 adult cohort (age ≥ 20). Design variables (WTMEC2YR, SDMVPSU, SDMVSTRA) are excluded from this table."

library(dplyr)
library(tidyr)
library(knitr)

# Keep only analytic variables for Table 1
tbl1_dat <- adult %>%
  transmute(
    age,
    bmi,
    sex,
    race3,
    # CHANGE: make diabetes_ind a factor so it plays nice with pivot_longer()
    diabetes_ind = factor(diabetes_ind, levels = c(0, 1), labels = c("No", "Yes"))
  )

# Continuous summaries: n, missing, mean, sd, min, max
cont_vars <- c("age", "bmi")

cont_sum <- tbl1_dat %>%
  select(all_of(cont_vars)) %>%
  pivot_longer(everything(), names_to = "Variable", values_to = "value") %>%
  group_by(Variable) %>%
  summarise(
    N          = sum(!is.na(value)),
    Missing    = sum(is.na(value)),
    Mean       = mean(value, na.rm = TRUE),
    SD         = sd(value, na.rm = TRUE),
    Min        = min(value, na.rm = TRUE),
    Max        = max(value, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  mutate(across(c(Mean, SD, Min, Max), ~round(.x, 2)))

# Categorical summaries: counts and percents
cat_vars <- c("sex", "race3", "diabetes_ind")

cat_sum <- tbl1_dat %>%
  # CHANGE: ensure all cat vars are factors and include NA as an explicit "(Missing)" level
  mutate(across(all_of(cat_vars),
                ~ forcats::fct_explicit_na(as.factor(.x), na_level = "(Missing)"))) %>%
  select(all_of(cat_vars)) %>%
  pivot_longer(everything(), names_to = "Variable", values_to = "Level") %>%
  count(Variable, Level, name = "n") %>%
  group_by(Variable) %>%
  mutate(pct = round(100 * n / sum(n), 1)) %>%
  ungroup() %>%
  arrange(Variable, desc(n))

kable(cont_sum, caption = "Table 1a. Continuous variables (age, BMI): N, missing, mean (SD), range.")
kable(cat_sum,  caption = "Table 1b. Categorical variables (sex, race3, diabetes_ind): counts and percentages.")
```

```{r}
#| label: adult-n
#| echo: false
adult_n <- nrow(adult)
```

```{r}
#| label: tbl-adult
#| tbl-cap: !expr sprintf("Excerpt of the NHANES 2013–2014 adult cohort (age ≥ 20; N = %s) with derived and standardized variables.", format(adult_n, big.mark = ","))
knitr::kable(head(adult))
```

As shown in @tbl-adult, the analytic adult cohort (N =
`r format(adult_n, big.mark=",")`) includes standardized variables for
age and BMI (`age_c`, `bmi_c`), categorical indicators for sex and
race/ethnicity (`race3`), and a binary doctor-diagnosed diabetes
variable (`diabetes_ind`).

### 1.3 Missing Data Summary

```{r}
#| label: miss-pcts
#| echo: false
miss_bmi  <- mean(is.na(adult_eda$bmi)) * 100
miss_diab <- mean(is.na(adult_eda$diabetes_ind)) * 100
```

```{r}
#| label: fig-missing
#| fig-cap: "Missing data pattern for analytic variables (outcome and predictors only)."
#| echo: false
DataExplorer::plot_missing(
  adult_eda[, c("diabetes_ind", "bmi", "age")]
)
```

```{r}
#| label: fig-cohort-age
#| fig-cap: "Age distribution (age ≥ 20)."
#| echo: false
ggplot(adult_eda, aes(x = age)) +
  geom_histogram(binwidth = 5) +
  labs(x = "Age (years)", y = "Count") +
  theme_minimal()
```

```{r}
#| label: fig-cohort-bmi
#| fig-cap: "BMI distribution."
#| echo: false
ggplot(adult_eda, aes(x = bmi)) +
  geom_histogram(binwidth = 1) +
  labs(x = "BMI (kg/m²)", y = "Count") +
  theme_minimal()
```

```{r}
#| label: fig-cohort-sex
#| fig-cap: "Sex composition."
#| echo: false
adult_eda %>%
  count(sex) %>%
  ggplot(aes(x = sex, y = n)) +
  geom_col() +
  labs(x = NULL, y = "Count") +
  theme_minimal()
```

```{r}
#| label: fig-cohort-race
#| fig-cap: "Race/ethnicity composition (race3)."
#| echo: false
adult_eda %>%
  count(race3) %>%
  ggplot(aes(x = race3, y = n)) +
  geom_col() +
  labs(x = NULL, y = "Count") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 30, hjust = 1))
```

The EDA missingness summary shows approximately
`r sprintf("%.1f", miss_bmi)`% **missing BMI** and
`r sprintf("%.1f", miss_diab)`% **missing diabetes status**
(`diabetes_ind`). All design variables (WTMEC2YR, SDMVPSU, SDMVSTRA), as
well as age, sex, and race3, are complete—sex and race NAs are encoded
as explicit “(Missing)” levels in the EDA view.

These low missingness rates support the use of **multiple imputation
(MICE)** to preserve the analytic sample and reduce bias associated with
listwise deletion.

## 2. Modeling Frameworks

We evaluated three complementary modeling frameworks:\
(1) a **survey-weighted logistic regression** to account for NHANES
design elements,\
(2) **multiple imputation (MICE)** to address missing data, and\
(3) a **Bayesian logistic regression** using weakly informative priors.\
Each model used the same outcome (`diabetes_ind`) and predictors
(standardized age, standardized BMI, sex, and race3).

### 2.1 Survey-Weighted Logistic Regression (Design-Based MLE)

```{r}
#| label: data-clean
#| echo: true
#| message: false
#| warning: false

adult_clean <- adult %>%
  dplyr::mutate(
    sex   = forcats::fct_drop(sex),
    race3 = forcats::fct_drop(race3),
    age_c = as.numeric(age_c),
    bmi_c = as.numeric(bmi_c)
  ) %>%
  dplyr::filter(!is.na(diabetes_ind), !is.na(age_c), !is.na(bmi_c)) %>%
  droplevels()

str(adult_clean[, c("diabetes_ind","sex","race3","age_c","bmi_c")])
```

```{r}
#| label: model-survey
#| echo: true
#| message: false
#| warning: false

options(survey.lonely.psu = "adjust")

nhanes_design_adult <- survey::svydesign(
id = ~SDMVPSU,
strata = ~SDMVSTRA,
weights = ~WTMEC2YR,
nest = TRUE,
data = adult_clean
)

des_cc <- subset(nhanes_design_adult, !is.na(diabetes_ind) &
!is.na(age_c) & !is.na(bmi_c) &
!is.na(sex) & !is.na(race3))

# Sanity checks before modeling
stopifnot(nlevels(adult_clean$sex)   >= 2)
stopifnot(nlevels(adult_clean$race3) >= 2)

table(adult_clean$sex)
table(adult_clean$race3)

svy_fit <- survey::svyglm(
diabetes_ind ~ age_c + bmi_c + sex + race3,
design = des_cc,
family = quasibinomial()
)

svy_or <- broom::tidy(svy_fit, conf.int = TRUE) %>%
dplyr::mutate(
OR = exp(estimate),
LCL = exp(conf.low),
UCL = exp(conf.high)
) %>%
dplyr::select(term, OR, LCL, UCL, p.value) %>%
dplyr::filter(term != "(Intercept)")

#| label: tbl-survey
#| tbl-cap: "Survey-weighted logistic regression: odds ratios (OR) and 95% confidence intervals for diabetes diagnosis among adults (NHANES 2013–2014)."
knitr::kable(svy_or)
```

The NHANES 2013–2014 data use a complex, multistage probability design
involving **strata** (`SDMVSTRA`), **primary sampling units (PSUs;
SDMVPSU)**, and **examination weights (`WTMEC2YR`)** to ensure
nationally representative estimates [@nchs2014].

This model incorporates those design elements via the `svydesign()`
function to produce population-weighted logistic regression estimates
that reflect the U.S. civilian, noninstitutionalized population.

Odds ratios are expressed per one standard deviation (SD) increase in
**age** and **BMI**, quantifying the association between each predictor
and the probability of a doctor-diagnosed diabetes outcome.

Notes: Reference categories are **Male** for sex and **White** for
race3. **Age** and **BMI** ORs are per **1 SD** increase.

### 2.2 Multiple Imputation (MICE)

```{r}
#| label: model-mice
#| echo: true
#| message: false
#| warning: false

# Build data for imputation

mi_dat <- adult %>%
dplyr::select(diabetes_ind, age, bmi, sex, race3, WTMEC2YR, SDMVPSU, SDMVSTRA)

# Specify methods and predictor matrix

meth <- mice::make.method(mi_dat)
pred <- mice::make.predictorMatrix(mi_dat)

# Outcome not imputed; used as predictor only

meth["diabetes_ind"] <- ""
pred["diabetes_ind", ] <- 0
pred[, "diabetes_ind"] <- 1

# Continuous vars: age = normal; bmi = predictive mean matching (PMM).
# Categoricals: sex = logistic; race3 = polytomous regression.

meth[c("age","bmi")]   <- c("norm","pmm")
meth[c("sex","race3")] <- c("logreg","polyreg")

# Design variables as auxiliaries only

meth[c("WTMEC2YR","SDMVPSU","SDMVSTRA")] <- ""
pred[, c("WTMEC2YR","SDMVPSU","SDMVSTRA")] <- 1

# Run imputation

imp <- mice::mice(mi_dat, m = 5, method = meth, predictorMatrix = pred, seed = 123)

# Fit logistic regression within each imputed dataset

fits <- with(
  imp,
  glm(
    diabetes_ind ~ scale(age) + scale(bmi) + relevel(sex, "Male") + relevel(race3, "White"),
    family = binomial()
  )
)

# Pool estimates across imputations

pool_mi <- mice::pool(fits)

mi_or <- summary(pool_mi, conf.int = TRUE, exponentiate = TRUE) %>%
dplyr::rename(OR = estimate, LCL = `2.5 %`, UCL = `97.5 %`) %>%
dplyr::filter(term != "(Intercept)")
```

```{r}
#| label: tbl-mice
miss_age  <- sum(is.na(mi_dat$age))
miss_bmiN <- sum(is.na(mi_dat$bmi))

mi_caption <- if (miss_age > 0 && miss_bmiN > 0) {
  "Multiple Imputation (MICE): pooled odds ratios (OR) and 95% confidence intervals after imputing missing age (normal) and BMI (PMM) (m = 5); diabetes status was not imputed."
} else if (miss_bmiN > 0) {
  "Multiple Imputation (MICE): pooled odds ratios (OR) and 95% confidence intervals after imputing missing BMI (PMM) (m = 5); diabetes status was not imputed."
} else if (miss_age > 0) {
  "Multiple Imputation (MICE): pooled odds ratios (OR) and 95% confidence intervals after imputing missing age (normal) (m = 5); diabetes status was not imputed."
} else {
  "Multiple Imputation (MICE): pooled odds ratios (OR) and 95% confidence intervals (no variables required imputation); diabetes status was not imputed."
}
mi_caption <- paste0(mi_caption, " Odds ratios are per 1 SD for age and BMI.")

knitr::kable(mi_or, caption = mi_caption)
```

The multiple imputation (MICE) framework addressed missingness in
**BMI** using predictive mean matching (PMM). **Diabetes status
(`diabetes_ind`) was not imputed** (used as a predictor only).
Categorical variables (`sex`, `race3`) used logistic/polytomous
regression. We generated m = 5 imputations and pooled estimates with Rubin’s rules. Odds ratios are reported per one standard deviation (1 SD) increase in age and BMI.

### 2.3 Bayesian Logistic Regression

```{r}
#| label: model-bayesian
#| cache: true
#| echo: true
#| message: false
#| warning: false
#| results: 'hide'

# Prepare a single imputed dataset (from MICE results)

adult_imp1 <- mice::complete(imp, 1) %>%
dplyr::mutate(
sex   = factor(sex,   levels = levels(adult$sex)),
race3 = factor(race3, levels = levels(adult$race3)),
age_c = as.numeric(scale(age)),
bmi_c = as.numeric(scale(bmi)),
wt_norm = WTMEC2YR / mean(WTMEC2YR, na.rm = TRUE)
) %>%
dplyr::filter(!is.na(diabetes_ind), !is.na(age_c), !is.na(bmi_c)) %>%
droplevels()

# Define model formula

fml_bayes <- diabetes_ind | weights(wt_norm) ~ age_c + bmi_c + sex + race3

# Specify weakly informative priors

priors <- c(
brms::set_prior("normal(0, 2.5)", class = "b"),
brms::set_prior("student_t(3, 0, 10)", class = "Intercept")
)

# Fit Bayesian logistic regression

bayes_fit <- brms::brm(
formula = fml_bayes,
data    = adult_imp1,
family  = bernoulli(link = "logit"),
prior   = priors,
chains  = 4, iter = 2000, seed = 123,
control = list(adapt_delta = 0.95),
refresh = 0
)

# Shared objects for diagnostics/plots
yobs <- adult_imp1$diabetes_ind

# Extract posterior summaries and transform to odds ratios

bayes_or <- brms::posterior_summary(bayes_fit, pars = "^b_") %>%
as.data.frame() %>%
tibble::rownames_to_column("raw") %>%
dplyr::mutate(
term = gsub("^b_", "", raw),
OR   = exp(Estimate),
LCL  = exp(Q2.5),
UCL  = exp(Q97.5)
) %>%
dplyr::select(term, OR, LCL, UCL)
```

```{r}
#| label: tbl-bayes
#| tbl-cap: "Bayesian logistic regression: posterior odds ratios (OR) with 95% credible intervals for diabetes diagnosis among adults (NHANES 2013–2014)."
knitr::kable(
  dplyr::mutate(bayes_or, dplyr::across(c(OR, LCL, UCL), ~ round(.x, 2)))
)
```

As shown in @tbl-bayes, this Bayesian logistic regression model
estimates the log-odds of diabetes using standardized predictors.\

Weakly informative priors (Normal(0, 2.5) for slopes; Student-t(3, 0,
10) for the intercept) help stabilize estimation and prevent
overfitting.\

Posterior means and 95% credible intervals (CrI) provide full
uncertainty quantification for each predictor, enabling direct
probabilistic interpretation.

Note: The Bayesian model used normalized NHANES exam weights as
importance weights, approximating design-based inference; strata and
PSUs were not directly modeled.

```{r}
#| label: tbl-bayesR2
#| tbl-cap: "Bayesian R² summary."
knitr::kable(as.data.frame(brms::bayes_R2(bayes_fit)))
```

```{r}
#| label: tbl-mcmc-diagnostics
#| tbl-cap: "MCMC diagnostics (R-hat and Effective Sample Sizes) for model parameters (including intercept)."
#| echo: false

# Get diagnostics as plain columns (use metric names as strings)
diag <- posterior::summarise_draws(
  bayes_fit,
  "rhat", "ess_bulk", "ess_tail"
)

# Keep only slope parameters (b_*) and build a clean table.
diag_b <- diag |>
  dplyr::as_tibble() |>
  dplyr::filter(grepl("^b_", .data$variable)) |>
  dplyr::transmute(
    Parameter = .data$variable,
    Rhat      = .data$rhat,
    Bulk_ESS  = .data$ess_bulk,
    Tail_ESS  = .data$ess_tail
  )

knitr::kable(diag_b, digits = 1)
```

```{r}
#| label: model-comparison
#| tbl-cap: "Bayesian model comparison (LOO): base model vs models without race or without sex."
#| echo: true
#| message: false
#| warning: false

# Reduced models (reuse compiled Stan where possible)
fit_no_race <- update(
  bayes_fit,
  formula = update(fml_bayes, . ~ . - race3),
  recompile = FALSE,
  refresh = 0
)

fit_no_sex <- update(
  bayes_fit,
  formula = update(fml_bayes, . ~ . - sex),
  recompile = FALSE,
  refresh = 0
)

loo_base    <- loo::loo(bayes_fit)
loo_no_race <- loo::loo(fit_no_race)
loo_no_sex  <- loo::loo(fit_no_sex)

cmp <- loo::loo_compare(loo_base, loo_no_race, loo_no_sex)

cmp_df <- as.data.frame(cmp)
cmp_df$model <- rownames(cmp_df)
cmp_df <- dplyr::relocate(cmp_df, model)
knitr::kable(
  cmp_df,
  caption = "LOO comparison (rows sorted best→worst by elpd_loo; more positive = better)."
)

knitr::kable(as.data.frame(cmp), caption = "LOO comparison (lower expected log predictive density is worse).")
```

The base model shows the best expected out-of-sample fit (LOO), while
removing race or sex degrades predictive performance, indicating these
terms contribute to prediction.

```{r}
#| label: fig-mcmc-areas
#| fig-cap: "Posterior distributions (95% credible mass) for slope parameters."
#| echo: false
bayesplot::mcmc_areas(
as.array(bayes_fit),
regex_pars = "^b_",
prob = 0.95
)
```

```{r}
#| label: fig-mcmc-trace
#| fig-cap: "Trace plots for slope parameters (chain mixing and stationarity)."
#| echo: false
bayesplot::mcmc_trace(
as.array(bayes_fit),
regex_pars = "^b_"
)
```

```{r}
#| label: fig-ppc-bars
#| fig-cap: "Posterior predictive check: observed vs replicated outcome distribution (bars)."
#| echo: false

bayesplot::pp_check(bayes_fit, type = "bars", nsamples = 100)
```

```{r}
#| label: fig-ppc-mean-sd
#| fig-cap: "Posterior predictive checks for mean and standard deviation of the binary outcome."
#| echo: false
yrep <- brms::posterior_predict(bayes_fit, ndraws = 400)  # draws x observations

bayesplot::ppc_stat(y = yobs, yrep = yrep, stat = "mean")
bayesplot::ppc_stat(y = yobs, yrep = yrep, stat = "sd")
```

```{r}
#| label: fig-or-forest
#| fig-cap: "Posterior odds ratios (points) with 95% credible intervals (lines)."
#| echo: false
library(dplyr)
library(ggplot2)
library(dplyr)
library(ggplot2)
library(stringr)

or_plot <- bayes_or %>%
  dplyr::filter(!is.na(term), term != "Intercept") %>%
  dplyr::mutate(
    term_clean = term |>
      as.character() |>
      stringr::str_replace("^age_c$",        "Age (per 1 SD)") |>
      stringr::str_replace("^bmi_c$",        "BMI (per 1 SD)") |>
      stringr::str_replace("^sexFemale$",    "Female vs Male") |>
      stringr::str_replace("^race3Black$",   "Race: Black (vs White)") |>
      stringr::str_replace("^race3Hispanic$","Race: Hispanic (vs White)") |>
      stringr::str_replace("^race3Other$",   "Race: Other (vs White)")
  )

ggplot(or_plot, aes(x = OR, y = reorder(term_clean, OR))) +
  geom_vline(xintercept = 1, linetype = 2) +
  geom_point() +
  geom_errorbarh(aes(xmin = LCL, xmax = UCL), height = 0.15) +
  labs(x = "Odds ratio (logit model, posterior)", y = NULL)
```

```{r}
#| label: fig-pred-calibration
#| fig-cap: "Observed outcome vs mean predicted probability (calibration scatter with smoother)."
#| echo: false
pred_mean <- colMeans(brms::posterior_epred(bayes_fit))
ggplot(data.frame(pred = pred_mean, obs = yobs),
       aes(x = pred, y = obs)) +
  geom_point(alpha = 0.15, position = position_jitter(height = 0.03)) +
  geom_smooth(method = "loess", se = TRUE) +
  labs(x = "Mean predicted probability", y = "Observed diabetes (0/1)")
```

```{r}
#| label: fig-posterior-prevalence
#| fig-cap: "Posterior predictive distribution of diabetes prevalence compared to observed NHANES prevalence."
#| echo: false

# Posterior draws of individual-level predicted probabilities (draws x observations)
post_pred <- brms::posterior_epred(bayes_fit, summary = FALSE)

# Collapse to posterior prevalence (mean across observations for each draw)
post_prev <- rowMeans(post_pred)  # numeric vector (iterations)

# Observed prevalence in the (imputed) analysis data
obs_prev  <- mean(adult_imp1$diabetes_ind, na.rm = TRUE)

# bayesplot expects a matrix/array; convert vector to 2D matrix (iters x 1 param)
post_prev_mat <- matrix(post_prev, ncol = 1, dimnames = list(NULL, "Prevalence"))

# Density plot with ggplot-style labels and an observed-prevalence reference line
p <- bayesplot::mcmc_dens(post_prev_mat) +
  ggplot2::labs(
    title = "Posterior diabetes prevalence",
    x = "Predicted prevalence",
    y = NULL
  ) +
  ggplot2::geom_vline(xintercept = obs_prev, linetype = 2)

p
```

```{r}
#| label: fig-prior-posterior-overlay
#| fig-cap: "Overlay of posterior densities for key predictors (Age, BMI, Sex, Race)."
#| echo: false

# Get available coefficient names from model
avail_b <- grep("^b_", brms::parnames(bayes_fit), value = TRUE)

# Specify the ones we *want* to plot
target <- c("b_age_c", "b_bmi_c", "b_sexFemale", "b_race3Black", "b_race3Hispanic", "b_race3Other")

# Keep only those that actually exist in the model
pars_ok <- intersect(target, avail_b)

# Stop gracefully if none are found
if (length(pars_ok) == 0) {
  stop("No matching slope parameters found for overlay. Check available names via brms::parnames(bayes_fit).")
}

# Plot posterior density overlays for the available parameters
bayesplot::mcmc_dens_overlay(
  as.array(bayes_fit),
  pars = pars_ok
) +
  ggplot2::labs(
    title = "Posterior distributions for key predictors",
    x = "Coefficient value (log-odds scale)",
    y = NULL
  )
```

The posterior predictive distribution of diabetes prevalence closely
aligned with the observed NHANES prevalence, suggesting good model
calibration. Priors and posteriors overlapped primarily for weak
predictors, while strong effects (BMI, age, race) showed distinct
posterior shifts, confirming that inference was data-driven rather than
dominated by priors.

## 3. Results

```{r}
#| label: results-compare
#| echo: true
#| message: false
#| warning: false

# Combine results from all three models

svy_tbl   <- if (exists("svy_or") && nrow(svy_or) > 0)
dplyr::mutate(svy_or,   Model = "Survey-weighted (MLE)") else NULL
mi_tbl    <- if (exists("mi_or") && nrow(mi_or) > 0)
dplyr::mutate(mi_or,    Model = "MICE Pooled") else NULL
bayes_tbl <- if (exists("bayes_or") && nrow(bayes_or) > 0)
dplyr::mutate(bayes_or, Model = "Bayesian") %>%
dplyr::filter(term != "Intercept") else NULL

all_tbl <- dplyr::bind_rows(Filter(Negate(is.null), list(svy_tbl, mi_tbl, bayes_tbl))) %>%
dplyr::mutate(
term = case_when(
  grepl("bmi", term,  ignore.case = TRUE) ~ "BMI (per 1 SD)",
  grepl("age", term,  ignore.case = TRUE) ~ "Age (per 1 SD)",
  grepl("^sexFemale$", term)              ~ "Female vs Male",
  grepl("^race3Hispanic$", term)          ~ "Race = Hispanic",
  grepl("^race3Black$", term)             ~ "Race = Black",
  grepl("^race3Other$", term)             ~ "Race = Other",
  TRUE ~ term
),
OR_CI = sprintf("%.2f (%.2f – %.2f)", OR, LCL, UCL)
) %>%
dplyr::select(Model, term, OR_CI)

#| label: tbl-comparison
#| tbl-cap: "Comparison of odds ratios (per 1 SD for age and BMI) and 95% intervals across survey-weighted, MICE, and Bayesian frameworks."
knitr::kable(all_tbl, align = c("l","l","c"))
```

This table summarizes results from the survey-weighted (design-based),
multiple-imputation, and Bayesian models.\

Across all frameworks, **age** and **BMI** consistently show strong
positive associations with diabetes risk, confirming robustness to
missing-data handling and modeling assumptions.\

The **Bayesian** model’s credible intervals closely align with the
frequentist confidence intervals but provide a more direct probabilistic
interpretation of uncertainty.

## 4. Discussion and Limitations

### 4.1 Interpretation

The Bayesian logistic regression framework produced results that were
highly consistent with both the survey-weighted and MICE-pooled
frequentist models. Age and BMI remained the most influential predictors
of doctor-diagnosed diabetes, each showing a strong and positive
association with diabetes risk. Posterior credible intervals were
slightly narrower than frequentist confidence intervals, reflecting the
stabilizing influence of weakly informative priors.

Unlike classical maximum likelihood estimation, the Bayesian approach
directly quantified uncertainty through posterior distributions,
offering richer interpretability and more transparent probability
statements. The alignment between Bayesian and design-based estimates
supports the robustness of these associations and highlights the
practicality of Bayesian modeling for complex, weighted population data.
Moreover, the Bayesian framework allowed diagnostic evaluation through
posterior predictive checks and Bayesian R², confirming that model fit
and predictive adequacy were acceptable.

Overall, this study demonstrates how Bayesian inference complements
traditional epidemiologic methods—maintaining interpretability while
improving stability under missing data and modest sample imbalance.

### 4.2 Limitations

While this analysis demonstrates the value of Bayesian logistic
regression for epidemiologic modeling, several limitations should be
acknowledged.

First, the use of a single imputed dataset for the Bayesian model—rather
than full joint modeling of imputation uncertainty—may understate total
variance.

Second, NHANES exam weights were normalized and treated as importance
weights, which approximate but do not fully reproduce design-based
inference.

Third, the weakly informative priors *Normal(0, 2.5)* for slopes and
*Student-t(3, 0, 10)* for the intercept were not empirically tuned;
alternative prior specifications could slightly alter posterior
intervals.

Finally, although convergence diagnostics (R̂ ≈ 1, sufficient effective
sample sizes, and stable posterior predictive checks) indicated good
model performance, results are conditional on the 2013–2014 NHANES cycle
and may not generalize to later datasets or longitudinal analyses.

## 5. Conclusion

The Bayesian, survey-weighted, and imputed logistic regression
frameworks all identified consistent predictors of diabetes risk in U.S.
adults: advancing age, higher BMI, male sex, and non-White
race/ethnicity.

The Bayesian model produced estimates nearly identical in direction and
magnitude to the frequentist results while providing a more
comprehensive assessment of uncertainty through posterior distributions
and credible intervals.

By incorporating prior information and using MCMC to sample from the
full posterior distribution, Bayesian inference enhances model
transparency and interpretability.

Its agreement with traditional approaches underscores that Bayesian
methods can be applied confidently in large-scale public health datasets
such as NHANES.

Future extensions could integrate hierarchical priors, longitudinal
NHANES cycles, or Bayesian model averaging to better capture population
heterogeneity and evolving diabetes risk patterns over time.
