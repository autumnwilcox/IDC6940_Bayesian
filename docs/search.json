[
  {
    "objectID": "Summaries/et/paper6.html",
    "href": "Summaries/et/paper6.html",
    "title": "Variable Selection Using Bayesian Additive Regression Trees",
    "section": "",
    "text": "Chuji Luo, Michael J. Daniels\nStatistical Science, Vol.39, No. 2, 286-304, May 2024\nhttps://arxiv.org/abs/2112.13998\n\n\nThis paper introduces a novel method for variable selection in regression models using Bayesian Additive Regression Trees (BART). The authors propose a permutation-based approach to assess the importance of each predictor variable by comparing the model’s performance with and without the variable. This method allows for the identification of relevant predictors while accounting for complex interactions and nonlinear relationships inherent in BART models. The authors demonstrate the effectiveness of their approach through simulations and real-world applications, showing that it outperforms traditional variable selection methods in terms of accuracy and interpretability.\n\n\n\nIn regression analysis, identifying the most relevant predictor variables is crucial for building accurate and interpretable models. Traditional variable selection methods often struggle with complex data structures, such as nonlinear relationships and interactions among predictors. Bayesian Additive Regression Trees (BART) offer a flexible modeling framework that can capture these complexities, but they lack built-in mechanisms for variable selection. The challenge addressed in this paper is to develop a robust method for selecting important variables within the BART framework, enabling researchers to leverage BART’s strengths while ensuring model interpretability.\n\n\n\nThe authors propose a permutation-based variable selection method within the BART framework. The key steps of the methodology are as follows: 1. Fit a BART model to the data using all predictor variables. 2. For each predictor variable, create a permuted version of the dataset by randomly shuffling the values of that variable while keeping the other variables unchanged. 3. Fit a BART model to the permuted dataset and evaluate its performance using a suitable metric (e.g., mean squared error). 4. Compare the performance of the original BART model with the permuted model. A significant drop in performance when a variable is permuted indicates that the variable is important for predicting the outcome. 5. Repeat the permutation process multiple times to obtain a distribution of performance metrics for each variable, allowing for statistical inference about variable importance. 6. Select variables based on their importance scores, using a threshold to determine which variables are considered relevant. 7. The authors validate their method through extensive simulations and applications to real-world datasets, demonstrating its effectiveness in identifying important predictors while accounting for complex interactions and nonlinearities.\n\n\n\nThe proposed permutation-based variable selection method using BART was evaluated through simulations and real-world applications. The results showed that the method effectively identified important predictor variables, outperforming traditional variable selection techniques in terms of accuracy and interpretability. The authors reported that their approach was able to capture complex interactions and nonlinear relationships among predictors, leading to improved model performance. Additionally, the method provided a clear ranking of variable importance, facilitating the interpretation of results. Overall, the findings suggest that the proposed method is a valuable tool for variable selection in regression models, particularly when using BART.\n\n\n\n\nThe paper introduced three new variable selection approaches: (1) a permutation-based approach using within-type BART Variable Inclusion Proportion (VIP), (2) a permutation-based approach using BART Metropolis Importance (MI), and (3) a backward selection procedure with two filters.\nThese new approaches were designed specifically to address issues with existing methods, such as being biased against categorical predictors and being conservative in including relevant predictors, particularly in data settings with mixed-type predictors (e.g., continuous and binary).\nBased on the simulation results, where success was defined as an excellent capability of including all relevant predictors (rmiss \\(\\le 0.1\\)) and acceptable capability of excluding irrelevant predictors (precision \\(\\ge 0.6\\)), the three proposed approaches consistently perform well in identifying all the relevant predictors and excluding irrelevant predictors.\n\nRanking of Successful Approaches (by Success Rate)\nThe backward selection approach with two filters achieves the highest success rate (70.8%) across the various simulation scenarios tested. The two new permutation-based approaches followed closely in performance: 1. Backward selection with two filters: 70.8% success rate. 2. Permutation-based approach using BART MI: 66.7% success rate. 3. Permutation-based approach using BART Within-Type VIP: 62.5% success rate.\nIdentified Drawbacks\n\nA significant drawback of the three proposed approaches, similar to existing BART-based variable selection methods, is that they suffer from multicollinearity (correlated predictors). This challenge is particularly noticeable when the noise is high or when dealing with a binary response variable.\nThe computational cost of the backward selection approach is a shortcoming because it requires running BART multiple times, although this cost can be reduced by fitting the models in parallel on multiple cores.\nWhile the permutation-based approach using BART within-type VIP improves upon the existing BART VIP approach for small numbers of mixed-type predictors, it also suffers from multicollinearity."
  },
  {
    "objectID": "Summaries/et/paper6.html#summary",
    "href": "Summaries/et/paper6.html#summary",
    "title": "Variable Selection Using Bayesian Additive Regression Trees",
    "section": "",
    "text": "This paper introduces a novel method for variable selection in regression models using Bayesian Additive Regression Trees (BART). The authors propose a permutation-based approach to assess the importance of each predictor variable by comparing the model’s performance with and without the variable. This method allows for the identification of relevant predictors while accounting for complex interactions and nonlinear relationships inherent in BART models. The authors demonstrate the effectiveness of their approach through simulations and real-world applications, showing that it outperforms traditional variable selection methods in terms of accuracy and interpretability."
  },
  {
    "objectID": "Summaries/et/paper6.html#problem",
    "href": "Summaries/et/paper6.html#problem",
    "title": "Variable Selection Using Bayesian Additive Regression Trees",
    "section": "",
    "text": "In regression analysis, identifying the most relevant predictor variables is crucial for building accurate and interpretable models. Traditional variable selection methods often struggle with complex data structures, such as nonlinear relationships and interactions among predictors. Bayesian Additive Regression Trees (BART) offer a flexible modeling framework that can capture these complexities, but they lack built-in mechanisms for variable selection. The challenge addressed in this paper is to develop a robust method for selecting important variables within the BART framework, enabling researchers to leverage BART’s strengths while ensuring model interpretability."
  },
  {
    "objectID": "Summaries/et/paper6.html#methodology",
    "href": "Summaries/et/paper6.html#methodology",
    "title": "Variable Selection Using Bayesian Additive Regression Trees",
    "section": "",
    "text": "The authors propose a permutation-based variable selection method within the BART framework. The key steps of the methodology are as follows: 1. Fit a BART model to the data using all predictor variables. 2. For each predictor variable, create a permuted version of the dataset by randomly shuffling the values of that variable while keeping the other variables unchanged. 3. Fit a BART model to the permuted dataset and evaluate its performance using a suitable metric (e.g., mean squared error). 4. Compare the performance of the original BART model with the permuted model. A significant drop in performance when a variable is permuted indicates that the variable is important for predicting the outcome. 5. Repeat the permutation process multiple times to obtain a distribution of performance metrics for each variable, allowing for statistical inference about variable importance. 6. Select variables based on their importance scores, using a threshold to determine which variables are considered relevant. 7. The authors validate their method through extensive simulations and applications to real-world datasets, demonstrating its effectiveness in identifying important predictors while accounting for complex interactions and nonlinearities."
  },
  {
    "objectID": "Summaries/et/paper6.html#results-and-performance",
    "href": "Summaries/et/paper6.html#results-and-performance",
    "title": "Variable Selection Using Bayesian Additive Regression Trees",
    "section": "",
    "text": "The proposed permutation-based variable selection method using BART was evaluated through simulations and real-world applications. The results showed that the method effectively identified important predictor variables, outperforming traditional variable selection techniques in terms of accuracy and interpretability. The authors reported that their approach was able to capture complex interactions and nonlinear relationships among predictors, leading to improved model performance. Additionally, the method provided a clear ranking of variable importance, facilitating the interpretation of results. Overall, the findings suggest that the proposed method is a valuable tool for variable selection in regression models, particularly when using BART."
  },
  {
    "objectID": "Summaries/et/paper6.html#conclusion",
    "href": "Summaries/et/paper6.html#conclusion",
    "title": "Variable Selection Using Bayesian Additive Regression Trees",
    "section": "",
    "text": "The paper introduced three new variable selection approaches: (1) a permutation-based approach using within-type BART Variable Inclusion Proportion (VIP), (2) a permutation-based approach using BART Metropolis Importance (MI), and (3) a backward selection procedure with two filters.\nThese new approaches were designed specifically to address issues with existing methods, such as being biased against categorical predictors and being conservative in including relevant predictors, particularly in data settings with mixed-type predictors (e.g., continuous and binary).\nBased on the simulation results, where success was defined as an excellent capability of including all relevant predictors (rmiss \\(\\le 0.1\\)) and acceptable capability of excluding irrelevant predictors (precision \\(\\ge 0.6\\)), the three proposed approaches consistently perform well in identifying all the relevant predictors and excluding irrelevant predictors.\n\nRanking of Successful Approaches (by Success Rate)\nThe backward selection approach with two filters achieves the highest success rate (70.8%) across the various simulation scenarios tested. The two new permutation-based approaches followed closely in performance: 1. Backward selection with two filters: 70.8% success rate. 2. Permutation-based approach using BART MI: 66.7% success rate. 3. Permutation-based approach using BART Within-Type VIP: 62.5% success rate.\nIdentified Drawbacks\n\nA significant drawback of the three proposed approaches, similar to existing BART-based variable selection methods, is that they suffer from multicollinearity (correlated predictors). This challenge is particularly noticeable when the noise is high or when dealing with a binary response variable.\nThe computational cost of the backward selection approach is a shortcoming because it requires running BART multiple times, although this cost can be reduced by fitting the models in parallel on multiple cores.\nWhile the permutation-based approach using BART within-type VIP improves upon the existing BART VIP approach for small numbers of mixed-type predictors, it also suffers from multicollinearity."
  },
  {
    "objectID": "Summaries/et/paper3.html",
    "href": "Summaries/et/paper3.html",
    "title": "An introduction to using Bayesian linear regression with clinical data",
    "section": "",
    "text": "An introduction to using Bayesian linear regression with clinical data\nScott A. Baldwin, Michael J. Larson\nBehaviour Research and Therapy, Volume 98, 2017, Pages 58-75\nhttps://doi.org/10.1016/j.brat.2016.12.016\n\nSummary\nThis paper provides an introduction to Bayesian linear regression within the context of clinical data analysis. It contrasts Bayesian methodologies with traditional frequentist methods, highlighting the limitations of the latter, particularly in relation to p-values and confidence intervals. The article explains fundamental Bayesian concepts such as priors, likelihood, and posterior distributions. It presents an example using electroencephalogram (EEG) and anxiety study data to demonstrate the relationship between error-related negativity (ERN) and trait anxiety. It also covers practical aspects like Markov Chain Monte Carlo (MCMC) sampling, assessing model convergence, interpreting credible intervals, and evaluating model fit using WAIC and LOO-CV, extending the discussion to other models like logistic and Poisson regression.\n\n\nWhat is the goal of the paper?\nThe goal of the paper is to introduce Bayesian linear regression techniques to researchers in clinical psychology and related fields, demonstrating how these methods can provide more informative and nuanced insights compared to traditional frequentist approaches.\n\n\nWhat is the problem being addressed?\nThe problem being addressed is the limitations of frequentist statistical methods, particularly the reliance on p-values and confidence intervals, which can lead to misinterpretations and less informative results in clinical data analysis. ### Why is it important? It is important because clinical data often involve complexities such as small sample sizes, missing data, and hierarchical structures that traditional methods may not handle well. Bayesian methods offer a more flexible and robust framework for analyzing such data, leading to better-informed clinical decisions and research outcomes. ### What are the key results? Key results include a detailed explanation of Bayesian linear regression concepts, a practical example using EEG and anxiety data, and guidance on implementing Bayesian methods using software like R and Stan. The paper also discusses how to assess model convergence and fit, providing a comprehensive overview of Bayesian analysis in a clinical context. ### What are the limitations of the paper? Limitations of the paper include its focus on linear regression, which may not cover all types of clinical data analysis needs."
  },
  {
    "objectID": "Summaries/et/paper5.html",
    "href": "Summaries/et/paper5.html",
    "title": "A Dynamic Bayesian Model for Identifying High-Mortality Risk in Hospitalized COVID-19 Patients",
    "section": "",
    "text": "Amir Momeni-Boroujeni, Rachelle Mendoza, Isaac J. Stopard, Ben Lambert, and Alejandro Zuretti\nInfect. Dis. Rep. 2021, 13, 239–250. https://doi.org/10.3390/idr13010027\n\n\nTHis paper describes a study that used a Bayesian Markov model to better predict mortality risk for hospitalized COVID-19 patients by incorporating dynamic changes in laboratory values over time, rather than relying solely on admission data. The researchers collected demographic, comorbidity, and lab data for 553 PCR-positive patients and found that factors like age over 80 and certain comorbidities increased risk, but including dynamic changes in biomarkers significantly improved the predictive accuracy of the model. The study concludes by presenting a clinical decision tool that uses the most important factors for patient risk stratification based on available information at different stages of hospitalization.\n\n\n\nThe central problem this research aims to solve is how to accurately and quickly predict which hospitalized COVID-19 patients face the highest risk of death. Because hospitalization rates remained high and the disease caused millions of deaths globally, there was an urgent need to identify factors that predict severe disease and mortality to improve patient care and outcomes. The developed model is intended to allow for prioritization at the systems level and the individualization of care for each patient.\nA major shortcoming of previous prognostic tools was that they were static. Existing models generally looked at patient data and biomarkers only at the time of hospital admission to predict a single outcome, like death, at a single future time point. However, the sources note that a patient’s risk changes constantly during hospitalization, as biomarkers show outcome-specific dynamic changes. Existing models struggled to incorporate these dynamic changes and had difficulty handling the competing risks a patient faces, like remaining in the hospital, being discharged, or dying. Failing to account for these ongoing changes meant that risk predictions were often inaccurate in real-time clinical settings.\nTherefore, the authors developed a “Dynamic Bayesian Model” to overcome these limitations. This new model combines a patient’s initial information (demographics and comorbidities) with daily dynamic changes in laboratory test values throughout their hospital stay. By incorporating these time-dependent measurements, the model achieved dramatic improvements in predictive accuracy compared to models that relied only on admission data. This allows the model to provide daily adjustments to the patient’s in-hospital mortality risk, making it a more effective tool for clinicians deciding on appropriate care and resource allocation.\n\n\n\n\nCase selection\n\nPatients admitted to SUNY Downstate Medical Center, with COVID-19 related symptoms, between February 2020 and March 2020.\n\nSample\n\n553 PCR-positive patients included in the study.\nStratified into two groups: 200 patients who were discharged and 200 patients who died.\nData collected: demographic information, comorbidities, and laboratory test values.\n\nStatistical modeling approaches\n\nThe researchers conducted two main sets of analyses, both estimated using a Bayesian framework:\n\nDynamic Bayesian Markov Model\n\nThe primary goal was to develop a prognostic Markiv model that incorporates dynamic laboratory values with patients’ admission profiles.\nMarkov model accounts for competing risks (discharge vs. death) and allows for daily updates to mortality risk based on changing lab values.\nPredictor sets included:\n\nDemographics, comorbidities, admission, and lab values.\n\nThe model structure included:\n\nSecondary methodology: Logistic Regression Analysis\n\nThis analysis was conducted to determine factors most predictive of patient mortality but was specifically not intended to assess dynamic changes in mortality risk. This approach considered only patients’ outcomes, without factoring in the time taken for the outcome to occur.\n\nUnivariate Analysis (Baseline Comparison)\n\nBefore the multivariate analyses, univariate Cox survival analyses were used to illustrate the baseline patient characteristics and lab values upon admission that, individually, were the strongest determinants of risk.\n\n\n\nModel Validation\n\nInternal Validation: To assess the internal validity of the Markov model, the researchers performed k-fold cross-validation for each of the four regressions. The predictive accuracy increased significantly when dynamic test values were included, boosting accuracy from around 64–67% (for static models) to 83% (for the dynamic model).\nParameter Interpretation: Since the models were estimated in a Bayesian framework, the results are presented as probabilities representing the posterior probability that a given variable had an odds ratio exceeding one, removing the need for an arbitrary significance cutoff.\n\n\n\n\n\nThe performance results demonstrate that relying solely on static admission data (demographics, comorbidities, and initial lab values) yielded a mean predictive accuracy of only 64% to 67% across the initial regression sets. In contrast, incorporating dynamic changes in laboratory values (percentage changes relative to admission values) throughout hospitalization significantly boosted the model’s predictive power to an accuracy of 83%.\n\n\n\nThe article details a Dynamic Bayesian Markov Model that uses time-dependent changes in biomarkers to achieve 83% accuracy in identifying high-mortality risk in hospitalized COVID-19 patients, significantly outperforming static admission-only models (64–67% accuracy)."
  },
  {
    "objectID": "Summaries/et/paper5.html#summary",
    "href": "Summaries/et/paper5.html#summary",
    "title": "A Dynamic Bayesian Model for Identifying High-Mortality Risk in Hospitalized COVID-19 Patients",
    "section": "",
    "text": "THis paper describes a study that used a Bayesian Markov model to better predict mortality risk for hospitalized COVID-19 patients by incorporating dynamic changes in laboratory values over time, rather than relying solely on admission data. The researchers collected demographic, comorbidity, and lab data for 553 PCR-positive patients and found that factors like age over 80 and certain comorbidities increased risk, but including dynamic changes in biomarkers significantly improved the predictive accuracy of the model. The study concludes by presenting a clinical decision tool that uses the most important factors for patient risk stratification based on available information at different stages of hospitalization."
  },
  {
    "objectID": "Summaries/et/paper5.html#problem",
    "href": "Summaries/et/paper5.html#problem",
    "title": "A Dynamic Bayesian Model for Identifying High-Mortality Risk in Hospitalized COVID-19 Patients",
    "section": "",
    "text": "The central problem this research aims to solve is how to accurately and quickly predict which hospitalized COVID-19 patients face the highest risk of death. Because hospitalization rates remained high and the disease caused millions of deaths globally, there was an urgent need to identify factors that predict severe disease and mortality to improve patient care and outcomes. The developed model is intended to allow for prioritization at the systems level and the individualization of care for each patient.\nA major shortcoming of previous prognostic tools was that they were static. Existing models generally looked at patient data and biomarkers only at the time of hospital admission to predict a single outcome, like death, at a single future time point. However, the sources note that a patient’s risk changes constantly during hospitalization, as biomarkers show outcome-specific dynamic changes. Existing models struggled to incorporate these dynamic changes and had difficulty handling the competing risks a patient faces, like remaining in the hospital, being discharged, or dying. Failing to account for these ongoing changes meant that risk predictions were often inaccurate in real-time clinical settings.\nTherefore, the authors developed a “Dynamic Bayesian Model” to overcome these limitations. This new model combines a patient’s initial information (demographics and comorbidities) with daily dynamic changes in laboratory test values throughout their hospital stay. By incorporating these time-dependent measurements, the model achieved dramatic improvements in predictive accuracy compared to models that relied only on admission data. This allows the model to provide daily adjustments to the patient’s in-hospital mortality risk, making it a more effective tool for clinicians deciding on appropriate care and resource allocation."
  },
  {
    "objectID": "Summaries/et/paper5.html#methodology",
    "href": "Summaries/et/paper5.html#methodology",
    "title": "A Dynamic Bayesian Model for Identifying High-Mortality Risk in Hospitalized COVID-19 Patients",
    "section": "",
    "text": "Case selection\n\nPatients admitted to SUNY Downstate Medical Center, with COVID-19 related symptoms, between February 2020 and March 2020.\n\nSample\n\n553 PCR-positive patients included in the study.\nStratified into two groups: 200 patients who were discharged and 200 patients who died.\nData collected: demographic information, comorbidities, and laboratory test values.\n\nStatistical modeling approaches\n\nThe researchers conducted two main sets of analyses, both estimated using a Bayesian framework:\n\nDynamic Bayesian Markov Model\n\nThe primary goal was to develop a prognostic Markiv model that incorporates dynamic laboratory values with patients’ admission profiles.\nMarkov model accounts for competing risks (discharge vs. death) and allows for daily updates to mortality risk based on changing lab values.\nPredictor sets included:\n\nDemographics, comorbidities, admission, and lab values.\n\nThe model structure included:\n\nSecondary methodology: Logistic Regression Analysis\n\nThis analysis was conducted to determine factors most predictive of patient mortality but was specifically not intended to assess dynamic changes in mortality risk. This approach considered only patients’ outcomes, without factoring in the time taken for the outcome to occur.\n\nUnivariate Analysis (Baseline Comparison)\n\nBefore the multivariate analyses, univariate Cox survival analyses were used to illustrate the baseline patient characteristics and lab values upon admission that, individually, were the strongest determinants of risk.\n\n\n\nModel Validation\n\nInternal Validation: To assess the internal validity of the Markov model, the researchers performed k-fold cross-validation for each of the four regressions. The predictive accuracy increased significantly when dynamic test values were included, boosting accuracy from around 64–67% (for static models) to 83% (for the dynamic model).\nParameter Interpretation: Since the models were estimated in a Bayesian framework, the results are presented as probabilities representing the posterior probability that a given variable had an odds ratio exceeding one, removing the need for an arbitrary significance cutoff."
  },
  {
    "objectID": "Summaries/et/paper5.html#results-and-performance",
    "href": "Summaries/et/paper5.html#results-and-performance",
    "title": "A Dynamic Bayesian Model for Identifying High-Mortality Risk in Hospitalized COVID-19 Patients",
    "section": "",
    "text": "The performance results demonstrate that relying solely on static admission data (demographics, comorbidities, and initial lab values) yielded a mean predictive accuracy of only 64% to 67% across the initial regression sets. In contrast, incorporating dynamic changes in laboratory values (percentage changes relative to admission values) throughout hospitalization significantly boosted the model’s predictive power to an accuracy of 83%."
  },
  {
    "objectID": "Summaries/et/paper5.html#conclusion",
    "href": "Summaries/et/paper5.html#conclusion",
    "title": "A Dynamic Bayesian Model for Identifying High-Mortality Risk in Hospitalized COVID-19 Patients",
    "section": "",
    "text": "The article details a Dynamic Bayesian Markov Model that uses time-dependent changes in biomarkers to achieve 83% accuracy in identifying high-mortality risk in hospitalized COVID-19 patients, significantly outperforming static admission-only models (64–67% accuracy)."
  },
  {
    "objectID": "Summaries/nm/My_references.html",
    "href": "Summaries/nm/My_references.html",
    "title": "Bayesian Logistic Regression Capstone",
    "section": "",
    "text": "@article{Zeger2020, abstract = {The PCORI mission is to address questions about health care from the patients’ perspective, such as “What is my health status and its trajectory?” and “What are my treatment options and the expected benefits and harms of each?” The purpose of this PCORI-funded project is to make it easier for clinicians and patients to find valid answers to these and other clinical questions by using modern digital tools that support (1) learning from the experience of prior patients, and (2) translating what is learned to inform the decision at hand, taking into account each patient’s unique circumstances. For this project, we developed and implemented statistical methods called bayesian hierarchical models that combine existing data on past clinical experience from a reference population with new measurements for the individual. Clinicians currently use such methods when screening patients for disease. Modern technologies make it possible for this proven approach to extend far beyond its current use. The recent revolution in information technology has unleashed new types of health data, from DNA sequences to functional images of the brain to patient-reported outcomes. Furthermore, the electronic health record captures every patient’s sequence of health measurements, diagnoses, and treatments. The bayesian methods developed and reported on here combine even complex data to produce predictions about an individual patient’s health status, trajectory, and likely benefits and harms of interventions. In addition to developing novel methods, we facilitated their use by creating and locally disseminating a software package, OSLER inHealth, that will allow other researchers to apply this methodology. The software repository is open-source and includes the methodology developed as part of this research as well as other existing methods that facilitate individualized health prediction. We have tested the proposed methods and software on 3 case studies to (1) estimate the frequency with which various pathogens cause children’s pneumonia and predict which pathogen is likely to be causing a particular child’s pneumonia given her or his clinical data, potentially reducing unnecessary use of antibiotics; (2) infer whether a prostate cancer is indolent or aggressive for a patient under active surveillance; and (3) characterize the variation in multiple, time-varying symptoms of major mental disorders, including schizophrenia and depression, and then use this knowledge to provide patient-specific estimates of past and, likely, future trajectories. With this project, we have developed and demonstrated the value of combining even complex measurements on a population of patients, then translating this experience into more valid assessments of a new patient’s health status and trajectory. The model also supports inferences about the likely benefits and harms associated with available interventions. Copyright {} 2020. Johns Hopkins Bloomberg School of Public Health. All Rights Reserved.}, author = {Zeger, Scott L and Wu, Zhenke and Coley, Yates and Fojo, Anthony Todd and Carter, Bal and O’Brien, Katherine and Zandi, Peter and Cooke, Mary and Carey, Vince and Crainiceanu, Ciprian and Muscelli, John and Gherman, Adrian and Mekosh, Jason}, mendeley-groups = {CapStone_2025/CapStone_DS_2025}, number = {2020}, title = {{Using a Bayesian Approach to Predict Patients’ Health and Response to Treatment}}, url = {http://ovidsp.ovid.com/ovidweb.cgi?T=JS&PAGE=reference&D=medp&NEWS=N&AN=37708307}, year = {2020} }\n@article{Chatzimichail2023, abstract = {Medical diagnosis is the basis for treatment and management decisions in healthcare. Conventional methods for medical diagnosis commonly use established clinical criteria and fixed numerical thresholds. The limitations of such an approach may result in a failure to capture the intricate relations between diagnostic tests and the varying prevalence of diseases. To explore this further, we have developed a freely available specialized computational tool that employs Bayesian inference to calculate the posterior probability of disease diagnosis. This novel software comprises of three distinct modules, each designed to allow users to define and compare parametric and nonparametric distributions effectively. The tool is equipped to analyze datasets generated from two separate diagnostic tests, each performed on both diseased and nondiseased populations. We demonstrate the utility of this software by analyzing fasting plasma glucose, and glycated hemoglobin A1c data from the National Health and Nutrition Examination Survey. Our results are validated using the oral glucose tolerance test as a reference standard, and we explore both parametric and nonparametric distribution models for the Bayesian diagnosis of diabetes mellitus.}, author = {Chatzimichail, Theodora and Hatjimihail, Aristides T.}, doi = {10.3390/DIAGNOSTICS13193135,}, file = {:C:/Users/Namita/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chatzimichail, Hatjimihail - 2023 - A Bayesian Inference Based Computational Tool for Parametric and Nonparametric Medical Diagnosis.pdf:pdf}, issn = {20754418}, journal = {Diagnostics}, keywords = {Bayesian diagnosis,Bayesian inference,copula distribution,diabetes mellitus,kernel density estimator,likelihood,nonparametric distribution,parametric distribution,posterior probability,prior probability,probability density function}, mendeley-groups = {CapStone_2025}, month = {oct}, number = {19}, publisher = {Multidisciplinary Digital Publishing Institute (MDPI)}, title = {{A Bayesian Inference Based Computational Tool for Parametric and Nonparametric Medical Diagnosis}}, url = {https://pubmed.ncbi.nlm.nih.gov/37835877/}, volume = {13}, year = {2023} }\n@article{VandeSchoot2021, abstract = {Bayesian statistics is an approach to data analysis based on Bayes’ theorem, where available knowledge about parameters in a statistical model is updated with the information in observed data. The background knowledge is expressed as a prior distribution and combined with observational data in the form of a likelihood function to determine the posterior distribution. The posterior can also be used for making predictions about future events. This Primer describes the stages involved in Bayesian analysis, from specifying the prior and data models to deriving inference, model checking and refinement. We discuss the importance of prior and posterior predictive checking, selecting a proper technique for sampling from a posterior distribution, variational inference and variable selection. Examples of successful applications of Bayesian analysis across various research fields are provided, including in social sciences, ecology, genetics, medicine and more. We propose strategies for reproducibility and reporting standards, outlining an updated WAMBS (when to Worry and how to Avoid the Misuse of Bayesian Statistics) checklist. Finally, we outline the impact of Bayesian analysis on artificial intelligence, a major goal in the next decade.}, author = {van de Schoot, Rens and Depaoli, Sarah and King, Ruth and Kramer, Bianca and M{\"{a}}rtens, Kaspar and Tadesse, Mahlet G. and Vannucci, Marina and Gelman, Andrew and Veen, Duco and Willemsen, Joukje and Yau, Christopher}, doi = {10.1038/s43586-020-00001-2}, file = {:C:/Users/Namita/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/van de Schoot et al. - 2021 - Bayesian statistics and modelling.pdf:pdf}, issn = {2662-8449}, journal = {Nature Reviews Methods Primers}, keywords = {Scientific community,Statistics}, mendeley-groups = {CapStone_2025}, month = {jan}, number = {1}, pages = {1}, publisher = {Springer Nature}, title = {{Bayesian statistics and modelling}}, url = {https://www.nature.com/articles/s43586-020-00001-2}, volume = {1}, year = {2021} }\n@article{Klauenberg2015, abstract = {Regression is a common task in metrology and often applied to calibrate instruments, evaluate inter-laboratory comparisons or determine fundamental constants, for example. Yet, a regression model cannot be uniquely formulated as a measurement function, and consequently the Guide to the Expression of Uncertainty in Measurement (GUM) and its supplements are not applicable directly. Bayesian inference, however, is well suited to regression tasks, and has the advantage of accounting for additional a priori information, which typically robustifies analyses. Furthermore, it is anticipated that future revisions of the GUM shall also embrace the Bayesian view. Guidance on Bayesian inference for regression tasks is largely lacking in metrology. For linear regression models with Gaussian measurement errors this tutorial gives explicit guidance. Divided into three steps, the tutorial first illustrates how a priori knowledge, which is available from previous experiments, can be translated into prior distributions from a specific class. These prior distributions have the advantage of yielding analytical, closed form results, thus avoiding the need to apply numerical methods such as Markov Chain Monte Carlo. Secondly, formulas for the posterior results are given, explained and illustrated, and software implementations are provided. In the third step, Bayesian tools are used to assess the assumptions behind the suggested approach. These three steps (prior elicitation, posterior calculation, and robustness to prior uncertainty and model adequacy) are critical to Bayesian inference. The general guidance given here for Normal linear regression tasks is accompanied by a simple, but real-world, metrological example. The calibration of a flow device serves as a running example and illustrates the three steps. It is shown that prior knowledge from previous calibrations of the same sonic nozzle enables robust predictions even for extrapolations.}, author = {Klauenberg, Katy and W{\"{u}}bbeler, Gerd and Mickan, Bodo and Harris, Peter and Elster, Clemens}, doi = {10.1088/0026-1394/52/6/878}, file = {:C:/Users/Namita/Downloads/Klauenberg_2015_Metrologia_52_878.pdf:pdf;:C:/Users/Namita/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Klauenberg et al. - 2015 - A tutorial on Bayesian Normal linear regression.pdf:pdf}, issn = {16817575}, journal = {Metrologia}, keywords = {Bayesian inference,Gaussian measurement error,Normal inverse Gamma distribution,conjugate prior distribution,linear regression,prior knowledge,sonic nozzle calibration}, mendeley-groups = {CapStone_2025}, number = {6}, pages = {878–892}, publisher = {IOP Publishing}, title = {{A tutorial on Bayesian Normal linear regression}}, volume = {52}, year = {2015} }\n@article{DeLeeuw2012a, abstract = {In most research, linear regression analyses are performed without taking into account published results (i.e., reported summary statistics) of similar previous studies. Although the prior density in Bayesian linear regression could accommodate such prior knowledge, formal models for doing so are absent from the literature. The goal of this article is therefore to develop a Bayesian model in which a linear regression analysis on current data is augmented with the reported regression coefficients (and standard errors) of previous studies. Two versions of this model are presented. The first version incorporates previous studies through the prior density and is applicable when the current and all previous studies are exchangeable. The second version models all studies in a hierarchical structure and is applicable when studies are not exchangeable. Both versions of the model are assessed using simulation studies. Performance for each in estimating the regression coefficients is consistently superior to using current data alone and is close to that of an equivalent model that uses the data from previous studies rather than reported regression coefficients. Overall the results show that augmenting data with results from previous studies is viable and yields significant improvements in the parameter estimation. {} 2012 Copyright Taylor and Francis Group, LLC.}, author = {de Leeuw, Christiaan and Klugkist, Irene}, doi = {10.1080/00273171.2012.673957}, file = {:C:/Users/Namita/Downloads/Augmenting Data With Published Results in Bayesian Linear Regression.pdf:pdf}, issn = {00273171}, journal = {Multivariate Behavioral Research}, mendeley-groups = {CapStone_2025}, number = {3}, pages = {369–391}, title = {{Augmenting Data With Published Results in Bayesian Linear Regression}}, volume = {47}, year = {2012} }\n@article{Liu2013, abstract = {Background: A Bayesian clinical reasoning model was developed to predict an individual risk for cardiovascular disease (CVD) for desk-top reference. Methods: Three Bayesian models were constructed to estimate the CVD risk by sequentially incorporating demographic features (basic), six metabolic syndrome components (metabolic score) and conventional risk factors (enhanced model). By considering clinical weights (regression coefficients) of each model as normal distribution, individual risk can be predicted making allowance for uncertainty of clinical weights. A community-based cohort that enrolled 64,489 participants free of CVD at baseline and followed up over five years to ascertain newly diagnosed CVD cases during the period through 2000 to 2004 was used for the illustration of the three proposed models (full empirical data are available from website http://homepage.ntu.edu.tw/\\(\\sim\\)chenlin/CVD-prediction-data.rar). Results: The proposed models can be applied to predicting the CVD risk with any combination of risk factors. For a 47-year-old man, the five-year risk for CVD with the basic model was 11.2% (95% CI: 7.8%-15.6%). His metabolic syndrome score, leading to 1.488 of likelihood ratio, enhanced the risk for CVD up to 15.8% (95% CI: 11.0%-21.5%) and put him in highest deciles. As with the habit of smoking over 2 packs per-day and family history of CVD, yielding the likelihood ratios of 1.62 and 1.47, respectively, the risk was further raised to 30.9% (95% CI: 20.7%-39.8%). Conclusions: We demonstrate how to make individual risk prediction for CVD by incorporating routine information with a sequential Bayesian clinical reasoning approach. {} 2012 Elsevier Ireland Ltd.}, author = {Liu, Yi Ming and Chen, Sam Li Sheng and Yen, Amy Ming Fang and Chen, Hsiu Hsi}, doi = {10.1016/J.IJCARD.2012.05.016}, file = {:C:/Users/Namita/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu et al. - 2013 - Individual risk prediction model for incident cardiovascular disease A Bayesian clinical reasoning approach(9).pdf:pdf}, issn = {0167-5273}, journal = {International Journal of Cardiology}, keywords = {Bayes’ theorem,Bayesian,Cardiovascular disease,Likelihood ratio,Metabolic syndrome,Prediction model}, mendeley-groups = {CapStone_2025}, month = {sep}, number = {5}, pages = {2008–2012}, pmid = {22658349}, publisher = {Elsevier}, title = {{Individual risk prediction model for incident cardiovascular disease: A Bayesian clinical reasoning approach}}, url = {https://www.sciencedirect.com/science/article/pii/S0167527312006274}, volume = {167}, year = {2013} }"
  },
  {
    "objectID": "Summaries/nm/r code.html",
    "href": "Summaries/nm/r code.html",
    "title": "making subsets for each dataset",
    "section": "",
    "text": "NHANES DATASET -https://wwwn.cdc.gov/nchs/nhanes/continuousnhanes/default.aspx?BeginYear=2013 # loading packages\nlibrary(tidyverse) library(knitr) library(ggthemes) library(ggrepel) library(dslabs) library(Hmisc) library(dplyr) library(tidyr) library(forcats) library(ggplot2) library (“nhanesA”)\noptions(repos = c(CRAN = “https://cloud.r-project.org”))\ninstall.packages(“nhanesA”)\n\nmaking subsets for each dataset\n                   nhanesTables('EXAM', 2013)\n                   nhanesTables('QUESTIONNAIRE', 2013)\n                   nhanesTables('DEMOGRAPHICS', 2013)\n                   \n                   \nnhanesCodebook(“BMX_H”) nhanesCodebook(“SMQ_H”)\n# .xpt files read ( 2013–2014) bmx_h &lt;- nhanes(“BMX_H”) #Exam smq_h &lt;- nhanes(“SMQ_H”) #Quest demo_h &lt;- nhanes(“DEMO_H”) #Demo diq_h &lt;- nhanes(“DIQ_H”) #diabetes\n\n\nvariables of interest\nexam_sub &lt;- bmx_h %&gt;% select(SEQN, BMDBMIC) demo_sub &lt;- demo_h %&gt;% select(SEQN, RIDAGEYR, RIAGENDR, RIDRETH1, SDMVPSU, SDMVSTRA, WTMEC2YR) diq_sub &lt;- diq_h %&gt;% select (SEQN, DIQ240)\n\n\nNames of all variables selected for analysis\nnames(exam_sub) names(demo_sub) names(diq_sub)\n\n\nmerged dataframe\nmerged_data &lt;- exam_sub %&gt;% left_join(demo_sub, by = “SEQN”) %&gt;% left_join(diq_sub, by = “SEQN”) head(merged_data)\nnhanesCodebook(“DEMO_H”,‘RIDRETH1’) nhanesCodebook(“DEMO_H”,‘RIAGENDR’) nhanesCodebook(“DEMO_H”,‘RIDAGEYR’) nhanesCodebook(“DIQ_H”,“DIQ240”) nhanesCodebook(“BMX_H”,‘BMDBMIC’)"
  },
  {
    "objectID": "Summaries/nm/Summaries.html",
    "href": "Summaries/nm/Summaries.html",
    "title": "Bayesian Logistic Regression Capstone",
    "section": "",
    "text": "All 6 summaries and references & citation"
  },
  {
    "objectID": "Summaries/nm/Summaries.html#namitas-literature",
    "href": "Summaries/nm/Summaries.html#namitas-literature",
    "title": "Bayesian Logistic Regression Capstone",
    "section": "Namita’s Literature",
    "text": "Namita’s Literature"
  },
  {
    "objectID": "Summaries/nm/Summaries.html#introduction",
    "href": "Summaries/nm/Summaries.html#introduction",
    "title": "Bayesian Logistic Regression Capstone",
    "section": "Introduction",
    "text": "Introduction\n\nBayesian Hierarchical Model (Disease reclassification and prediction)\n\nWhat is the goal of the paper?\nThe authors develop a Bayesian hierarchical model for multivariate longitudinal data to predict health status, trajectories, and intervention effects at the individual level in the PCORI mission to address questions about health status from patients and clinicians.\nWhy is it important?\nHealthcare data (DNA sequences, functional images of the brain, patient-reported outcomes, and electronic health records with patients’ sequences of health measurements, diagnoses, and treatments) are complex, and the standard approaches are not adequate for clinical data analysis. Electronic health records (EHRs) could improve diagnostic accuracy and predict treatment effects. Visualizations of characteristics of posterior distributions can be immediately understood by clinicians and patients as relevant to their decision. Combining prior knowledge and patient data with evidence could predict the patient’s health status, trajectory, and/or likely benefits of interventions.\nHow is it solved?\nMethod: The authors applied Bayesian hierarchical regression for multivariate longitudinal patient data using open-source R-packages and developed 2 levels—time within person and persons within a population\nThe model combined exogenous (eg, age, clinical history) factors and endogenous (eg, current treatment) variables on the individual’s multivariate health measurements and the effects of health measurements at one time on subsequent interventions.\nThe model produced an estimate of the posterior distribution for each value of the predictor variable and an estimate of the marginal distribution of the regression coefficients for each coefficient that measures the outcome (health status) associated with its predictor variables. In a larger sample, the likelihood dominates the prior distribution for regression coefficients and Bayesian hierarchical model used a likelihood-based approach, used priors (prior laboratory and clinical trials data) that provided the assay sensitivities, which through the prior assumptions, made the model identifiable and the integration of Markov chain Monte Carlo (MCMC) estimates the posterior distributions, avoided missing data and complex outcome measurements.\nResults/limitations\nThree case studies: pneumonia etiology in children, prostate cancer, and mental disorders chosen for model development, identified low-risk patient population, reduced the risk of overtreatment, complications, adverse effects, and financial burden for patients (Disease Reclassification). Prostate cancer software was then implemented within the JHM HER.\nLimitation:\nModels were entirely parametric, and extensions to nonparametric or more flexible parametric models were recommended to improve approaches for neuroimage or genomic data.\nApplications:\n\nto scale a tool that addresses a particular unmet need across a larger, more diverse population of patients and clinicians\nuse in autoimmune diseases, sudden cardiac arrest, and diabetes.\nembed a collection of tools to acquire and use the most relevant information, agnostic to its level of measurement, to improve population and individual health decisions that cause better outcomes at more affordable costs. @Zeger2020\n\n\nBayesian Inference (parametric vs non-parametric)\n\nWhat is the goal of the paper?\nThe authors calculated the posterior probability of disease diagnosis and applied Bayesian inference to develop three modules comparing parametric (with a fixed set of parameters) and nonparametric distributions (which do not make a priori assumptions) by analyzing the National Health and Nutrition Examination Survey dataset from two separate diagnostic tests on both diseased and non-diseased populations.\nWhy is it important?\nMedical diagnosis, treatment, and management decisions are crucial, and conventional methods for diagnosis using clinical criteria and fixed numerical thresholds limit the capture of other information related to the intricate relationship between diagnostic tests and the varying prevalence of diseases. The probability distributions associated with quantitative diagnostic test outcomes often demonstrate some overlap between the diseased and nondiseased groups. The dichotomous method fails to capture the complexity and heterogeneity of disease presentations across diverse populations. The applicability of the normal distribution (conventional method) is critiqued, especially in dealing with clinical measurands having skewness, bimodality, or multimodality.\nHow is it solved?\nMethods: The Authors developed models employing Bayesian inference (Bayesian diagnostic approach) to calculate the posterior probability of @Liu2013"
  },
  {
    "objectID": "Summaries/nm/Comparing conventional and Bayesian.html",
    "href": "Summaries/nm/Comparing conventional and Bayesian.html",
    "title": "Bayesian Logistic Regression Capstone",
    "section": "",
    "text": "Comparing conventional and Bayesian workflows for clinical outcome prediction modelling with an exemplar cohort study of severe COVID-19 infection incorporating clinical biomarker test results\nRef: Sullivan, B., Barker, E., MacGregor, L. et al. Comparing conventional and Bayesian workflows for clinical outcome prediction modelling with an exemplar cohort study of severe COVID-19 infection incorporating clinical biomarker test results. BMC Med Inform Decis Mak 25, 123 (2025). https://doi.org/10.1186/s12911-025-02955-3\nProblem: Data curation is challenging as clinical data are heterogeneous in multiple ways. Biomarkers are recorded for different reasons. If missing data imputation is performed, it raises another decision point on whether to impute the continuous or the transformed categorical data.\nAim: Estimating predictive risk factors for disease outcomes with explainable statistical models is desirable for clinical use and decision making. The author provide a guide for modern Bayesian approaches for joint risk factor analysis and variable selection.\nStudy design: Retrospective observational cohort design, lab data linked to the patient data for laboratory markers and clinical outcomes from three hospitals in the Southwest region of England, UK on adults admitted between March to October 2020 and tested positive for SARS CoV-2 by PCR.\nMethod: Analyze range of laboratory blood marker values, Develop cross-validated logistic regression prediction models using the candidate biomarkers, highlighting biomarkers worthy of future research. Employed selection techniques, comparing LASSO frequentist method and Projective Prediction approach on Bayesian logistic regression models with horseshoe priors to illustrate the process of creating a reduced model. They considered models deliver good prediction performance with a small amount of biomarker data.\nVairables (1) Predictors: Includes a variety of clinical severity indices, lab biomarkers (microbiological, immunological, haematological and biochemistry) as parameters used as predictive variables in the regression models.\n\nOutcome: The primary prediction outcome was death or transfer to the ICU.\n\nData management: Continuous biomarkers were trannsfromed into categorical variables (reference ranges for clinical use).\nStaistical calcukation: Analytics carried out using the R statistical language using- Standard logistic regression analyses used the R Stats GLM package (v4.4.1); LASSO analyses, GLMnet (v4.1.8); and for Bayesian analyses, BRMS (v2.22.0) and ProjPred (v2.8.0).\nBefore running full regression models, the independent contribution of individual biomarkers were examined in the training dataset predicting ICU entry or death via standard logistic regressions and Bayesian logistic regressions with either a flat (aka uniform) or horseshoe prior and calculated p-values and odds ratios for each biomarker.\nPer biomarker, patients with and without the outcome were separated and then these groups were shuffled and split into 5 equal subgroups. These groups were then paired at random, to ensure training and test datasets have the same proportion of patients with a severe outcome as in full the sample for that biomarker. This was to improve the chance of convergence for biomarkers with high data missingness and only complete cases of training data available for each biomarker were considered for the study.\nEach individual biomarker model including age and gender (except univariate age and gender models) were compared against a standard model including only age and gender. Regressions were fit using all associated dummy variables for a given biomarker (e.g. ‘Mild’, ‘Moderate’, ‘Severe’) using ‘Normal’ as the reference.\nAnalysis using all valid biomarker data: After individual biomarker evaluation, logistic regression models considering all valid biomarkers (Prediction using individual variables section) and demographic variables were fit to the data and the predictions were tested via internal and external validation using the stratified cross-validation procedures.\nAnalysis using reduced variable models: Considering that eben though a model using all biomarker data may have strong predictive power, but clinically a strong prediction with the least amount of biomarkers might save on time, money and other resources, they used LASSO and Bayesian Projective Prediction methodologies to choose reduced variable models to predict COVID-19 severe outcomes.\nTo estimate variability in model performance and allow comparison between models, we compute inter-quantile AUC difference ranges using 5-fold 20-repeat cross-validation of models\nReduced variable models: LASSO and projective prediction performed for creation of reduced models with fewer biomarkers.\nModel performance evaluation and dissemination They chose to peform cross-validated estimates of AUC, sensitivity, and specificity and the Inter-quartile intervals over these measures.\nRecommendations: Categorization is worth critical consideration in model planning Reduced number of variables Imputation Bayesian approaches should include that coefficients estimated via Bayes should on average deliver better predictive performance than standard GLM"
  },
  {
    "objectID": "Summaries/aw/paper2_summary.html",
    "href": "Summaries/aw/paper2_summary.html",
    "title": "Summary: An Introduction to Using Bayesian Linear Regression with Clinical Data",
    "section": "",
    "text": "Scott A. Baldwin and Michael J. Larson (2017)\n\n\nTraditional statistics training in psychology emphasizes frequentist methods, and researchers often feel unsure about Bayesian alternatives. This paper aims to introduce Bayesian linear regression in a clear and practical way, using real clinical data to help researchers understand and apply Bayesian tools.\n\n\n\nThe authors walk readers through setting up practical Bayesian regression models. They explain how to choose and justify priors, run models using software (like R and Stan), check convergence, and interpret results—including posterior distributions, credible intervals, and predictive outputs. They also compare Bayesian and frequentist approaches and offer useful model-comparison tools. The article includes EEG-anxiety data, and it comes with the data and R code for easy replication.\n\n\n\nThe paper shows how Bayesian regression can be applied step-by-step and how outputs like posterior distributions and interval estimates offer more intuitive, flexible insights than traditional methods. Including data and code makes the method transparent and replicable, while model diagnostics (like convergence checks) ensure reliability.\n\n\n\nThe paper is introductory; it doesn’t dive into advanced modeling extensions or deeply mathematical derivations. It focuses on a single case study (EEG data and anxiety) so broader generalization requires additional application. Expertise is still needed to adapt methods to other complex datasets.\n\n\n\nThe authors illustrate methods using clinical EEG data related to error-related negativity (ERN) and trait anxiety. They provide both the actual dataset and R/Stan code for readers to replicate and learn directly from the example."
  },
  {
    "objectID": "Summaries/aw/paper2_summary.html#problem-the-article-is-addressing",
    "href": "Summaries/aw/paper2_summary.html#problem-the-article-is-addressing",
    "title": "Summary: An Introduction to Using Bayesian Linear Regression with Clinical Data",
    "section": "",
    "text": "Traditional statistics training in psychology emphasizes frequentist methods, and researchers often feel unsure about Bayesian alternatives. This paper aims to introduce Bayesian linear regression in a clear and practical way, using real clinical data to help researchers understand and apply Bayesian tools."
  },
  {
    "objectID": "Summaries/aw/paper2_summary.html#how-it-has-been-solved",
    "href": "Summaries/aw/paper2_summary.html#how-it-has-been-solved",
    "title": "Summary: An Introduction to Using Bayesian Linear Regression with Clinical Data",
    "section": "",
    "text": "The authors walk readers through setting up practical Bayesian regression models. They explain how to choose and justify priors, run models using software (like R and Stan), check convergence, and interpret results—including posterior distributions, credible intervals, and predictive outputs. They also compare Bayesian and frequentist approaches and offer useful model-comparison tools. The article includes EEG-anxiety data, and it comes with the data and R code for easy replication."
  },
  {
    "objectID": "Summaries/aw/paper2_summary.html#results",
    "href": "Summaries/aw/paper2_summary.html#results",
    "title": "Summary: An Introduction to Using Bayesian Linear Regression with Clinical Data",
    "section": "",
    "text": "The paper shows how Bayesian regression can be applied step-by-step and how outputs like posterior distributions and interval estimates offer more intuitive, flexible insights than traditional methods. Including data and code makes the method transparent and replicable, while model diagnostics (like convergence checks) ensure reliability."
  },
  {
    "objectID": "Summaries/aw/paper2_summary.html#limitations",
    "href": "Summaries/aw/paper2_summary.html#limitations",
    "title": "Summary: An Introduction to Using Bayesian Linear Regression with Clinical Data",
    "section": "",
    "text": "The paper is introductory; it doesn’t dive into advanced modeling extensions or deeply mathematical derivations. It focuses on a single case study (EEG data and anxiety) so broader generalization requires additional application. Expertise is still needed to adapt methods to other complex datasets."
  },
  {
    "objectID": "Summaries/aw/paper2_summary.html#datasets",
    "href": "Summaries/aw/paper2_summary.html#datasets",
    "title": "Summary: An Introduction to Using Bayesian Linear Regression with Clinical Data",
    "section": "",
    "text": "The authors illustrate methods using clinical EEG data related to error-related negativity (ERN) and trait anxiety. They provide both the actual dataset and R/Stan code for readers to replicate and learn directly from the example."
  },
  {
    "objectID": "Summaries/aw/paper4_summary.html",
    "href": "Summaries/aw/paper4_summary.html",
    "title": "Summary: Dropout as a Bayesian Approximation—Representing Model Uncertainty in Deep Learning",
    "section": "",
    "text": "Yarin Gal & Zoubin Ghahramani (2016)\n\n\nDeep neural networks achieve strong predictive accuracy but typically do not quantify uncertainty. This limitation is critical in applications like healthcare or autonomous systems, where overconfidence can lead to serious consequences. Traditional Bayesian neural networks can model uncertainty but are often computationally expensive and hard to train.\n\n\n\nThe authors demonstrate that standard dropout, a common regularization technique, can serve as an approximate Bayesian inference method. By keeping dropout active during both training and testing and performing multiple stochastic forward passes, the resulting distribution of predictions can be interpreted as samples from a posterior. This provides an efficient, scalable way to estimate model uncertainty without altering model architecture.\n\n\n\nThe paper shows that “Monte Carlo Dropout” (MC-Dropout) yields well-calibrated uncertainty estimates and competitive accuracy on tasks such as image classification (MNIST, CIFAR-10) and regression. The method improves out-of-distribution detection, active learning, and decision-making under uncertainty.\n\n\n\nThe uncertainty estimates depend on the dropout rate and can underestimate epistemic uncertainty for small or biased datasets. The approach is an approximation, not a full Bayesian posterior, so results may deviate from exact inference. Additional calibration steps may still be required.\n\n\n\nExperiments include standard image datasets (MNIST, CIFAR-10) and regression benchmarks from the UCI repository. Predictions are made using multiple forward passes with dropout enabled to approximate posterior mean and variance."
  },
  {
    "objectID": "Summaries/aw/paper4_summary.html#problem-the-article-is-addressing",
    "href": "Summaries/aw/paper4_summary.html#problem-the-article-is-addressing",
    "title": "Summary: Dropout as a Bayesian Approximation—Representing Model Uncertainty in Deep Learning",
    "section": "",
    "text": "Deep neural networks achieve strong predictive accuracy but typically do not quantify uncertainty. This limitation is critical in applications like healthcare or autonomous systems, where overconfidence can lead to serious consequences. Traditional Bayesian neural networks can model uncertainty but are often computationally expensive and hard to train."
  },
  {
    "objectID": "Summaries/aw/paper4_summary.html#how-it-has-been-solved",
    "href": "Summaries/aw/paper4_summary.html#how-it-has-been-solved",
    "title": "Summary: Dropout as a Bayesian Approximation—Representing Model Uncertainty in Deep Learning",
    "section": "",
    "text": "The authors demonstrate that standard dropout, a common regularization technique, can serve as an approximate Bayesian inference method. By keeping dropout active during both training and testing and performing multiple stochastic forward passes, the resulting distribution of predictions can be interpreted as samples from a posterior. This provides an efficient, scalable way to estimate model uncertainty without altering model architecture."
  },
  {
    "objectID": "Summaries/aw/paper4_summary.html#results",
    "href": "Summaries/aw/paper4_summary.html#results",
    "title": "Summary: Dropout as a Bayesian Approximation—Representing Model Uncertainty in Deep Learning",
    "section": "",
    "text": "The paper shows that “Monte Carlo Dropout” (MC-Dropout) yields well-calibrated uncertainty estimates and competitive accuracy on tasks such as image classification (MNIST, CIFAR-10) and regression. The method improves out-of-distribution detection, active learning, and decision-making under uncertainty."
  },
  {
    "objectID": "Summaries/aw/paper4_summary.html#limitations",
    "href": "Summaries/aw/paper4_summary.html#limitations",
    "title": "Summary: Dropout as a Bayesian Approximation—Representing Model Uncertainty in Deep Learning",
    "section": "",
    "text": "The uncertainty estimates depend on the dropout rate and can underestimate epistemic uncertainty for small or biased datasets. The approach is an approximation, not a full Bayesian posterior, so results may deviate from exact inference. Additional calibration steps may still be required."
  },
  {
    "objectID": "Summaries/aw/paper4_summary.html#datasets",
    "href": "Summaries/aw/paper4_summary.html#datasets",
    "title": "Summary: Dropout as a Bayesian Approximation—Representing Model Uncertainty in Deep Learning",
    "section": "",
    "text": "Experiments include standard image datasets (MNIST, CIFAR-10) and regression benchmarks from the UCI repository. Predictions are made using multiple forward passes with dropout enabled to approximate posterior mean and variance."
  },
  {
    "objectID": "Summaries/aw/paper6_summary.html",
    "href": "Summaries/aw/paper6_summary.html",
    "title": "Summary: Bayesian Model Averaging: A Practical Review",
    "section": "",
    "text": "Jennifer A. Hoeting, David Madigan, Adrian E. Raftery, & Chris T. Volinsky (1999)\n\n\nIn applied statistics and data science, analysts often face uncertainty about which model best represents the data. Standard practice is to select a single “best” model using criteria like AIC or BIC, but this ignores model uncertainty and leads to overconfident inferences and biased predictions. The paper addresses how to properly account for model uncertainty in statistical inference.\n\n\n\nThe authors propose Bayesian Model Averaging (BMA), a framework that combines predictions from multiple models weighted by their posterior probabilities. Instead of committing to a single model, BMA integrates over all plausible models using Bayes’ theorem. This approach accounts for uncertainty in both model selection and parameter estimation, yielding more robust predictions.\n\n\n\nBMA is shown to improve predictive performance and reduce overfitting across a variety of domains, including regression, classification, and time-series modeling. The method provides posterior model probabilities and model-averaged parameter estimates, offering a principled way to handle model uncertainty. The paper also presents examples and computational techniques for implementing BMA, such as Markov Chain Monte Carlo Model Composition (MC³).\n\n\n\nBMA can be computationally demanding when the number of candidate models is large, as it requires calculating and storing posterior probabilities for each. Its success also depends on reasonable prior specifications over both models and parameters. Simplified approximations (like Occam’s window) may introduce bias but are often necessary for feasibility.\n\n\n\nThe authors apply BMA to several real-world and simulated datasets, including linear regression examples and ecological modeling case studies. The paper focuses on demonstrating methodology rather than a specific dataset, emphasizing reproducible model-averaging workflows and sensitivity analysis."
  },
  {
    "objectID": "Summaries/aw/paper6_summary.html#problem-the-article-is-addressing",
    "href": "Summaries/aw/paper6_summary.html#problem-the-article-is-addressing",
    "title": "Summary: Bayesian Model Averaging: A Practical Review",
    "section": "",
    "text": "In applied statistics and data science, analysts often face uncertainty about which model best represents the data. Standard practice is to select a single “best” model using criteria like AIC or BIC, but this ignores model uncertainty and leads to overconfident inferences and biased predictions. The paper addresses how to properly account for model uncertainty in statistical inference."
  },
  {
    "objectID": "Summaries/aw/paper6_summary.html#how-it-has-been-solved",
    "href": "Summaries/aw/paper6_summary.html#how-it-has-been-solved",
    "title": "Summary: Bayesian Model Averaging: A Practical Review",
    "section": "",
    "text": "The authors propose Bayesian Model Averaging (BMA), a framework that combines predictions from multiple models weighted by their posterior probabilities. Instead of committing to a single model, BMA integrates over all plausible models using Bayes’ theorem. This approach accounts for uncertainty in both model selection and parameter estimation, yielding more robust predictions."
  },
  {
    "objectID": "Summaries/aw/paper6_summary.html#results",
    "href": "Summaries/aw/paper6_summary.html#results",
    "title": "Summary: Bayesian Model Averaging: A Practical Review",
    "section": "",
    "text": "BMA is shown to improve predictive performance and reduce overfitting across a variety of domains, including regression, classification, and time-series modeling. The method provides posterior model probabilities and model-averaged parameter estimates, offering a principled way to handle model uncertainty. The paper also presents examples and computational techniques for implementing BMA, such as Markov Chain Monte Carlo Model Composition (MC³)."
  },
  {
    "objectID": "Summaries/aw/paper6_summary.html#limitations",
    "href": "Summaries/aw/paper6_summary.html#limitations",
    "title": "Summary: Bayesian Model Averaging: A Practical Review",
    "section": "",
    "text": "BMA can be computationally demanding when the number of candidate models is large, as it requires calculating and storing posterior probabilities for each. Its success also depends on reasonable prior specifications over both models and parameters. Simplified approximations (like Occam’s window) may introduce bias but are often necessary for feasibility."
  },
  {
    "objectID": "Summaries/aw/paper6_summary.html#datasets",
    "href": "Summaries/aw/paper6_summary.html#datasets",
    "title": "Summary: Bayesian Model Averaging: A Practical Review",
    "section": "",
    "text": "The authors apply BMA to several real-world and simulated datasets, including linear regression examples and ecological modeling case studies. The paper focuses on demonstrating methodology rather than a specific dataset, emphasizing reproducible model-averaging workflows and sensitivity analysis."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk Using NHANES 2013–2014 Data",
    "section": "",
    "text": "Slides: slides.html (Edit slides.qmd.)"
  },
  {
    "objectID": "index.html#aims",
    "href": "index.html#aims",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk Using NHANES 2013–2014 Data",
    "section": "Aims",
    "text": "Aims\nThe present study employs Bayesian logistic regression to predict diabetes status and examine the relationships between diabetes and key predictors, including body mass index (BMI), age (≥20 years), sex, and race. Using retrospective data from the 2013–2014 NHANES survey, the analysis accounts for the study’s complex sampling design, which involves stratification, clustering, and the oversampling of specific sub-populations rather than simple random sampling. The Bayesian framework is applied to address common analytical challenges such as missing data, complete case bias, and data separation, thereby improving the robustness and reliability of inference compared to traditional logistic regression methods."
  },
  {
    "objectID": "index.html#bayesian-logistic-regression",
    "href": "index.html#bayesian-logistic-regression",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk Using NHANES 2013–2014 Data",
    "section": "Bayesian Logistic Regression",
    "text": "Bayesian Logistic Regression\nThe Bayesian framework integrates prior knowledge with observed data to generate posterior distributions, allowing parameters to be interpreted directly in probabilistic terms.\nUnlike traditional frequentist approaches that yield single-point estimates and p-values, Bayesian methods represent parameters as random variables with full probability distributions.\nThis provides greater flexibility, incorporates parameter uncertainty, and produces credible intervals that directly quantify the probability that a parameter lies within a given range."
  },
  {
    "objectID": "index.html#model-structure",
    "href": "index.html#model-structure",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk Using NHANES 2013–2014 Data",
    "section": "Model Structure",
    "text": "Model Structure\nBayesian logistic regression models the log-odds of a binary outcome as a linear combination of predictors:\n\\[\n\\text{logit}(P(Y = 1)) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_k X_k\n\\]\nwhere\n\n\\(P(Y = 1)\\) is the probability of the event of interest,\n\\(\\beta_0\\) is the intercept (log-odds when all predictors are zero), and\n\\(\\beta_j\\) represents the effect of predictor \\(X_j\\) on the log-odds of the outcome, holding other predictors constant.\n\nIn the Bayesian framework, model parameters (\\(\\boldsymbol{\\beta}\\)) are treated as random variables and assigned prior distributions that reflect existing knowledge or plausible ranges before observing the data. After incorporating the observed evidence, the priors are updated through Bayes’ theorem (Leeuw and Klugkist 2012; Klauenberg et al. 2015):\n\\[\n\\text{Posterior} \\propto \\text{Likelihood} \\times \\text{Prior}\n\\]\n\nLikelihood: represents the probability of the observed data given the model parameters—it captures how well different parameter values explain the data.\nPrior: expresses beliefs or existing information about the parameters before observing the data.\nPosterior: combines both, representing the updated distribution of parameter values after accounting for the data.\n\nThis formulation allows uncertainty to propagate naturally through the model, producing posterior distributions for each coefficient that can be directly interpreted as probabilities."
  },
  {
    "objectID": "index.html#prior-specification",
    "href": "index.html#prior-specification",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk Using NHANES 2013–2014 Data",
    "section": "Prior Specification",
    "text": "Prior Specification\nWeakly informative priors were used to regularize estimation without imposing strong assumptions:\n\nRegression Coefficients: \\(N(0, 2.5)\\), providing gentle regularization while allowing substantial variation in plausible effects (Gelman et al. 2008; Vande Schoot et al. 2021).\nIntercept: Student’s t-distribution prior, \\(t(3, 0, 10)\\) (Schoot et al. 2013; Vande Schoot et al. 2021), which has\n\n3 degrees of freedom (heavy tails to allow occasional large effects),\nmean 0 (no bias toward positive or negative effects), and\nscale 10 (broad range of possible values).\n\n\nSuch priors help stabilize estimation in the presence of multicollinearity, limited sample size, or potential outliers."
  },
  {
    "objectID": "index.html#advantages-of-bayesian-logistic-regression",
    "href": "index.html#advantages-of-bayesian-logistic-regression",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk Using NHANES 2013–2014 Data",
    "section": "Advantages of Bayesian Logistic Regression",
    "text": "Advantages of Bayesian Logistic Regression\n\nUncertainty Quantification: Produces full posterior distributions instead of single estimates.\nCredible Intervals: Provide the range within which a parameter lies with a specified probability (e.g., 95%).\nFlexible Priors: Allow integration of expert knowledge or findings from prior studies.\nProbabilistic Predictions: Posterior predictive distributions yield direct probabilities for new or future observations.\nModel Evaluation: PPCs assess how well simulated outcomes reproduce observed data."
  },
  {
    "objectID": "index.html#posterior-predictions",
    "href": "index.html#posterior-predictions",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk Using NHANES 2013–2014 Data",
    "section": "Posterior Predictions",
    "text": "Posterior Predictions\nPosterior distributions of regression coefficients were used to estimate the probability of the outcome for given predictor values. This allows statements such as: &gt; Given the predictors, the probability of the outcome lies between X% and Y%.\nPosterior predictions account for two key sources of uncertainty:\n\nParameter Uncertainty: Variability in estimated model coefficients.\nPredictive Uncertainty: Variability in possible future outcomes given those parameters.\n\nIn Bayesian analysis, all unknown quantities—coefficients, means, variances, or probabilities—are treated as random variables described by their posterior distributions."
  },
  {
    "objectID": "index.html#model-evaluation-and-diagnostics",
    "href": "index.html#model-evaluation-and-diagnostics",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk Using NHANES 2013–2014 Data",
    "section": "Model Evaluation and Diagnostics",
    "text": "Model Evaluation and Diagnostics\nModel quality and convergence were assessed using standard Bayesian diagnostics:\n\nPosterior Sampling: Conducted via Markov Chain Monte Carlo (MCMC) using the No-U-Turn Sampler (NUTS), a variant of Hamiltonian Monte Carlo (HMC) (Austin et al. 2021). Four chains were run with sufficient warm-up iterations to ensure convergence.\nConvergence Metrics: The potential scale reduction factor (\\(\\hat{R}\\)) and effective sample size (ESS) were used to verify stability and mixing across chains.\nAutocorrelation Checks: Ensured independence between successive draws.\nPosterior Predictive Checks (PPCs): Compared simulated outcomes to observed data to evaluate fit.\nBayesian \\(R^2\\): Quantified the proportion of variance explained by predictors, incorporating posterior uncertainty."
  },
  {
    "objectID": "index.html#data-preparation",
    "href": "index.html#data-preparation",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk Using NHANES 2013–2014 Data",
    "section": "Data Preparation",
    "text": "Data Preparation\nThis study used publicly available 2013–2014 NHANES data published by the CDC’s National Center for Health Statistics (National Center for Health Statistics (NCHS) 2014). Three component files were utilized: DEMO_H (demographics), BMX_H (body measures), and DIQ_H (diabetes questionnaire). Each file was imported in .XPT format using the haven package in R, and merged using the unique participant identifier SEQN to create a single adult analytic dataset (age ≥ 20 years).\nAll variables were coerced to consistent numeric or factor types prior to merging to ensure atomic columns suitable for survey-weighted analysis and modeling. The use of SEQN preserved respondent integrity across datasets and ensured accurate record linkage. This preprocessing step standardized variable formats and minimized inconsistencies between files.\nData wrangling, cleaning, and merging were performed in R using a combination of base functions and tidyverse packages. Bayesian logistic regression modeling was later implemented using the brms interface to Stan, allowing probabilistic inference within a reproducible workflow that accommodated the NHANES complex survey design and missing data considerations.\n\nData Import and Merging\n\n\nCode\nmerged_data &lt;- readRDS(\"data/merged_2013_2014.rds\")\n\nmerged_n &lt;- nrow(merged_data)\n\n\nThe merged dataset contains 10,175 participants. It integrates demographic, examination, and diabetes questionnaire data. We then restrict the sample to adults (age ≥ 20) to define the analytic cohort used in subsequent analyses. A small proportion of records have missing values in BMI and diabetes status, which will be addressed later through multiple imputation.\n\n\n\nPreview of merged NHANES 2013–2014 dataset limited to analysis variables (source columns only).\n\n\nRIDAGEYR\nBMXBMI\nRIAGENDR\nRIDRETH1\nDIQ010\n\n\n\n\n69\n26.7\n1\n4\n1\n\n\n54\n28.6\n1\n3\n1\n\n\n72\n28.9\n1\n3\n1\n\n\n9\n17.1\n1\n3\n2\n\n\n73\n19.7\n2\n3\n2\n\n\n56\n41.7\n1\n1\n2\n\n\n0\nNA\n1\n3\nNA\n\n\n61\n35.7\n2\n3\n2\n\n\n42\nNA\n1\n2\n2\n\n\n56\n26.5\n2\n3\n2\n\n\n\n\n\n\n\nVariable Definitions\n\nResponse Variable:\ndiabetes_dx (binary) represents a Type 2 diabetes diagnosis, excluding gestational diabetes. It was derived from DIQ010 (“Doctor told you have diabetes”), while DIQ050 (insulin use) was excluded to prevent treatment-related confounding.\nPredictor Variables:\n\nBMXBMI – Body Mass Index (kg/m^2), treated as continuous and categorized into six BMI classes (bmi_cat).\n\nRIDAGEYR – Age (continuous, 20–80 years)\n\nRIAGENDR – Sex (factor, two levels)\n\nRIDRETH1 – Ethnicity (factor, five levels)\n\n\n\n\nCode\nvar_tbl &lt;- tribble(\n  ~Variable,   ~Description,                                                                                         ~Type,         ~Origin,\n  \"diabetes_dx\",\"Type 2 diabetes diagnosis (1 = Yes, 0 = No) derived from DIQ010; gestational diabetes excluded.\",  \"Categorical\", \"Derived from DIQ010\",\n  \"age\",        \"Age in years.\",                                                                                    \"Continuous\",  \"NHANES RIDAGEYR\",\n  \"bmi\",        \"Body Mass Index (kg/m^2) computed from measured height and weight.\",                               \"Continuous\",  \"NHANES BMXBMI\",\n  \"bmi_cat\",    \"BMI categories: Underweight, Normal, Overweight, Obesity I–III (Normal is reference in models).\",  \"Categorical\", \"Derived from bmi\",\n  \"sex\",        \"Sex of participant (Male, Female).\",                                                               \"Categorical\", \"NHANES RIAGENDR\",\n  \"race\",       \"Race/ethnicity collapsed to four levels: White, Black, Hispanic, Other.\",                          \"Categorical\", \"Derived from RIDRETH1\",\n  \"WTMEC2YR\",   \"Examination sample weight for Mobile Examination Center participants.\",                            \"Weight\",      \"NHANES design\",\n  \"SDMVPSU\",    \"Primary Sampling Unit used for variance estimation in the complex survey design.\",                 \"Design\",      \"NHANES design\",\n  \"SDMVSTRA\",   \"Stratum identifier used to define strata for the complex survey design.\",                          \"Design\",      \"NHANES design\",\n  \"age_c\",      \"Centered and standardized age (z-score).\",                                                         \"Continuous\",  \"Derived from age\",\n  \"bmi_c\",      \"Centered and standardized BMI (z-score).\",                                                         \"Continuous\",  \"Derived from bmi\"\n)\n\nkbl(\n  var_tbl,\n  caption = \"Variable Descriptions: Adult Analytic Dataset\",\n  align = c(\"l\",\"l\",\"l\",\"l\"),\n  escape = TRUE   # or just remove this line, TRUE is the default\n) %&gt;%\n  kable_styling(full_width = FALSE, position = \"center\",\n                bootstrap_options = c(\"striped\",\"hover\")) %&gt;%\n  group_rows(\"Analysis variables\", 1, 6) %&gt;%\n  group_rows(\"Survey design variables\", 7, 9) %&gt;%\n  group_rows(\"Derived variables\", 10, 11)\n\n\n\nVariable Descriptions: Adult Analytic Dataset\n\n\nVariable\nDescription\nType\nOrigin\n\n\n\n\nAnalysis variables\n\n\ndiabetes_dx\nType 2 diabetes diagnosis (1 = Yes, 0 = No) derived from DIQ010; gestational diabetes excluded.\nCategorical\nDerived from DIQ010\n\n\nage\nAge in years.\nContinuous\nNHANES RIDAGEYR\n\n\nbmi\nBody Mass Index (kg/m^2) computed from measured height and weight.\nContinuous\nNHANES BMXBMI\n\n\nbmi_cat\nBMI categories: Underweight, Normal, Overweight, Obesity I–III (Normal is reference in models).\nCategorical\nDerived from bmi\n\n\nsex\nSex of participant (Male, Female).\nCategorical\nNHANES RIAGENDR\n\n\nrace\nRace/ethnicity collapsed to four levels: White, Black, Hispanic, Other.\nCategorical\nDerived from RIDRETH1\n\n\nSurvey design variables\n\n\nWTMEC2YR\nExamination sample weight for Mobile Examination Center participants.\nWeight\nNHANES design\n\n\nSDMVPSU\nPrimary Sampling Unit used for variance estimation in the complex survey design.\nDesign\nNHANES design\n\n\nSDMVSTRA\nStratum identifier used to define strata for the complex survey design.\nDesign\nNHANES design\n\n\nDerived variables\n\n\nage_c\nCentered and standardized age (z-score).\nContinuous\nDerived from age\n\n\nbmi_c\nCentered and standardized BMI (z-score).\nContinuous\nDerived from bmi\n\n\n\n\n\n\n\nStudy Design and Survey-Weighted Analysis\nThe National Health and Nutrition Examination Survey (NHANES) employs a complex, multistage probability sampling design with stratification, clustering, and oversampling of specific demographic groups (for example, racial/ethnic minorities and older adults) to produce nationally representative estimates of the U.S. population.\nSurvey design variables — primary sampling units (SDMVPSU), strata (SDMVSTRA), and examination sample weights (WTMEC2YR) — were retained to account for this complex design. These variables were applied to adjust for unequal probabilities of selection, nonresponse, and oversampling, ensuring valid standard errors, unbiased prevalence estimates, and generalizable population-level inference.\nA survey-weighted logistic regression (design-based) model was used to evaluate the association between diabetes status (diabetes_dx, binary outcome) and key predictors: body mass index (bmi), age (age), sex (sex), and race/ethnicity (race). Diabetes was defined using DIQ010 (“Doctor told you have diabetes”) and coded as 0/1, with DIQ050 (insulin use) excluded to avoid treatment-related confounding.\nCovariates included:\n- age (continuous; centered as age_c, categorized 20–80 years)\n- bmi (continuous; centered as bmi_c, and categorized by BMI class bmi_cat)\n- sex (male, female)\n- race (four ethnicity levels: White, Black, Hispanic, Other)\nThis approach accounts for NHANES’ complex sampling design, producing unbiased parameter estimates and valid inference for U.S. adults.\n\n\n\n\n\n\n\nStep\nDescription\n\n\n\n\nWeighting\nUsed the survey package to calculate weighted means for key variables (e.g., age and diabetes status) and to estimate design effects and effective sample size for the complex survey design.\n\n\nStandardization\nCentered and standardized BMI and age (bmi_c, age_c) for use in regression models.\n\n\nAge Categorization\nNot implemented in the analytic dataset (continuous age retained). Reference retained for potential descriptive grouping (20–&lt;30, 30–&lt;40, 40–&lt;50, 50–&lt;60, 60–&lt;70, 70–80).\n\n\nBMI Categorization\nRecoded as: &lt;18.5 (Underweight), 18.5–&lt;25 (Normal), 25–&lt;30 (Overweight), 30–&lt;35 (Obesity I), 35–&lt;40 (Obesity II), ≥40 (Obesity III).\n\n\nEthnicity Recoding\nRIDRETH1 recoded as: 1 = Mexican American, 2 = Other Hispanic, 3 = Non-Hispanic White, 4 = Non-Hispanic Black, 5 = Other/Multi; then NH White set as the reference level (five analytical levels retained).\n\n\nSpecial Codes\nTransformed nonresponse codes (e.g., 3, 7, 9) to NA. These missing codes were evaluated for potential nonrandom patterns (MAR/MNAR).\n\n\nMissing Data\nRetained and visualized missing values (primarily in BMI and diabetes status) to assess their pattern and informativeness before multiple imputation.\n\n\nFinal Dataset\nCreated the cleaned analytic dataset (adult) using Non-Hispanic White and Male as reference groups for modeling, preserving NHANES survey design variables (WTMEC2YR, SDMVPSU, SDMVSTRA).\n\n\n\n\n\nAdult Cohort (NHANES 2013–2014)\nThis analysis uses data from the 2013–2014 cycle of the National Health and Nutrition Examination Survey (NHANES), a nationally representative survey of U.S. adults. Individuals aged 20 years and older with available diabetes outcome data and key demographic and anthropometric variables were included in the analytic cohort.\nThree NHANES components were merged to construct the dataset:\n\nDEMO_H: demographic characteristics and sample design variables\n\nBMX_H: measured height and weight used to calculate BMI\n\nDIQ_H: doctor-diagnosed diabetes status\n\nThe combined dataset contained n = 5,769 adults after applying inclusion criteria and preparing variables for analysis. The primary outcome was doctor-diagnosed diabetes (diabetes_dx). Predictor variables included:\n\nAge, standardized as a z-score for interpretability\n\nBMI, standardized as a z-score\n\nSex (Female vs. Male)\n\nRace/Ethnicity, with Non-Hispanic White as the reference group\n\nSurvey design features—strata, primary sampling units, and sample weights—were retained to support design-based logistic regression and ensure appropriate population-level inference.\n\n\nAdult Cohort Definition\n\n\nCode\n# NHANES survey design object for the adult analytic cohort\n\nnhanes_design_adult &lt;- survey::svydesign(\nid      = ~SDMVPSU,\nstrata  = ~SDMVSTRA,\nweights = ~WTMEC2YR,\nnest    = TRUE,\ndata    = adult\n)\n\n# Quick weighted checks\n\nsurvey::svymean(~age, nhanes_design_adult, na.rm = TRUE)\n\n\n      mean     SE\nage 47.496 0.3805\n\n\nCode\nsurvey::svymean(~diabetes_dx, nhanes_design_adult, na.rm = TRUE)\n\n\n                mean     SE\ndiabetes_dx 0.089016 0.0048\n\n\nCode\n# Design effect and effective sample size for `diabetes_dx`\n\nv_hat &lt;- as.numeric(survey::svyvar(~diabetes_dx, nhanes_design_adult, na.rm = TRUE))\np_hat &lt;- mean(adult$diabetes_dx, na.rm = TRUE)\nn_obs &lt;- nrow(adult)\nv_srs &lt;- p_hat * (1 - p_hat) / n_obs\ndeff  &lt;- v_hat / v_srs\n\nn_total &lt;- sum(weights(nhanes_design_adult), na.rm = TRUE)\ness     &lt;- as.numeric(n_total / deff)\n\ncat(\"Design effect for diabetes_dx:\", round(deff, 2), \"\\n\")\n\n\nDesign effect for diabetes_dx: 4759.91 \n\n\nCode\ncat(\"Effective sample size for diabetes_dx:\", round(ess), \"\\n\")\n\n\nEffective sample size for diabetes_dx: 48142 \n\n\nDescriptive statistics for continuous and categorical variables are presented below.\n\n\nCode\n# Keep only analytic variables for Table 1\ntbl1_dat &lt;- adult %&gt;%\n  transmute(\n    age,\n    bmi,\n    bmi_cat,\n    sex,\n    race,\n    diabetes_dx = factor(diabetes_dx, levels = c(0, 1), labels = c(\"No\", \"Yes\"))\n  )\n\n# Continuous summaries: N, missing, mean, sd, min, max\ncont_vars &lt;- c(\"age\", \"bmi\")\n\ncont_sum &lt;- tbl1_dat %&gt;%\n  select(all_of(cont_vars)) %&gt;%\n  pivot_longer(everything(), names_to = \"Variable\", values_to = \"value\") %&gt;%\n  group_by(Variable) %&gt;%\n  summarise(\n    N       = sum(!is.na(value)),\n    Missing = sum(is.na(value)),\n    Mean    = round(mean(value, na.rm = TRUE), 2),\n    SD      = round(sd(value, na.rm = TRUE), 2),\n    Min     = round(min(value, na.rm = TRUE), 1),\n    Max     = round(max(value, na.rm = TRUE), 1),\n    .groups = \"drop\"\n  )\n\n# Categorical summaries: counts and percents\ncat_vars &lt;- c(\"sex\", \"race\", \"diabetes_dx\", \"bmi_cat\")\n\ncat_sum &lt;- tbl1_dat %&gt;%\n  mutate(across(all_of(cat_vars),\n                ~ forcats::fct_explicit_na(as.factor(.x), na_level = \"(Missing)\"))) %&gt;%\n  select(all_of(cat_vars)) %&gt;%\n  pivot_longer(everything(), names_to = \"Variable\", values_to = \"Level\") %&gt;%\n  count(Variable, Level, name = \"n\") %&gt;%\n  group_by(Variable) %&gt;%\n  mutate(pct = round(100 * n / sum(n), 1)) %&gt;%\n  ungroup() %&gt;%\n  arrange(Variable, desc(n))\n\n# Render tables\nkable(cont_sum,\n      caption = \"Table 1a. Continuous variables (age, BMI): N, missing, mean (SD), range.\") %&gt;%\n  kable_styling(full_width = FALSE)\n\n\n\nTable 1a. Continuous variables (age, BMI): N, missing, mean (SD), range.\n\n\nVariable\nN\nMissing\nMean\nSD\nMin\nMax\n\n\n\n\nage\n5769\n0\n49.11\n17.56\n20.0\n80.0\n\n\nbmi\n5520\n249\n29.10\n7.15\n14.1\n82.9\n\n\n\n\n\nCode\nkable(cat_sum,\n      caption = \"Table 1b. Categorical variables (sex, race, diabetes_dx, bmi_cat): counts and percentages.\") %&gt;%\n  kable_styling(full_width = FALSE)\n\n\n\nTable 1b. Categorical variables (sex, race, diabetes_dx, bmi_cat): counts and percentages.\n\n\nVariable\nLevel\nn\npct\n\n\n\n\nbmi_cat\n25–&lt;30\n1768\n30.6\n\n\nbmi_cat\n18.5–&lt;25\n1579\n27.4\n\n\nbmi_cat\n30–&lt;35\n1145\n19.8\n\n\nbmi_cat\n35–&lt;40\n519\n9.0\n\n\nbmi_cat\n≥40\n419\n7.3\n\n\nbmi_cat\n(Missing)\n249\n4.3\n\n\nbmi_cat\n&lt;18.5\n90\n1.6\n\n\ndiabetes_dx\nNo\n4974\n86.2\n\n\ndiabetes_dx\nYes\n618\n10.7\n\n\ndiabetes_dx\n(Missing)\n177\n3.1\n\n\nrace\nNH White\n2472\n42.8\n\n\nrace\nNH Black\n1177\n20.4\n\n\nrace\nOther/Multi\n845\n14.6\n\n\nrace\nMexican American\n767\n13.3\n\n\nrace\nOther Hispanic\n508\n8.8\n\n\nsex\nFemale\n3011\n52.2\n\n\nsex\nMale\n2758\n47.8\n\n\n\n\n\nTable 1a and 1b summarize the analytic variables included in subsequent models. Mean age and BMI values indicate an adult cohort spanning a wide range of body composition, while categorical summaries confirm balanced sex representation and sufficient sample sizes across race/ethnicity categories. These variables were standardized and used as predictors in all modeling frameworks (analytic cohort N = 5,769 adults ≥ 20 years).\nThe analytic adult cohort (N = 5,769) includes standardized variables for age and BMI (age_c, bmi_c), categorical indicators for sex and race/ethnicity (race), and a binary doctor-diagnosed diabetes variable (diabetes_dx). This is a clean dataset with all NAs removed.\n\n\nCode\n# Visual structure and type overview\nplot_intro(adult, title = \"Adult Dataset: Variable Types and Completeness\")\n\n\n\n\n\nThe visual overview indicates that 75% of variables are continuous and 25% are categorical, with no completely missing columns. Approximately 92.7% of rows are fully complete, and only 1.3% of observations contain missing values, suggesting minimal data loss prior to imputation.\n\n\n\n\n\n\nMissing Data Summary\n\n\nCode\n# Visualize missing data pattern\nplot_missing(adult, title = \"Missing Data Pattern (Adult Dataset)\")\n\n\n\n\n\nMissing data were minimal across analytic variables. BMI-related fields (bmi, bmi_c, bmi_cat) showed ~4.3% missingness, and diabetes_dx showed ~3.1%. All demographic and survey design variables were complete, indicating that missingness was limited to health-related measures and appropriate for multiple imputation.\n\n\n\n\n\n\nCode\n# Summarize missingness for key analysis variables\n\nmiss_tbl &lt;- tibble::tibble(\nVariable    = c(\"bmi\", \"diabetes_dx\"),\nMissing_n   = c(sum(is.na(adult_eda$bmi)), sum(is.na(adult_eda$diabetes_dx))),\nMissing_pct = round(c(mean(is.na(adult_eda$bmi)), mean(is.na(adult_eda$diabetes_dx))) * 100, 1)\n)\n\nknitr::kable(\nmiss_tbl,\ncaption = \"Missingness for Key Analysis Variables.\"\n)\n\n\n\nMissingness for Key Analysis Variables.\n\n\nVariable\nMissing_n\nMissing_pct\n\n\n\n\nbmi\n249\n4.3\n\n\ndiabetes_dx\n177\n3.1\n\n\n\n\n\nOverall missingness was low (~7.3%). Gaps were concentrated in bmi (n = 249) and diabetes_dx (n = 177), while demographic and design variables were complete. This limited pattern of missingness is consistent with a Missing At Random (MAR) mechanism and likely reflects reduced participation in the physical examination component among certain adults.\n\n\nExploratory Data Analysis (EDA)\n\nMissing Data\nOverall missingness in the analytic variables was low, with approximately 3–4% missing data in age or BMI. This level of missingness supported the use of multiple imputation during data preparation and ensured that complete-case analyses remained consistent for model comparison.\n\n\nOutcome Distribution\nDoctor-diagnosed diabetes prevalence in the adult cohort was approximately 11%, which aligns with national estimates for U.S. adults during the same time period.\n\n\nPredictor Patterns\nExploratory analyses revealed several expected patterns:\n\nAge: Diabetes prevalence increased steadily across older age groups.\n\nBMI: Individuals with diabetes tended to have higher BMI values, with clear separation between diabetes and non-diabetes groups.\n\nSex: Females exhibited slightly lower diabetes prevalence compared to males.\n\nRace/Ethnicity: Prevalence was higher among Non-Hispanic Black, Hispanic, and “Other” racial/ethnic groups compared to Non-Hispanic White adults.\n\nThese distributional patterns reflect established epidemiologic relationships and support the suitability of the dataset for logistic regression modeling.\n\n\nEDA Visuals\n\n\nCode\n# Age distribution (analytic adult)\nggplot(adult, aes(x = age)) +\n  geom_histogram(binwidth = 5, color = \"white\") +\n  labs(title = \"Distribution of Age (≥20 years)\", x = \"Age (years)\", y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\nDistribution of age among adults aged ≥20 years. The sample spans 20–80 years, with peak representation between 30 and 50 years and a gradual decline in older age groups, reflecting a balanced adult cohort suitable for regression modeling.\n\n\n\n\n\n\nCode\n# Diabetes outcome distribution\nggplot(adult, aes(x = factor(diabetes_dx, levels = c(0,1), labels = c(\"No\",\"Yes\")))) +\n  geom_bar() +\n  labs(title = \"Diabetes Outcome Distribution (≥20 years)\", x = \"Diabetes (No/Yes)\", y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\nDistribution of diabetes outcomes among adults aged ≥20 years. Most participants reported no diabetes diagnosis (No), while approximately 11% had diabetes (Yes) and 3% had missing responses, reflecting expected population prevalence and limited outcome missingness.\n\n\n\n\n\n\nCode\n# BMI category distribution\nggplot(adult, aes(x = bmi_cat)) +\n  geom_bar(color = \"white\", fill = \"skyblue\") +\n  labs(title = \"Distribution of BMI Categories (≥20 years)\", x = \"BMI Category\", y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\nDistribution of BMI categories among adults aged ≥20 years. The majority of participants fell within the overweight (25–&lt;30) and obese (≥30) ranges, with fewer individuals classified as underweight (&lt;18.5). This distribution aligns with national trends in adult body composition, supporting the dataset’s representativeness for metabolic health analyses.\n\n\n\n\n\n\nCode\n# BMI by diabetes outcome (boxplot)\n# (You can’t use boxplot with categorical y, so revert to numeric BMI here)\nggplot(adult, aes(x = factor(diabetes_dx, levels = c(0,1), labels = c(\"No\",\"Yes\")), y = bmi)) +\n  geom_boxplot(fill = \"lightblue\") +\n  labs(title = \"BMI by Diabetes Diagnosis (≥20 years)\", x = \"Diabetes (No/Yes)\", y = \"BMI (numeric)\") +\n  theme_minimal()\n\n\n\n\n\nDistribution of BMI by diabetes diagnosis among adults aged ≥20 years. Participants with diabetes (Yes) show a higher median BMI and greater variability compared to those without diabetes (No), supporting the established positive association between obesity and diabetes risk.\n\n\n\n\n\n\nCode\n# Diabetes by race (dodged bars)\nggplot(adult, aes(x = race, fill = factor(diabetes_dx, levels = c(0,1), labels = c(\"No\",\"Yes\")))) +\n  geom_bar(position = \"dodge\") +\n  labs(title = \"Diabetes Diagnosis by race/Ethnicity (≥20 years)\",\n       x = \"race/Ethnicity (race)\", y = \"Count\", fill = \"Diabetes\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\nDiabetes diagnosis by race/ethnicity among adults aged ≥20 years. Non-Hispanic Black and Hispanic participants show higher proportions of diabetes diagnoses compared with Non-Hispanic White participants, reflecting known disparities in diabetes prevalence across racial and ethnic groups."
  },
  {
    "objectID": "index.html#modeling-frameworks",
    "href": "index.html#modeling-frameworks",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk Using NHANES 2013–2014 Data",
    "section": "Modeling Frameworks",
    "text": "Modeling Frameworks\nTwo modeling frameworks were compared using identical predictors—standardized age, BMI, sex, and race—and the binary outcome diabetes_dx:\n\nSurvey-weighted logistic regression to account for the NHANES complex sampling design,\nBayesian logistic regression with weakly informative priors to quantify parameter uncertainty.\n\n\nSurvey-Weighted Logistic Regression\nThe NHANES 2013–2014 data use a complex, multistage probability design involving strata (SDMVSTRA), primary sampling units (PSUs; SDMVPSU), and examination weights (WTMEC2YR) to ensure nationally representative estimates (National Center for Health Statistics (NCHS) 2014).\nEstimates are population-weighted using NHANES survey design variables (WTMEC2YR, SDMVSTRA, SDMVPSU). Odds ratios are reported per one standard deviation (1 SD) increase in age and BMI, with reference groups Male and White.\n\n\nCode\nadult_clean &lt;- adult %&gt;%\n  dplyr::mutate(\n    sex   = forcats::fct_drop(sex),\n    race = forcats::fct_drop(race),\n    age_c = as.numeric(age_c),\n    bmi_c = as.numeric(bmi_c)\n  ) %&gt;%\n  dplyr::filter(\n    !is.na(diabetes_dx),\n    !is.na(age_c),\n    !is.na(bmi_c),\n    !is.na(sex),\n    !is.na(race)\n  )\n\n\nBelow is a structure of the analytic dataset used for regression modeling, showing variable names, types, and sample values (N = 5,349).\n\n\nCode\nstr(adult_clean[, c(\"diabetes_dx\",\"sex\",\"race\",\"age_c\",\"bmi_c\")])\n\n\n'data.frame':   5349 obs. of  5 variables:\n $ diabetes_dx: num  1 1 1 0 0 0 0 0 0 1 ...\n $ sex        : Factor w/ 2 levels \"Male\",\"Female\": 1 1 1 2 1 2 2 1 2 1 ...\n $ race       : Factor w/ 5 levels \"NH White\",\"Mexican American\",..: 4 1 1 1 2 1 1 1 1 1 ...\n $ age_c      : num  1.132 0.278 1.303 1.36 0.392 ...\n $ bmi_c      : num  -0.3359 -0.0703 -0.0283 -1.3144 1.761 ...\n\n\n\n\nCode\noptions(survey.lonely.psu = \"adjust\")\n\nnhanes_design_adult &lt;- survey::svydesign(\n  id      = ~SDMVPSU,\n  strata  = ~SDMVSTRA,\n  weights = ~WTMEC2YR,\n  nest    = TRUE,\n  data    = adult_clean\n)\n\nsvy_fit &lt;- survey::svyglm(\n  diabetes_dx ~ age_c + bmi_c + sex + race,\n  design = nhanes_design_adult,\n  family = quasibinomial()\n)\n\nsvy_or &lt;- broom::tidy(svy_fit, conf.int = TRUE) %&gt;%\n  dplyr::mutate(\n    OR  = exp(estimate),\n    LCL = exp(conf.low),\n    UCL = exp(conf.high)\n  ) %&gt;%\n  dplyr::select(term, OR, LCL, UCL, p.value) %&gt;%\n  dplyr::filter(term != \"(Intercept)\")\n\n\n\n\nCode\nknitr::kable(svy_or)\n\n\n\n\nTable 1: Survey-weighted logistic regression: odds ratios (OR) and 95% confidence intervals for diabetes diagnosis among adults (NHANES 2013–2014).\n\n\n\n\n\n\nterm\nOR\nLCL\nUCL\np.value\n\n\n\n\nage_c\n3.0292807\n2.6967690\n3.4027912\n0.0000000\n\n\nbmi_c\n1.8853571\n1.6526296\n2.1508579\n0.0000039\n\n\nsexFemale\n0.5281132\n0.4104905\n0.6794397\n0.0003857\n\n\nraceMexican American\n2.0358434\n1.4850041\n2.7910081\n0.0008262\n\n\nraceOther Hispanic\n1.5915182\n1.1664529\n2.1714810\n0.0087119\n\n\nraceNH Black\n1.6689718\n1.1605895\n2.4000450\n0.0116773\n\n\nraceOther/Multi\n2.3270527\n1.5451752\n3.5045697\n0.0014331\n\n\n\n\n\n\n\n\n\nInterpretation\nage_c and bmi_c emerge as the strongest predictors of diabetes in the NHANES 2013–2014 adult cohort.\n- For age_c, each 1-SD increase is associated with an odds ratio of 3.03 (95% CI: 2.70–3.40), indicating that older adults have nearly three times higher odds of diagnosed diabetes compared to younger adults.\n- For bmi_c, each 1-SD increase corresponds to an odds ratio of 1.89 (95% CI: 1.65–2.15), meaning higher BMI substantially elevates diabetes risk.\nSex differences were also observed.\n- Compared to males, females had significantly lower odds of diabetes (OR = 0.53, 95% CI: 0.41–0.68), consistent with known sex-related differences in metabolic and cardiometabolic profiles.\nRacial and ethnic disparities in diabetes risk were pronounced.\nUsing Non-Hispanic Whites as the reference group:\n- Mexican American adults had about 2.04× higher odds of diabetes (95% CI: 1.49–2.79).\n- Other Hispanic adults showed 1.59× higher odds (95% CI: 1.17–2.17).\n- Non-Hispanic Black adults had 1.67× higher odds (95% CI: 1.16–2.40).\n- Other/Multi-racial adults exhibited 2.33× higher odds (95% CI: 1.55–3.50).\nAll predictors were statistically significant (p &lt; 0.05), and the direction and magnitude of effects reflect well-established demographic and clinical patterns in diabetes risk.\n\n\n\nBayesian Logistic Regression\nBayesian logistic regression was used to quantify parameter uncertainty and compare posterior estimates with the survey-weighted model. Weakly informative priors were applied to regularize estimates while preserving flexibility in inference.\nModel Specifications: - Family: Bernoulli with logit link\n- Data: adult_imp1 (N = 5,592)\n- Chains: 4 (2,000 iterations each; 1,000 warmup)\n- Adaptation delta: 0.95\n- Weights: Normalized NHANES examination weights (wt_norm, mean ≈ 1.00, SD ≈ 0.79)\n- Predictors: Standardized age, BMI, sex, and race\n\nDefine Model and Priors\n\n\nCode\nfml_bayes &lt;- diabetes_dx | weights(wt_norm) ~ age_c + bmi_c + sex + race\n\npriors &lt;- c(\n  brms::set_prior(\"normal(0, 2.5)\", class = \"b\"),\n  brms::set_prior(\"student_t(3, 0, 10)\", class = \"Intercept\")\n)\n\n\n\n\nCode\nprior_draws &lt;- tibble::tibble(\nterm = rep(c(\"Age (per 1 SD)\", \"BMI (per 1 SD)\"), each = 4000),\nvalue = c(\nstats::rnorm(4000, mean = 0, sd = 2.5),\nstats::rnorm(4000, mean = 0, sd = 2.5)\n)\n)\n\nggplot2::ggplot(prior_draws, ggplot2::aes(x = value, fill = term)) +\nggplot2::geom_density(alpha = 0.5) +\nggplot2::theme_minimal() +\nggplot2::labs(\ntitle = \"Prior Distributions for Age and BMI Coefficients\",\nx = \"Coefficient value\",\ny = \"Density\",\nfill = NULL\n)\n\n\n\n\n\nPrior distributions for standardized age and BMI coefficients, assuming Normal(0, 2.5) priors. These weakly informative priors constrain extreme coefficient values while allowing flexibility in posterior estimation, ensuring regularization without strong bias.\n\n\n\n\n\n\nData Preparation Using Multiple Imputation by Chained Equations (MICE)\nMultiple Imputation by Chained Equations (MICE) was used to address missing data in the analytic dataset (Stef van Buuren and Groothuis-Oudshoorn 2011; S. van Buuren 2012). MICE iteratively imputes each incomplete variable using regression models based on the available data, producing multiple completed datasets that incorporate uncertainty from the imputation process. Estimates across imputations are then combined using Rubin’s rules to obtain final parameter estimates and confidence intervals.\nAs an alternative to full Bayesian joint modeling, MICE offers a flexible and robust approach for handling missingness through chained regression equations. For large sample sizes (n ≥ 400), even when a single variable contains substantial missingness (up to 75%), MICE performs reliably under skewed, multimodal, or heavy-tailed distributions without materially affecting mean-structure estimation (S. van Buuren 2012).\nIn this study, continuous variables were imputed using regression-based methods (age via normal linear regression and BMI via predictive mean matching to preserve distributional shape). Categorical variables (sex and race) were imputed using logistic and polytomous regression models. The outcome variable (diabetes_dx) was not imputed and was used only as a predictor during the imputation process. Twenty imputations were generated to reduce Monte Carlo error and ensure stable variance estimation.\nThe chained-equations process demonstrated stable convergence across iterations, supporting reliable estimation of missing BMI values (and age, where applicable). After imputation, the completed analytic dataset contained n = 5,592 adults with all key predictors available for downstream modeling, including the Bayesian logistic regression framework.\n\n\nCode\nadult_imp1 &lt;- mice::complete(imp, 1) %&gt;%\n  dplyr::mutate(\n    age_c  = as.numeric(scale(age)),\n    bmi_c  = as.numeric(scale(bmi)),\n    wt_norm = WTMEC2YR / mean(WTMEC2YR, na.rm = TRUE),\n    race = forcats::fct_relevel(race, \"NH White\"),\n    sex  = forcats::fct_relevel(sex,  \"Male\")\n  ) %&gt;%\n  dplyr::filter(\n    !is.na(diabetes_dx),\n    !is.na(age_c),\n    !is.na(bmi_c),\n    !is.na(sex),\n    !is.na(race)\n  ) %&gt;%\n  droplevels()\n\nstr(adult_imp1)\n\n\n'data.frame':   5592 obs. of  11 variables:\n $ diabetes_dx: num  1 1 1 0 0 0 0 0 0 0 ...\n $ age        : num  69 54 72 73 56 61 42 56 65 26 ...\n $ bmi        : num  26.7 28.6 28.9 19.7 41.7 35.7 23.6 26.5 22 20.3 ...\n $ sex        : Factor w/ 2 levels \"Male\",\"Female\": 1 1 1 2 1 2 1 2 1 2 ...\n $ race       : Factor w/ 5 levels \"NH White\",\"Mexican American\",..: 4 1 1 1 2 1 3 1 1 1 ...\n $ WTMEC2YR   : num  13481 24472 57193 65542 25345 ...\n $ SDMVPSU    : num  1 1 1 2 1 1 2 1 2 2 ...\n $ SDMVSTRA   : num  112 108 109 116 111 114 106 112 112 113 ...\n $ age_c      : num  1.132 0.278 1.303 1.36 0.392 ...\n $ bmi_c      : num  -0.3332 -0.0676 -0.0256 -1.3118 1.7639 ...\n $ wt_norm    : num  0.339 0.616 1.44 1.65 0.638 ...\n\n\n\n\nCode\nadult_long &lt;- adult_imp1 %&gt;%\ndplyr::select(bmi_c, age_c) %&gt;%\ntidyr::pivot_longer(\ncols = dplyr::everything(),\nnames_to = \"Coefficient\",\nvalues_to = \"Value\"\n)\n\nggplot2::ggplot(adult_long, ggplot2::aes(x = Value, fill = Coefficient)) +\nggplot2::geom_density(alpha = 0.5) +\nggplot2::theme_minimal() +\nggplot2::labs(\ntitle = \"Distributions for Standardized Age and BMI (adult_imp1)\",\nx = \"Standardized value (z-score)\",\ny = \"Density\",\nfill = \"Coefficient\"\n)\n\n\n\n\n\nDistribution of standardized age (age_c) and BMI (bmi_c) in the imputed dataset (adult_imp1). Both variables were mean-centered and scaled (z-scores) for inclusion in regression models. The overlapping density curves indicate approximate normality and comparable variance, supporting suitability for standardized coefficient estimation.\n\n\n\n\n\n\n\n\nAfter applying exclusions and resolving the small amount of missingness (≈3–4%), both the survey-weighted and Bayesian models were fit to the same complete-case adult dataset (n = 5,769).\n\n\nFit the Model\n\n\nCode\npriors &lt;- c(\n  brms::set_prior(\"normal(0, 2.5)\", class = \"b\"),\n  brms::set_prior(\"student_t(3, 0, 10)\", class = \"Intercept\")\n)\n\nbayes_fit &lt;- brms::brm(\n  formula = diabetes_dx | weights(wt_norm) ~ age_c + bmi_c + sex + race,\n  data    = adult_imp1,\n  family  = bernoulli(link = \"logit\"),\n  prior   = priors,\n  chains  = 4, iter = 2000, seed = 123,\n  control = list(adapt_delta = 0.95),\n  refresh = 0\n)\n\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 11.5 seconds.\nChain 2 finished in 10.4 seconds.\nChain 3 finished in 10.9 seconds.\nChain 4 finished in 11.3 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 11.0 seconds.\nTotal execution time: 44.5 seconds.\n\n\nCode\nsummary(bayes_fit)\n\n\n Family: bernoulli \n  Links: mu = logit \nFormula: diabetes_dx | weights(wt_norm) ~ age_c + bmi_c + sex + race \n   Data: adult_imp1 (Number of observations: 5592) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept              -2.66      0.09    -2.83    -2.50 1.00     3548     3512\nage_c                   1.10      0.06     0.98     1.22 1.00     2349     2618\nbmi_c                   0.63      0.05     0.54     0.72 1.00     3327     2826\nsexFemale              -0.66      0.10    -0.86    -0.47 1.00     3668     3124\nraceMexicanAmerican     0.69      0.17     0.34     1.03 1.00     3657     2821\nraceOtherHispanic       0.43      0.25    -0.07     0.89 1.00     4242     3014\nraceNHBlack             0.53      0.15     0.23     0.83 1.00     3809     3012\nraceOtherDMulti         0.81      0.19     0.45     1.18 1.00     3948     2809\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nBayesian logistic regression model fit summary for diabetes diagnosis (diabetes_dx) with standardized predictors (age, BMI, sex, and race) and normalized NHANES weights. All four MCMC chains (4,000 post-warmup draws) converged successfully (R̂ ≈ 1.00), indicating stable estimation across parameters.\n\n\nCode\n# Extract fixed effects and convert to odds ratios\nbayes_fixef &lt;- brms::fixef(bayes_fit, summary = TRUE)\n\nbayes_or &lt;- bayes_fixef %&gt;%\n  as.data.frame() %&gt;%\n  tibble::rownames_to_column(\"term\") %&gt;%\n  dplyr::mutate(\n    OR  = exp(Estimate),\n    LCL = exp(Q2.5),\n    UCL = exp(Q97.5)\n  )\n\n\n\n\nPosterior Odd Ratios (Main Results)\n\n\nCode\nknitr::kable(\ndplyr::mutate(bayes_or, dplyr::across(c(OR, LCL, UCL), ~ round(.x, 2)))\n)\n\n\n\n\nTable 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nEstimate\nEst.Error\nQ2.5\nQ97.5\nOR\nLCL\nUCL\n\n\n\n\nIntercept\n-2.6633187\n0.0868613\n-2.8341138\n-2.4958967\n0.07\n0.06\n0.08\n\n\nage_c\n1.0968784\n0.0618886\n0.9783744\n1.2200119\n2.99\n2.66\n3.39\n\n\nbmi_c\n0.6282273\n0.0467939\n0.5366821\n0.7199012\n1.87\n1.71\n2.05\n\n\nsexFemale\n-0.6624742\n0.1034594\n-0.8645869\n-0.4660003\n0.52\n0.42\n0.63\n\n\nraceMexicanAmerican\n0.6898163\n0.1710160\n0.3432716\n1.0298163\n1.99\n1.41\n2.80\n\n\nraceOtherHispanic\n0.4252184\n0.2458586\n-0.0669575\n0.8870126\n1.53\n0.94\n2.43\n\n\nraceNHBlack\n0.5307334\n0.1524774\n0.2283617\n0.8328511\n1.70\n1.26\n2.30\n\n\nraceOtherDMulti\n0.8143883\n0.1876762\n0.4467512\n1.1763335\n2.26\n1.56\n3.24\n\n\n\n\n\n\n\n\n\n\nCode\n# Combine survey-weighted and Bayesian OR results\nsvy_tbl &lt;- svy_or %&gt;%\n  mutate(Model = \"Survey-weighted\")\n\nbayes_tbl &lt;- bayes_or %&gt;%\n  mutate(Model = \"Bayesian\") %&gt;%\n  filter(term != \"Intercept\")\n\n# Long format\nall_tbl &lt;- bind_rows(svy_tbl, bayes_tbl) %&gt;%\n  mutate(\n    term = case_when(\n      grepl(\"bmi\", term, ignore.case = TRUE) ~ \"BMI (per 1 SD)\",\n      grepl(\"age\", term, ignore.case = TRUE) ~ \"Age (per 1 SD)\",\n      grepl(\"^sexFemale$\", term) ~ \"Female (vs. Male)\",\n# ---- Race recoding (handles ALL messy variants) ----\n      grepl(\"Mexican\", term, ignore.case = TRUE) ~ \"Mexican American (vs. White)\",\n      grepl(\"Black\", term, ignore.case = TRUE) ~ \"Black (vs. White)\",\n      grepl(\"Hispanic\", term, ignore.case = TRUE) ~ \"Hispanic (vs. White)\",\n      grepl(\"Other\", term, ignore.case = TRUE) ~ \"Other/Multi (vs. White)\",\n      TRUE ~ term\n    ),\n    OR_CI = sprintf(\"%.2f (%.2f – %.2f)\", OR, LCL, UCL)\n  ) %&gt;%\n  select(Model, term, OR_CI)\n\n# Wide format\nvertical_tbl &lt;- all_tbl %&gt;%\n  pivot_wider(\n    names_from = Model,\n    values_from = OR_CI\n  ) %&gt;%\n  arrange(term)\n\n# Output\nknitr::kable(vertical_tbl,\n             align = \"lcc\",\n             caption = \"Odds Ratios (95% CI) Across Survey-weighted and Bayesian Models\")\n\n\n\n\nTable 3: Odds Ratios (95% CI) for Key Predictors Across Survey-weighted and Bayesian Models.\n\n\n\n\nOdds Ratios (95% CI) Across Survey-weighted and Bayesian Models\n\n\n\n\n\n\n\nterm\nSurvey-weighted\nBayesian\n\n\n\n\nAge (per 1 SD)\n3.03 (2.70 – 3.40)\n2.99 (2.66 – 3.39)\n\n\nBMI (per 1 SD)\n1.89 (1.65 – 2.15)\n1.87 (1.71 – 2.05)\n\n\nBlack (vs. White)\n1.67 (1.16 – 2.40)\n1.70 (1.26 – 2.30)\n\n\nFemale (vs. Male)\n0.53 (0.41 – 0.68)\n0.52 (0.42 – 0.63)\n\n\nHispanic (vs. White)\n1.59 (1.17 – 2.17)\n1.53 (0.94 – 2.43)\n\n\nMexican American (vs. White)\n2.04 (1.49 – 2.79)\n1.99 (1.41 – 2.80)\n\n\nOther/Multi (vs. White)\n2.33 (1.55 – 3.50)\n2.26 (1.56 – 3.24)\n\n\n\n\n\n\n\n\n\nAge and BMI show strong positive associations with diabetes (credible intervals exclude 1).\nFemale sex shows lower odds than male (protective factor).\nNon-White racial groups have higher odds compared with Whites, consistent with known disparities.\nAll model parameters exhibit well-defined, unimodal posteriors with narrow credible intervals.\n\n\n\nPosterior Coefficients (log-odds scale)\nIntercept\n- b_Intercept = –2.66 (SD = 0.09, 95% CrI: –2.83 to –2.50)\n- Represents baseline log-odds of diabetes for:\nMale, Non-Hispanic White, average age, average BMI\n- Corresponds to a baseline probability of ~6.5%.\n\nAge (per 1 SD)\n- b_age_c = 1.10 (SD = 0.06, 95% CrI: 0.98 to 1.22)\n- Strong positive effect on diabetes risk.\n- Odds ratio: exp(1.10) ≈ 2.99\n- A 1-SD increase in age nearly triples the odds of diabetes.\n- Credible interval excludes 0 → strong evidence of association.\n\nBMI (per 1 SD)\n- b_bmi_c = 0.63 (SD = 0.05, 95% CrI: 0.54 to 0.72)\n- Positive association with diabetes.\n- Odds ratio: exp(0.63) ≈ 1.87\n- A 1-SD increase in BMI increases odds of diabetes by ~87%.\n- Tight CrI → high certainty in the estimate.\n\nSex (Female vs. Male)\n- b_sexFemale = –0.66 (SD = 0.10, 95% CrI: –0.86 to –0.47)\n- Indicates lower log-odds for females.\n- Odds ratio: exp(–0.66) ≈ 0.52\n- Females have ~48% lower odds of diabetes compared to males, adjusting for age, BMI, and race.\n\n\nRace/Ethnicity\n(Reference: Non-Hispanic White)\nMexican American\n- b = 0.69 (SD = 0.17, 95% CrI: 0.34 to 1.03)\n- OR ≈ 1.99\n- Nearly twice the odds of diabetes.\nOther Hispanic\n- b = 0.43 (SD = 0.25, 95% CrI: –0.07 to 0.89)\n- OR ≈ 1.54\n- Mean effect positive, but CrI crosses 0 → weaker evidence.\nNon-Hispanic Black\n- b = 0.53 (SD = 0.15, 95% CrI: 0.23 to 0.83)\n- OR ≈ 1.71\n- ~70% higher odds vs NH Whites, strong evidence.\nOther / Multiracial\n- b = 0.81 (SD = 0.19, 95% CrI: 0.45 to 1.18)\n- OR ≈ 2.27\n- More than double the odds compared to NH Whites.\n\n\n\nInterpretation\n\nAge and BMI show strong positive associations with diabetes risk in the posterior distribution.\n\nFemale sex shows a clear protective association.\n\nRacial/ethnic groups, particularly Mexican American, NH Black, and Other/Multi, have elevated odds relative to NH Whites.\n\nPosterior estimates demonstrate consistent directionality with survey-weighted models, with Bayesian credible intervals providing more intuitive uncertainty quantification.\n\n\n\n\nDiagnostics and Model Fit\n\n\nCode\nknitr::kable(as.data.frame(brms::bayes_R2(bayes_fit)))\n\n\n\n\nTable 4: Bayesian R² Summary\n\n\n\n\n\n\n\nEstimate\nEst.Error\nQ2.5\nQ97.5\n\n\n\n\nR2\n0.1316278\n0.0123417\n0.107432\n0.1565549\n\n\n\n\n\n\n\n\n\n\nCode\ndiag &lt;- posterior::summarise_draws(bayes_fit, \"rhat\", \"ess_bulk\", \"ess_tail\")\n\ndiag_b &lt;- diag |&gt;\ndplyr::as_tibble() |&gt;\ndplyr::filter(grepl(\"^b_\", .data$variable)) |&gt;\ndplyr::transmute(\nParameter = .data$variable,\nRhat      = .data$rhat,\nBulk_ESS  = .data$ess_bulk,\nTail_ESS  = .data$ess_tail\n)\n\nknitr::kable(diag_b, digits = 1)\n\n\n\n\nTable 5: MCMC Diagnostics (R-hat and Effective Sample Sizes) for Model Parameters\n\n\n\n\n\n\nParameter\nRhat\nBulk_ESS\nTail_ESS\n\n\n\n\nb_Intercept\n1\n3548.0\n3511.8\n\n\nb_age_c\n1\n2349.3\n2617.8\n\n\nb_bmi_c\n1\n3327.1\n2825.9\n\n\nb_sexFemale\n1\n3668.1\n3123.7\n\n\nb_raceMexicanAmerican\n1\n3656.6\n2821.2\n\n\nb_raceOtherHispanic\n1\n4242.3\n3013.5\n\n\nb_raceNHBlack\n1\n3809.1\n3012.2\n\n\nb_raceOtherDMulti\n1\n3947.9\n2809.1\n\n\n\n\n\n\n\n\nAll parameters achieved R̂ ≈ 1.00 and effective sample sizes &gt;2,000, indicating excellent convergence. The Bayesian R² ≈ 0.13, showing that age, BMI, sex, and race explain about 13% of diabetes variability.\n\n\nModel Comparison\n\n\nCode\ninvisible(capture.output({\nfit_no_race &lt;- update(bayes_fit, formula = update(fml_bayes, . ~ . - race))\nfit_no_sex  &lt;- update(bayes_fit, formula = update(fml_bayes, . ~ . - sex))\n}))\n\nloo_base    &lt;- loo::loo(bayes_fit)\nloo_no_race &lt;- loo::loo(fit_no_race)\nloo_no_sex  &lt;- loo::loo(fit_no_sex)\n\ncmp_df &lt;- as.data.frame(loo::loo_compare(loo_base, loo_no_race, loo_no_sex))\ncmp_df$Model &lt;- rownames(cmp_df)\ncmp_df &lt;- cmp_df[, c(\"Model\", setdiff(names(cmp_df), \"Model\"))]\n\nknitr::kable(\ncmp_df,\ncaption = \"LOO Comparison (higher elpd_loo indicates better predictive performance).\"\n)\n\n\n\nBayesian Model Comparison (LOO): Base Model vs. Reduced Models Without Race or Sex\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nelpd_diff\nse_diff\nelpd_loo\nse_elpd_loo\np_loo\nse_p_loo\nlooic\nse_looic\n\n\n\n\nbayes_fit\nbayes_fit\n0.00000\n0.000000\n-1418.258\n56.42097\n8.732434\n0.5944729\n2836.517\n112.8419\n\n\nfit_no_race\nfit_no_race\n-14.54234\n6.356781\n-1432.801\n54.03444\n5.351198\n0.3921720\n2865.602\n108.0689\n\n\nfit_no_sex\nfit_no_sex\n-19.89966\n8.205321\n-1438.158\n57.31338\n7.254853\n0.5102266\n2876.316\n114.6268\n\n\n\n\n\nModels excluding race or sex had lower expected log predictive density (elpd), confirming that both variables contribute meaningfully to model fit.\n\n\nPosterior Predictive Checks\n\n\nCode\nyobs &lt;- adult_imp1$diabetes_dx\n\n\n\n\nCode\nbayesplot::pp_check(bayes_fit, type = \"bars\", nsamples = 100)\n\n\n\n\n\n\n\n\nFigure 1: Posterior Predictive Check: Observed vs. Replicated Outcome Distribution (Bars)\n\n\n\n\n\nThe close alignment between observed (y) and replicated (y_rep) outcome distributions indicates that the Bayesian model reproduces the empirical data structure well.\n\n\nCode\nyrep &lt;- brms::posterior_predict(bayes_fit, ndraws = 400)\nbayesplot::ppc_stat(y = yobs, yrep = yrep, stat = \"mean\")\n\n\n\n\n\n\n\n\nFigure 2: Posterior predictive check for the mean of the binary outcome, comparing the observed mean (T(y)) to replicated means (T(y_rep)) across posterior draws.\n\n\n\n\n\n\n\nCode\nyrep &lt;- brms::posterior_predict(bayes_fit, ndraws = 400)\nbayesplot::ppc_stat(y = yobs, yrep = yrep, stat = \"sd\")\n\n\n\n\n\n\n\n\nFigure 3: Posterior predictive check for the standard deviation of the binary outcome (T(y)) compared with replicated datasets (T(y_rep)).\n\n\n\n\n\nThe posterior predictive checks demonstrate strong model calibration: simulated variability closely aligns with the observed data, indicating that the Bayesian model accurately captures both the mean and dispersion of the binary outcome.\n\n\nMCMC Diagnostics and Posterior Distributions\n\n\nCode\nbayesplot::mcmc_areas(as.array(bayes_fit), regex_pars = \"^b_\", prob = 0.95)\n\n\n\n\n\n\n\n\nFigure 4: Posterior distributions (95% credible mass) for slope parameters in the Bayesian logistic regression model.\n\n\n\n\n\nAll posteriors appear unimodal and well‐centered, indicating stable estimation and strong convergence across parameters. Positive coefficients (e.g., age, BMI) correspond to increased diabetes risk, while negative coefficients (e.g., female sex) indicate protective associations.\n\n\nCode\nbayesplot::mcmc_trace(as.array(bayes_fit), regex_pars = \"^b_\")\n\n\n\n\n\n\n\n\nFigure 5: Trace plots for slope parameters across four MCMC chains, demonstrating effective chain mixing and stationarity.\n\n\n\n\n\nAll parameters exhibit well-mixed, stable trace patterns with no visible drift, supporting convergence diagnostics (R̂ ≈ 1.00). This confirms that the posterior samples are representative and that the Bayesian model converged reliably.\n\n\nCode\npost_array &lt;- posterior::as_draws_array(bayes_fit)\nbayesplot::mcmc_acf(post_array, pars = c(\"b_age_c\", \"b_bmi_c\"))\n\n\n\n\n\n\n\n\nFigure 6: Autocorrelation plots for posterior samples of age and BMI coefficients, showing rapid decay of autocorrelation with lag. Low autocorrelation across lags confirms efficient MCMC sampling and good chain independence.\n\n\n\n\n\n\nTrace, density, and autocorrelation plots confirm smooth chain mixing, unimodal posteriors, and minimal autocorrelation across samples.\nAll four chains showed strong convergence with no signs of divergence or non-stationarity.\nTrace plots revealed stable, overlapping chains with consistent mixing across iterations, while autocorrelation decayed rapidly toward zero, confirming efficient sampling and low dependency between successive draws.\nTogether with R̂ ≈ 1.00 and large effective sample sizes, these diagnostics indicate a well-behaved posterior and reliable inference.\n\n\n\nCode\nbayesplot::mcmc_pairs(\nposterior::as_draws_array(bayes_fit),\npars = c(\"b_age_c\", \"b_bmi_c\", \"b_sexFemale\"),\noff_diag_args = list(size = 1.5, alpha = 0.4)\n)\n\n\n\n\n\n\n\n\nFigure 7: Posterior correlation among key regression parameters. Off-diagonal panels show pairwise joint posterior distributions, while diagonals show marginal posteriors. Weak correlations indicate predictors contribute independent information, supporting model stability.\n\n\n\n\n\nThe posterior pairwise correlation plot (above) shows that the regression coefficients (age_c, bmi_c, and sexFemale) have minimal posterior correlation, indicated by round, diffuse scatterplots without strong directional patterns. This suggests that these predictors contribute distinct, non-redundant information to the diabetes model. The diagonal panels show smooth, unimodal posterior densities, confirming stable estimates. The weak correlations further indicate low multicollinearity and well-behaved MCMC sampling, supporting reliable inference.\n\n\nPrior vs. Posterior\n\n\nCode\n# Extract posterior draws as a matrix, then convert to tibble\npost &lt;- as_draws_matrix(bayes_fit) %&gt;%   # safer than as_draws_df for manipulation\n  as.data.frame() %&gt;%\n  select(b_bmi_c, b_age_c) %&gt;%\n  pivot_longer(\n    everything(),\n    names_to = \"term\",\n    values_to = \"estimate\"\n  ) %&gt;%\n  mutate(\n    term = case_when(\n      term == \"b_bmi_c\" ~ \"BMI (per 1 SD)\",\n      term == \"b_age_c\" ~ \"Age (per 1 SD)\"\n    ),\n    type = \"Posterior\"\n  )\nprior_draws &lt;- tibble(\n  term = rep(c(\"BMI (per 1 SD)\", \"Age (per 1 SD)\"), each = 4000),\n  estimate = c(rnorm(4000, 0, 1), rnorm(4000, 0, 1)),\n  type = \"Prior\"\n)\ncombined_draws &lt;- bind_rows(prior_draws, post)\n\nggplot(combined_draws, aes(x = estimate, fill = type)) +\n  geom_density(alpha = 0.4) +\n  facet_wrap(~ term, scales = \"free\", ncol = 2) +\n  theme_minimal(base_size = 13) +\n  labs(\n    title = \"Prior vs Posterior Distributions\",\n    x = \"Coefficient estimate\",\n    y = \"Density\",\n    fill = \"\"\n  )\n\n\n\n\n\n\n\n\nFigure 8: Prior vs Posterior Distributions\n\n\n\n\n\nFor age and BMI, the posterior densities shift notably away from the N(0, 2.5) prior toward positive values and are narrower, indicating strong information from the data; for sex, the posterior remains closer to the prior with more overlap, indicating weaker evidence.\nThe overlay of prior and posterior densities illustrates that informative updates occurred primarily for BMI, age, and race coefficients, which showed distinct posterior shifts relative to the priors. In contrast, weaker predictors such as sex displayed overlapping distributions, indicating that inference for those parameters was more influenced by prior uncertainty than by the observed data. This balance confirms appropriate regularization rather than overfitting.\n\n\nModel Fit and Calibration\n\n\nCode\npred_mean &lt;- colMeans(brms::posterior_epred(bayes_fit))\nggplot(data.frame(pred = pred_mean, obs = yobs),\naes(x = pred, y = obs)) +\ngeom_point(alpha = 0.15, position = position_jitter(height = 0.03)) +\ngeom_smooth(method = \"loess\", se = TRUE) +\nlabs(x = \"Mean predicted probability\", y = \"Observed diabetes (0/1)\")\n\n\n\n\n\n\n\n\nFigure 9: Calibration plot comparing observed diabetes outcomes (0/1) to model-predicted probabilities with a smoothed Locally Estimated Scatterplot Smoothing (LOESS) curve. The close alignment between the blue line and the diagonal (ideal calibration) indicates good model fit and reliable probability estimates.\n\n\n\n\n\n\n\nCode\n# 1. Survey-weighted prevalence\nsvy_mean &lt;- svymean(~diabetes_dx, nhanes_design_adult, na.rm = TRUE)\n\n# 2. Posterior predictive prevalence (per draw)\npp_samples &lt;- brms::posterior_predict(bayes_fit, ndraws = 1000)  # draws x individuals\npp_proportion &lt;- rowMeans(pp_samples)                            # prevalence per draw\n\n# 3. Build comparison table\nsummary_table &lt;- tibble(\n  Method = c(\"Survey-weighted mean (NHANES)\", \n             \"Imputed dataset mean\", \n             \"Posterior predictive mean\"),\n  diabetes_mean = c(\n    coef(svy_mean),                           # survey-weighted mean\n    mean(adult_imp1$diabetes_dx, na.rm = TRUE),  # imputed dataset\n    mean(pp_proportion)                       # posterior predictive mean\n  ),\n  SE = c(\n    SE(svy_mean),   # survey-weighted SE\n    NA,             # not available for raw mean\n    NA              # not available for posterior predictive mean\n  )\n)\n\nkable(summary_table, digits = 4,\n      caption = \"Comparison of Diabetes Prevalence Across Methods\")\n\n\n\n\n\nComparison of Diabetes Prevalence Across Methods\n\n\nMethod\ndiabetes_mean\nSE\n\n\n\n\nSurvey-weighted mean (NHANES)\n0.0889\n0.0048\n\n\nImputed dataset mean\n0.1105\nNA\n\n\nPosterior predictive mean\n0.1094\nNA\n\n\n\n\n\nFigure 10: Comparison of diabetes prevalence across survey-weighted (NHANES), imputed, and posterior predictive estimates. The posterior predictive mean aligns closely with the observed NHANES prevalence, indicating strong model calibration.\n\n\n\n\nThe posterior predictive distribution of diabetes prevalence closely mirrored the survey-estimated prevalence, with the posterior mean aligning within about 1% of the observed rate.\n\n\nCode\n# Posterior predictive prevalence (replicated datasets)\n\nyrep &lt;- brms::posterior_predict(bayes_fit, ndraws = 2000)   # draws x observations (0/1)\npost_prev &lt;- rowMeans(yrep)                                 # prevalence each posterior draw\n\n# Survey-weighted observed prevalence (population estimate)\n\ndes_obs &lt;- survey::svydesign(\nid = ~SDMVPSU, strata = ~SDMVSTRA, weights = ~WTMEC2YR,\nnest = TRUE, data = adult_imp1\n)\nobs &lt;- survey::svymean(~diabetes_dx, des_obs, na.rm = TRUE)\nobs_prev  &lt;- as.numeric(obs[\"diabetes_dx\"])\nobs_se    &lt;- as.numeric(SE(obs)[\"diabetes_dx\"])\nobs_lcl   &lt;- max(0, obs_prev - 1.96 * obs_se)\nobs_ucl   &lt;- min(1, obs_prev + 1.96 * obs_se)\n\n# Plot: posterior density with weighted point estimate and 95% CI band\n\nggplot(data.frame(prev = post_prev), aes(x = prev)) +\ngeom_density(alpha = 0.6) +\nannotate(\"rect\", xmin = obs_lcl, xmax = obs_ucl, ymin = 0, ymax = Inf, alpha = 0.15) +\ngeom_vline(xintercept = obs_prev, linetype = 2) +\ncoord_cartesian(xlim = c(0, 1)) +\nlabs(\n  x = \"Diabetes prevalence\",\n  y = \"Posterior density\",\n  subtitle = sprintf(\"Survey-weighted NHANES prevalence = %.1f%%\", obs_prev * 100)\n) +\ntheme_minimal()\n\n\n\n\n\n\n\n\nFigure 11: Posterior predictive distribution of diabetes prevalence (solid density) overlaid with the survey-weighted NHANES prevalence (vertical dashed line) and its 95% confidence interval (shaded band). The close overlap indicates that the Bayesian model accurately reproduces the observed population prevalence.\n\n\n\n\n\nThe survey-weighted NHANES diabetes prevalence was approximately 8.9%, whereas the Bayesian model’s posterior predictive mean prevalence was also in the 8–9% range. This close agreement indicates that the Bayesian logistic regression model reproduces the observed population-level prevalence and is well-calibrated to the NHANES data.\n\n\nCode\n# --- 1. Survey-weighted (Population) prevalence ---\npop_est &lt;- svymean(~diabetes_dx, nhanes_design_adult, na.rm = TRUE)\npop_prev &lt;- as.numeric(pop_est)\npop_se &lt;- as.numeric(SE(pop_est))\npop_ci &lt;- c(pop_prev - 1.96 * pop_se, pop_prev + 1.96 * pop_se)\n\n# --- 2. Bayesian posterior prevalence ---\n# Uses your existing pp_samples matrix\npp_proportion &lt;- rowMeans(pp_samples)                   # prevalence per posterior draw\npost_prev &lt;- mean(pp_proportion)                        # posterior mean prevalence\npost_ci &lt;- quantile(pp_proportion, c(0.025, 0.975))     # 95% credible interval\n\n# --- 3. Combine into one data frame ---\nbar_df &lt;- tibble(\n  Source     = c(\"Survey-Weighted (Population)\", \"Bayesian Posterior\"),\n  Prevalence = c(pop_prev, post_prev),\n  CI_low     = c(pop_ci[1], post_ci[1]),\n  CI_high    = c(pop_ci[2], post_ci[2])\n)\n\n# --- 4. Plot ---\nggplot(bar_df, aes(x = Source, y = Prevalence, fill = Source)) +\n  geom_col(alpha = 0.85, width = 0.6) +\n  geom_errorbar(\n    aes(ymin = CI_low, ymax = CI_high),\n    width = 0.15,\n    color = \"black\",\n    linewidth = 0.8\n  ) +\n  guides(fill = \"none\") +\n  labs(\n    title = \"Population vs Posterior Diabetes Prevalence\",\n    y = \"Prevalence (Proportion with Diabetes)\",\n    x = NULL\n  ) +\n  theme_minimal(base_size = 13)\n\n\n\n\n\nPopulation vs Posterior Diabetes Prevalence\n\n\n\n\nThe Bayesian posterior predicts a slightly higher diabetes prevalence than the survey-weighted NHANES estimate, but the credible and confidence intervals overlap, indicating strong agreement.\nThis suggests the Bayesian model is well-calibrated and reproduces population-level prevalence without systematic over- or under-prediction.\n\n\nInternal Validation: Individual-Level Predictions\n\n\nCode\nadult_means &lt;- adult_imp1 %&gt;% summarise(\nage_mean = mean(age, na.rm = TRUE),\nage_sd   = sd(age, na.rm = TRUE),\nbmi_mean = mean(bmi, na.rm = TRUE),\nbmi_sd   = sd(bmi, na.rm = TRUE)\n)\n\nto_model_row &lt;- function(age_raw, bmi_raw, sex_lab, race_lab) {\ntibble(\nage_c  = (age_raw - adult_means$age_mean)/adult_means$age_sd,\nbmi_c  = (bmi_raw - adult_means$bmi_mean)/adult_means$bmi_sd,\nsex    = factor(sex_lab,   levels = levels(adult_imp1$sex)),\nrace  = factor(race_lab, levels = levels(adult_imp1$race)),\nwt_norm = 1\n)\n}\n\nplot_post_density &lt;- function(df_row, title_txt) {\nphat &lt;- posterior_linpred(bayes_fit, newdata = df_row, transform = TRUE)\nci95 &lt;- quantile(phat, c(0.025, 0.975))\nggplot(data.frame(pred = as.numeric(phat)), aes(x = pred)) +\ngeom_density(fill = \"skyblue\", alpha = 0.4) +\ngeom_vline(xintercept = ci95[1], linetype = \"dashed\", color = \"red\") +\ngeom_vline(xintercept = ci95[2], linetype = \"dashed\", color = \"red\") +\nlabs(x = \"P(Diabetes = 1)\", y = \"Density\", title = title_txt) +\ntheme_minimal()\n}\n\np1 &lt;- to_model_row(adult$age[1], adult$bmi[1],\nas.character(adult$sex[1]), as.character(adult$race[1]))\nplot_post_density(p1, \"Participant 1: Posterior Predictive Distribution (95% CrI)\")\n\n\n\n\n\nPosterior predictive distribution for an example participant, showing the estimated probability of diabetes (P = 1) with 95% credible intervals (red dashed lines).\n\n\n\n\nPosterior predictive densities for individual participants illustrate the uncertainty in diabetes risk estimates. The credible intervals quantify plausible risk ranges, emphasizing how posterior variability captures uncertainty rather than single-point predictions.\n\n\nPosterior Predictions and Inverse Inference\n\n\nCode\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# 1. Grid of BMI values (RAW BMI from 18 to 40)\nbmi_seq &lt;- seq(18, 40, by = 0.5)\n\n# 2. Newdata using the SAME factor levels as adult_imp1\nnewdata_grid &lt;- data.frame(\n  age_c  = 40,   # NOTE: Namita used 40 here even though age_c is standardized\n  bmi_c  = bmi_seq,   # she also used raw BMI in a column named bmi_c\n  sex    = factor(\"Female\",          levels = levels(adult_imp1$sex)),\n  race   = factor(\"Mexican American\", levels = levels(adult_imp1$race)),\n  wt_norm = 1\n)\n\n# 3. Posterior predicted probabilities\npred_probs &lt;- brms::posterior_linpred(\n  bayes_fit,\n  newdata   = newdata_grid,\n  transform = TRUE\n)\n\n# 4. Mean predicted probability at each BMI\nprob_mean &lt;- colMeans(pred_probs)\n\npred_df &lt;- dplyr::bind_cols(newdata_grid, prob_mean = prob_mean)\n\n# 5. Target probability\ntarget_prob &lt;- 0.30\n\n# Find the BMI whose predicted prob is closest to the target\nclosest &lt;- pred_df[which.min(abs(pred_df$prob_mean - target_prob)), , drop = FALSE]\n\n# 6. Plot\nggplot(pred_df, aes(x = bmi_c, y = prob_mean)) +\n  geom_line(color = \"darkblue\", linewidth = 1.2) +\n  geom_hline(yintercept = target_prob, color = \"red\", linetype = \"dashed\") +\n  geom_vline(xintercept = closest$bmi_c, color = \"red\", linetype = \"dotted\") +\n  annotate(\n    \"text\",\n    x     = closest$bmi_c,\n    y     = target_prob + 0.05,\n    label = paste0(\"Target BMI \\u2248 \", round(closest$bmi_c, 1)),\n    color = \"red\",\n    hjust = -0.1\n  ) +\n  labs(\n    x = \"BMI (kg/m^2)\",\n    y = \"Predicted Probability of Diabetes\",\n    title = \"Inverse Prediction: BMI Needed for Target Diabetes Risk\"\n  ) +\n  coord_cartesian(ylim = c(0, 1)) +\n  theme_bw()\n\n\n\n\n\nInverse prediction of BMI needed to reach a target diabetes probability (illustrative example).\n\n\n\n\nInverse inference explores what BMI value would yield a given diabetes risk under the posterior model. In this example, predicted diabetes probability remains near 1.0 across most BMI values, suggesting that other covariates (e.g., age or race) dominate predicted risk in this profile. The “target BMI ≈ 18” marks the approximate threshold for a 30% risk under this participant’s conditions."
  },
  {
    "objectID": "index.html#results",
    "href": "index.html#results",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk Using NHANES 2013–2014 Data",
    "section": "Results",
    "text": "Results\nA concise summary of posterior estimates is provided below.\n\nCode\ncat(paste(bullets, collapse = \"\\n\"))\n\n\nPopulation-level interpretation (posterior, odds ratios)\n\nConvergence. All R-hat ≈ 1.00; large ESS → excellent mixing.\nBaseline risk. Male, White, mean age/BMI: ~6.5% predicted diabetes prevalence.\nAge. +1 SD → 2.99× (95% CrI 2.66–3.39; CrI excludes 1).\nBMI. +1 SD → 1.87× (95% CrI 1.71–2.05; CrI excludes 1).\nFemale vs. Male. 0.52× (95% CrI 0.42–0.63; CrI excludes 1).\nBlack vs. White. NA× (95% CrI NA–NA; CrI overlaps 1).\nHispanic vs. White. NA× (95% CrI NA–NA; CrI overlaps 1).\nOther/Multi vs. White. NA× (95% CrI NA–NA; CrI overlaps 1).\n\n\n\nCode\n# Combine results from all models\n\nsvy_tbl   &lt;- if (exists(\"svy_or\") && nrow(svy_or) &gt; 0)\ndplyr::mutate(svy_or,   Model = \"Survey-weighted (design-based)\") else NULL\nbayes_tbl &lt;- if (exists(\"bayes_or\") && nrow(bayes_or) &gt; 0)\ndplyr::mutate(bayes_or, Model = \"Bayesian\") %&gt;%\ndplyr::filter(term != \"Intercept\") else NULL\n\nall_tbl &lt;- dplyr::bind_rows(Filter(Negate(is.null), list(svy_tbl, bayes_tbl))) %&gt;%\ndplyr::mutate(\nterm = dplyr::case_when(\n  grepl(\"bmi\", term,  ignore.case = TRUE) ~ \"BMI (per 1 SD)\",\n  grepl(\"age\", term,  ignore.case = TRUE) ~ \"Age (per 1 SD)\",\n  grepl(\"^sexFemale$\", term)              ~ \"Female (vs. Male)\",\n  grepl(\"^sexMale$\", term)                ~ \"Male (vs. Female)\",\n  grepl(\"^raceHispanic$\", term)          ~ \"Hispanic (vs. White)\",\n  grepl(\"^raceBlack$\", term)             ~ \"Black (vs. White)\",\n  grepl(\"^raceOther$\", term)             ~ \"Other (vs. White)\",\n  TRUE ~ term\n),\nOR_CI = sprintf(\"%.2f (%.2f – %.2f)\", OR, LCL, UCL)\n) %&gt;%\ndplyr::select(Model, term, OR_CI)\n\n\n\n\nCode\nknitr::kable(all_tbl, align = c(\"l\",\"l\",\"c\"))\n\n\n\n\nTable 6: Comparison of odds ratios (per 1 SD for age and BMI) and 95% intervals across survey-weighted and Bayesian frameworks.\n\n\n\n\n\n\n\n\n\n\n\nModel\nterm\nOR_CI\n\n\n\n\nSurvey-weighted (design-based)\nAge (per 1 SD)\n3.03 (2.70 – 3.40)\n\n\nSurvey-weighted (design-based)\nBMI (per 1 SD)\n1.89 (1.65 – 2.15)\n\n\nSurvey-weighted (design-based)\nFemale (vs. Male)\n0.53 (0.41 – 0.68)\n\n\nSurvey-weighted (design-based)\nraceMexican American\n2.04 (1.49 – 2.79)\n\n\nSurvey-weighted (design-based)\nraceOther Hispanic\n1.59 (1.17 – 2.17)\n\n\nSurvey-weighted (design-based)\nraceNH Black\n1.67 (1.16 – 2.40)\n\n\nSurvey-weighted (design-based)\nraceOther/Multi\n2.33 (1.55 – 3.50)\n\n\nBayesian\nAge (per 1 SD)\n2.99 (2.66 – 3.39)\n\n\nBayesian\nBMI (per 1 SD)\n1.87 (1.71 – 2.05)\n\n\nBayesian\nFemale (vs. Male)\n0.52 (0.42 – 0.63)\n\n\nBayesian\nraceMexicanAmerican\n1.99 (1.41 – 2.80)\n\n\nBayesian\nraceOtherHispanic\n1.53 (0.94 – 2.43)\n\n\nBayesian\nraceNHBlack\n1.70 (1.26 – 2.30)\n\n\nBayesian\nraceOtherDMulti\n2.26 (1.56 – 3.24)\n\n\n\n\n\n\n\n\nAcross both frameworks—survey-weighted (design-based) and Bayesian—age and BMI were consistently associated with higher odds of doctor-diagnosed diabetes. Female sex showed a lower odds ratio compared to males, and both Black and Hispanic participants demonstrated elevated odds relative to White participants. The similarity of effect sizes across frameworks underscores the robustness of these predictors to different modeling assumptions."
  },
  {
    "objectID": "index.html#discussion-and-limitations",
    "href": "index.html#discussion-and-limitations",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk Using NHANES 2013–2014 Data",
    "section": "Discussion and Limitations",
    "text": "Discussion and Limitations\n\nInterpretation\nThe Bayesian logistic regression framework produced results that were highly consistent with the survey-weighted model. Age and BMI remained the most influential predictors of doctor-diagnosed diabetes, each showing a strong and positive association with diabetes risk.\nUnlike classical maximum likelihood estimation, the Bayesian approach directly quantified uncertainty through posterior distributions, offering richer interpretability and more transparent probability statements. The alignment between Bayesian and survey-weighted estimates supports the robustness of these associations and highlights the practicality of Bayesian modeling for complex, weighted population data.\nPosterior predictive checks confirmed that simulated diabetes prevalence closely matched the observed NHANES estimate, supporting good model calibration. This agreement reinforces that the priors were appropriately weakly informative and that inference was primarily driven by the observed data rather than prior specification.\nOverall, this study demonstrates that Bayesian inference complements traditional epidemiologic methods by maintaining interpretability while enhancing stability and explicitly quantifying uncertainty in complex survey data.\n\n\nLimitations\nWhile this analysis highlights the strengths of Bayesian logistic regression for epidemiologic modeling, several limitations should be acknowledged.\nFirst, the Bayesian model used a single imputed dataset rather than jointly modeling imputation uncertainty, which may lead to a modest understatement of total variance.\nSecond, NHANES examination weights were normalized and treated as importance weights. This approach approximates—but does not fully replicate—survey-weighted inference and may slightly affect standard error estimation.\nThird, the weakly informative priors \\(N(0, 2.5)\\) for slopes and Student-t(3, 0, 10) for the intercept were not empirically tuned. Alternative prior choices could marginally shift posterior intervals or reduce shrinkage.\nFourth, although convergence diagnostics (R̂ ≈ 1.00, large effective sample sizes, and stable posterior predictive checks) indicated good model performance, results are conditional on the 2013–2014 NHANES cycle and may not generalize to later survey waves or longitudinal applications.\nFinally, the model has not undergone external validation or formal sensitivity analyses. The participant-level posterior risk estimates presented here are intended for illustration only and should not be used for individual prediction or clinical decision-making. Before external deployment or use in other settings, the model would require validation in independent datasets and sensitivity analyses to evaluate robustness to both modeling assumptions and prior specifications.\n\nTargeted Therapy\nThe Bayesian diabetes prediction project highlights the translational potential of Bayesian modeling in clinical decision-making and public health strategy.\nBy incorporating patient-level predictors such as age, BMI, sex, and race to estimate diabetes probability, the model shifts from descriptive statistics toward individualized risk prediction.\nThe translational value lies in converting these probabilistic outputs into actionable thresholds—for example, identifying the BMI or age at which predicted diabetes risk exceeds a clinically meaningful level (e.g., 30%).\nThese insights can support early screening, personalized lifestyle recommendations, and targeted prevention programs for higher-risk populations.\nThis approach reflects precision public health by bridging data science and clinical judgment to create tailored, evidence-based strategies that may improve diabetes prevention and management outcomes.\nWhat changes in modifiable predictors would lower diabetes risk?\n\n\nTranslational Research Implications\n\nThe model can be used to inform prevention and intervention strategies.\nBMI is the only modifiable predictor in the current model.\nChanges in BMI (via behavior or lifestyle interventions) can be explored to achieve a lower predicted risk.\nNon-modifiable predictors (e.g., sex and race) are held constant.\nThe modifiable predictor (BMI) is varied until the model reaches the desired probability threshold."
  },
  {
    "objectID": "index.html#conclusion",
    "href": "index.html#conclusion",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk Using NHANES 2013–2014 Data",
    "section": "Conclusion",
    "text": "Conclusion\nThe Bayesian and survey-weighted logistic regression (design-based) frameworks both identified consistent predictors of diabetes risk among U.S. adults: advancing age, higher BMI, sex (lower odds for females), and non-White race/ethnicity. These convergent findings reinforce the robustness of core risk factors and the validity of the analytic approach across modeling paradigms.\nThe Bayesian model produced estimates nearly identical in direction and magnitude to the frequentist results while offering richer uncertainty quantification through posterior distributions, credible intervals, and posterior predictive checks. Diagnostics (R̂ ≈ 1.00, large effective sample sizes, and Bayesian R² ≈ 0.13) confirmed model convergence and good fit, demonstrating that weakly informative priors effectively regularized estimation without distorting inference.\nCollectively, these results underscore the value of Bayesian inference in epidemiologic research involving complex survey data. By integrating prior information and leveraging MCMC sampling to approximate the full posterior distribution, Bayesian models enhance transparency, interpretability, and reproducibility. Future extensions could incorporate hierarchical priors, combine multiple NHANES cycles, or explore Bayesian model averaging to better capture population heterogeneity, temporal trends, and evolving diabetes risk patterns."
  },
  {
    "objectID": "slides.html#slide-1-introduction",
    "href": "slides.html#slide-1-introduction",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013–2014)",
    "section": "Slide 1: Introduction",
    "text": "Slide 1: Introduction\n\nDiabetes is a chronic disease with rising global prevalence.\nEarly identification of risk factors is key to prevention and control.\nBayesian methods allow flexible modeling with uncertainty quantification.\nAim: Predict diabetes diagnosis using Bayesian logistic regression with imputed data ."
  },
  {
    "objectID": "slides.html#slide-2-data-source",
    "href": "slides.html#slide-2-data-source",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013–2014)",
    "section": "Slide 2: Data Source",
    "text": "Slide 2: Data Source\n\nData: National Health and Nutrition Examination Survey (NHANES)\nAdult dataset (&gt;20 years).\nVariables included:\n\nDemographics: age, sex, race\nClinical: BMI\nOutcome: diabetes_dx (diagnosis: 0 = No, 1 = Yes)\nhead(adult)"
  },
  {
    "objectID": "slides.html#slide-3-missing-data-assessment",
    "href": "slides.html#slide-3-missing-data-assessment",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013–2014)",
    "section": "Slide 3: Missing Data Assessment",
    "text": "Slide 3: Missing Data Assessment\n\nOverall missingness: ~4%, No variable completely missing, Missingness is not uniform\nMissingness pattern: likely MAR (Missing At Random).\nClustered mainly in bmi and diabetes_dx.\nDecision: Apply Multiple Imputation by Chained Equations (MICE)."
  },
  {
    "objectID": "slides.html#slide-4-mice-imputation",
    "href": "slides.html#slide-4-mice-imputation",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013–2014)",
    "section": "Slide 4: MICE Imputation",
    "text": "Slide 4: MICE Imputation\n\nMethod: Predictive mean matching (PMM) for continuous vars; logistic regression for binary.\nIterations: 5 imputations, 10 iterations each, combined imputed datasets using Rubin’s rules.\nDistribution plots confirmed consistency with the original data."
  },
  {
    "objectID": "slides.html#slide-5-multivariate-imputation-by-chained-equations-pooled-logistic-regression",
    "href": "slides.html#slide-5-multivariate-imputation-by-chained-equations-pooled-logistic-regression",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013–2014)",
    "section": "Slide 5: Multivariate Imputation by Chained Equations (Pooled Logistic Regression)",
    "text": "Slide 5: Multivariate Imputation by Chained Equations (Pooled Logistic Regression)\n  glm(diabetes_dx ~ age_c + bmi_c + sex + race, family = binomial()) }) pool_mi &lt;- pool(fit_mi) \n\nn=5,769 participants\nModel significance: All predictors are statistically significant (p &lt; 0.01), suggesting a robust association across demographic and health variables."
  },
  {
    "objectID": "slides.html#slide-5-bayesian-logistic-regression",
    "href": "slides.html#slide-5-bayesian-logistic-regression",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013–2014)",
    "section": "Slide 5: Bayesian Logistic Regression",
    "text": "Slide 5: Bayesian Logistic Regression\nOutcome: diabetes_dx (0 = non-diabetic, 1 = diabetic)\nPredictors: age_c, bmi_c, sex, race.\n\nIntercept prior: student_t(3, 0, 10) — allows heavy tails for flexibility in the intercept estimate. (VanDeSchoot2013?)\nRegression coefficients prior: normal(0, 2.5) — providing weakly informative regularization, constraining extreme values without overpowering the data. (VandeSchoot2021?)\nImplemented in brms (Stan backend), Posterior draws = 4000 (4 chains × 1000 iterations). Logistic link function"
  },
  {
    "objectID": "slides.html#slide-6-bayesian-model-equation",
    "href": "slides.html#slide-6-bayesian-model-equation",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013–2014)",
    "section": "Slide 6: Bayesian Model Equation",
    "text": "Slide 6: Bayesian Model Equation\nlibrary(gt)\npriors &lt;- c(\n  set_prior(\"normal(0, 2.5)\", class = \"b\"),\n  set_prior(\"student_t(3, 0, 10)\", class = \"Intercept\") \n)\nbayes_fit &lt;- brm(\n  formula = diabetes_dx | weights(wt_norm) ~ age_c + bmi_c + sex + race,\n  data    = adult_imp1,\n  family  = bernoulli(link = \"logit\"),\n  prior   = priors,\n  chains  = 4, iter = 2000, seed = 123,\n  control = list(adapt_delta = 0.95),\n  refresh = 0   # quiet Stan output\n)\n\\[\n\\text{logit}(P(Y=1)) = \\beta_0 + \\beta_1 \\text{age}_c + \\beta_2 \\text{bmi}_c + \\beta_3 \\text{sex} + \\beta_4 \\text{race}\n\\]\n\n( P(Y=1) ): Probability of being diabetic\nCoefficients estimated with posterior means and 95% credible intervals."
  },
  {
    "objectID": "slides.html#slide-7-bayesian-model-diagnostics",
    "href": "slides.html#slide-7-bayesian-model-diagnostics",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013–2014)",
    "section": "Slide 7: Bayesian Model Diagnostics",
    "text": "Slide 7: Bayesian Model Diagnostics\n\nRhat ≈ 1.00 → convergence achieved.\nBulk ESS &gt; 3000 for all parameters → good mixing.\nTrace plots showed stable sampling across chains.\nPosterior distributions were unimodal and well-centered."
  },
  {
    "objectID": "slides.html#slide-8-posterior-estimates",
    "href": "slides.html#slide-8-posterior-estimates",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013–2014)",
    "section": "Slide 8: Posterior Estimates",
    "text": "Slide 8: Posterior Estimates\n\n\n\n\n\n\n\n\n\nPredictor\nEstimate\n95% CI\nInterpretation\n\n\n\n\nIntercept\n-2.66\n[-2.84, -2.50]\nBaseline log-odds\n\n\nAge_c\n1.09\n[0.97, 1.22]\n↑ age increases diabetes risk\n\n\nBMI_c\n0.88\n[0.76, 1.01]\nHigher BMI linked with higher risk\n\n\nHTN\n0.65\n[0.50, 0.81]\nHypertension predicts diabetes"
  },
  {
    "objectID": "slides.html#slide-9-posterior-predictive-distribution",
    "href": "slides.html#slide-9-posterior-predictive-distribution",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013–2014)",
    "section": "Slide 9: Posterior Predictive Distribution",
    "text": "Slide 9: Posterior Predictive Distribution\nlibrary(ggplot2)\n\nggplot(combined_draws, aes(x = estimate, fill = type)) +\n  geom_density(alpha = 0.4) +\n  facet_wrap(~ term, scales = \"free\", ncol = 2) +\n  theme_minimal(base_size = 13) +\n  labs(\n    title = \"Prior vs Posterior Distributions\",\n    x = \"Coefficient estimate\",\n    y = \"Density\",\n    fill = \"\"\n  )"
  },
  {
    "objectID": "slides.html#slide-10-model-interpretation",
    "href": "slides.html#slide-10-model-interpretation",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013–2014)",
    "section": "Slide 10: Model Interpretation",
    "text": "Slide 10: Model Interpretation\n\nStrong positive relationship between age, BMI, and diabetes probability.\nPosterior predictive checks confirm the model captures data patterns well.\nImputation reduced bias and improved model robustness."
  },
  {
    "objectID": "slides.html#slide-11-limitations",
    "href": "slides.html#slide-11-limitations",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013–2014)",
    "section": "Slide 11: Limitations",
    "text": "Slide 11: Limitations\n\nNHANES data are cross-sectional → no causal inference.\nPotential unmeasured confounding (diet, physical activity).\nLimited predictors → simplified model structure.\nImputation assumes MAR; violations may introduce bias."
  },
  {
    "objectID": "slides.html#slide-12-conclusion",
    "href": "slides.html#slide-12-conclusion",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013–2014)",
    "section": "Slide 12: Conclusion",
    "text": "Slide 12: Conclusion\n\nBayesian logistic regression effectively models uncertainty.\nMICE improved data completeness and reliability.\nPosterior predictions provide interpretable probabilities for diabetes risk.\nFramework adaptable to other health outcomes (e.g., hypertension, obesity)."
  },
  {
    "objectID": "slides.html#slide-13-references",
    "href": "slides.html#slide-13-references",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013–2014)",
    "section": "Slide 13: References",
    "text": "Slide 13: References\n\nvan Buuren S., Groothuis-Oudshoorn K. (2011). MICE: Multivariate Imputation by Chained Equations in R.\nGelman A., Hill J. (2007). Data Analysis Using Regression and Multilevel/Hierarchical Models.\nNHANES Data Documentation (CDC).\nMcElreath R. (2020). Statistical Rethinking: A Bayesian Course with Examples in R and Stan."
  },
  {
    "objectID": "slides.html#slide-14-acknowledgements",
    "href": "slides.html#slide-14-acknowledgements",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013–2014)",
    "section": "Slide 14: Acknowledgements",
    "text": "Slide 14: Acknowledgements\n\nFaculty: Dr. Ashraf Cohen, PhD, MS\nUniversity of West Florida, Department: Mathematics and Statistics\n        Thanks for the guidance"
  },
  {
    "objectID": "slides.html#references",
    "href": "slides.html#references",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "References",
    "text": "References\n\n\nBuuren, Stef van, and Karin Groothuis-Oudshoorn. 2011. “Mice: Multivariate Imputation by Chained Equations in R.” Journal of Statistical Software 45 (3): 1–67. https://doi.org/10.18637/jss.v045.i03.\n\n\nGelman, A., A. Jakulin, M. G. Pittau, and Y. S. Su. 2008. “A Weakly Informative Default Prior Distribution for Logistic and Other Regression Models.” Annals of Applied Statistics 2 (4): 1360–83. https://doi.org/10.1214/08-AOAS191.\n\n\nSchoot, Rens van de, David Kaplan, Jaap Denissen, Jens Asendorpf, Franz Neyer, and Marcel van Aken. 2013. “A Gentle Introduction to Bayesian Analysis: Applications to Developmental Research.” European Journal of Developmental Psychology 10 (6): 723–49. https://doi.org/10.1080/17405629.2013.803373."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Us",
    "section": "",
    "text": "Dr. Namita Mishra is a physician, Head and Neck surgeon, and public health researcher with a strong foundation in medicine, epidemiology, and data science. She is currently a graduate student in Data Science (Health Analytics).\nHer work focuses on the early detection and prevention of non-communicable diseases (cancer, obesity) and on health disparities at the community level. She has conducted research on salivary gland tumors, cardiac implants, and community-based healthy food access. Leveraging her skills in Data Science, she integrates statistical modeling and Bayesian methods into her analyses. Her bioinformatics expertise includes using geodata visualization tools (3D maps and GIS) for presentations. Passionate about bridging clinical insight with data-driven approaches, she is dedicated to advancing sustainable, evidence-based solutions in epidemiology and community health.\nOutside of work, she enjoys gardening, cooking, singing, and sewing.\nDr. Mishra developed the project plan, content draft, analytic coding, and coordinated commits and collaboration with the group on GitHub.\n📧 Contact: nmishra@uwf.edu\n\nAutumn S. Wilcox is a U.S. Navy veteran and Data Science graduate student at the University of West Florida, specializing in Analytics and Modeling. She has over nine years of experience in Network Operations and Technical Writing, including her current role at Navy Federal Credit Union, where she supports enterprise technology and process documentation initiatives. Autumn also holds certification in Clinical Research Quality Management (CRQM) and has contributed to quality oversight and compliance efforts in clinical research settings.\nHer background bridges technology, analytics, and healthcare, with a focus on applying data-driven approaches to improve communication and systems reliability. Outside of work, Autumn enjoys traveling, photography, and finding creative inspiration through music.\nAutumn contributed to analytic coding, content draft, structured the project workflow, and collaborated actively via GitHub.\n📧 Contact: awr12@students.uwf.edu"
  },
  {
    "objectID": "Summaries/aw/paper3_summary.html",
    "href": "Summaries/aw/paper3_summary.html",
    "title": "Bayesian Logistic Regression Capstone",
    "section": "",
    "text": "Paper 3: Abdullah, Hassan, & Mustafa (2022). “A Review on Bayesian Deep Learning in Healthcare: Applications and Challenges”\nGoal\nThe paper systematically reviews how Bayesian deep learning (BDL) is being applied in healthcare: its use cases, methodological approaches, and the challenges and future directions.\nImportance\nHealthcare data is often uncertain (noisy measurements, missing data, variability) and involves high stakes where mistakes can cost lives. While deep learning is powerful, it typically lacks mechanisms for representing uncertainty or handling limited data in a principled way. Bayesian techniques address these gaps by incorporating uncertainty, prior knowledge, and probabilistic reasoning—making models potentially safer, more trustworthy, and interpretable in clinical settings.\nMethods\n- Conduct a literature survey of recent work combining Bayesian methods with deep learning in healthcare.\n- Categorize approaches such as variational inference, Monte Carlo dropout, ensemble methods, Gaussian processes, and Bayesian neural networks.\n- Map these methods to healthcare tasks like diagnosis, prognosis, and treatment planning.\n- Evaluate strengths/weaknesses in terms of data availability, computational costs, interpretability, uncertainty calibration, and privacy.\nResults & Limitations\n- Results: BDL has shown success in disease classification, survival analysis, medical image segmentation, and predictive modeling. It often improves uncertainty quantification, may improve generalization, and provides clinicians with added information (e.g., confidence in predictions).\n- Limitations: High computational demands, scalability issues, difficulty specifying priors, and interpretability challenges remain. Many studies are proof-of-concept and lack validation in real-world clinical environments. Regulatory, privacy, and workflow integration concerns also limit deployment."
  },
  {
    "objectID": "Summaries/aw/paper1_summary.html",
    "href": "Summaries/aw/paper1_summary.html",
    "title": "Summary: Bayesian Data Analysis for Newcomers",
    "section": "",
    "text": "John K. Kruschke and Torrin M. Liddell (2017)\n\n\nThe paper tackles the issue that Bayesian data analysis is often viewed as too complicated or intimidating for newcomers. Traditional explanations rely heavily on mathematical formulas, which discourages students and researchers who want to understand the ideas without advanced math. The authors aim to show that Bayesian reasoning is actually natural and aligns with how people already think in everyday life.\n\n\n\nThe authors present Bayesian concepts in plain language with minimal technical notation. They explain the main idea of Bayesian analysis as reallocating credibility among possible explanations as new data comes in. Simple examples are used to show how priors, likelihoods, and posteriors work together. They also clarify common misunderstandings, such as the role of prior distributions and how Bayesian inference differs from frequentist methods like null hypothesis significance testing.\n\n\n\nThe article provides readers with a clear conceptual framework for Bayesian analysis. Instead of being overwhelmed by formulas, the reader can see how Bayesian methods produce intuitive and interpretable results. Examples highlight how Bayesian outputs, such as posterior distributions and credible intervals, can be directly applied to research questions.\n\n\n\nBecause the article is written as an introduction, it does not go into technical detail or advanced modeling. It is not a substitute for mathematical training or for learning to implement Bayesian models in practice. The focus is on clarity rather than rigor.\n\n\n\nThe paper does not use any real-world datasets. Instead, it relies on simple examples meant to illustrate the principles of Bayesian reasoning. These examples are conceptual rather than empirical, making the ideas easier to grasp for newcomers like myself."
  },
  {
    "objectID": "Summaries/aw/paper1_summary.html#problem-the-article-is-addressing",
    "href": "Summaries/aw/paper1_summary.html#problem-the-article-is-addressing",
    "title": "Summary: Bayesian Data Analysis for Newcomers",
    "section": "",
    "text": "The paper tackles the issue that Bayesian data analysis is often viewed as too complicated or intimidating for newcomers. Traditional explanations rely heavily on mathematical formulas, which discourages students and researchers who want to understand the ideas without advanced math. The authors aim to show that Bayesian reasoning is actually natural and aligns with how people already think in everyday life."
  },
  {
    "objectID": "Summaries/aw/paper1_summary.html#how-it-has-been-solved",
    "href": "Summaries/aw/paper1_summary.html#how-it-has-been-solved",
    "title": "Summary: Bayesian Data Analysis for Newcomers",
    "section": "",
    "text": "The authors present Bayesian concepts in plain language with minimal technical notation. They explain the main idea of Bayesian analysis as reallocating credibility among possible explanations as new data comes in. Simple examples are used to show how priors, likelihoods, and posteriors work together. They also clarify common misunderstandings, such as the role of prior distributions and how Bayesian inference differs from frequentist methods like null hypothesis significance testing."
  },
  {
    "objectID": "Summaries/aw/paper1_summary.html#results",
    "href": "Summaries/aw/paper1_summary.html#results",
    "title": "Summary: Bayesian Data Analysis for Newcomers",
    "section": "",
    "text": "The article provides readers with a clear conceptual framework for Bayesian analysis. Instead of being overwhelmed by formulas, the reader can see how Bayesian methods produce intuitive and interpretable results. Examples highlight how Bayesian outputs, such as posterior distributions and credible intervals, can be directly applied to research questions."
  },
  {
    "objectID": "Summaries/aw/paper1_summary.html#limitations",
    "href": "Summaries/aw/paper1_summary.html#limitations",
    "title": "Summary: Bayesian Data Analysis for Newcomers",
    "section": "",
    "text": "Because the article is written as an introduction, it does not go into technical detail or advanced modeling. It is not a substitute for mathematical training or for learning to implement Bayesian models in practice. The focus is on clarity rather than rigor."
  },
  {
    "objectID": "Summaries/aw/paper1_summary.html#datasets",
    "href": "Summaries/aw/paper1_summary.html#datasets",
    "title": "Summary: Bayesian Data Analysis for Newcomers",
    "section": "",
    "text": "The paper does not use any real-world datasets. Instead, it relies on simple examples meant to illustrate the principles of Bayesian reasoning. These examples are conceptual rather than empirical, making the ideas easier to grasp for newcomers like myself."
  },
  {
    "objectID": "Summaries/aw/paper5_summary.html",
    "href": "Summaries/aw/paper5_summary.html",
    "title": "Summary: The No-U-Turn Sampler (NUTS)—Adaptively Setting Path Lengths in Hamiltonian Monte Carlo",
    "section": "",
    "text": "Matthew D. Hoffman & Andrew Gelman (2014)\n\n\nHamiltonian Monte Carlo (HMC) offers efficient sampling from complex posteriors but requires manual tuning of the trajectory length (number of leapfrog steps). Poor tuning can cause slow convergence or biased sampling, making HMC difficult for non-experts to use effectively.\n\n\n\nThe authors introduce the No-U-Turn Sampler (NUTS), an adaptive extension of HMC that automatically determines the optimal trajectory length. NUTS expands the trajectory until it detects a “U-turn” in parameter space—meaning further movement would retrace steps—and then stops. Combined with dual-averaging step-size adaptation, this removes nearly all manual tuning while preserving the sampling efficiency of HMC.\n\n\n\nAcross hierarchical models and logistic regression tasks, NUTS produces higher effective sample sizes per second and improved convergence diagnostics compared to traditional HMC or Gibbs sampling. It is robust across diverse posterior geometries and scales well with model complexity, forming the default MCMC algorithm in Stan.\n\n\n\nWhile adaptive, NUTS still relies on differentiable log-posteriors and can be computationally intensive for extremely complex or multimodal distributions. Performance depends on the choice of priors and model reparameterization, and it remains unsuitable for discrete or non-differentiable models.\n\n\n\nThe paper evaluates NUTS on simulated hierarchical models and real logistic regression datasets, reporting metrics like effective sample size (ESS) and Gelman–Rubin convergence diagnostics (()) to assess efficiency and accuracy."
  },
  {
    "objectID": "Summaries/aw/paper5_summary.html#problem-the-article-is-addressing",
    "href": "Summaries/aw/paper5_summary.html#problem-the-article-is-addressing",
    "title": "Summary: The No-U-Turn Sampler (NUTS)—Adaptively Setting Path Lengths in Hamiltonian Monte Carlo",
    "section": "",
    "text": "Hamiltonian Monte Carlo (HMC) offers efficient sampling from complex posteriors but requires manual tuning of the trajectory length (number of leapfrog steps). Poor tuning can cause slow convergence or biased sampling, making HMC difficult for non-experts to use effectively."
  },
  {
    "objectID": "Summaries/aw/paper5_summary.html#how-it-has-been-solved",
    "href": "Summaries/aw/paper5_summary.html#how-it-has-been-solved",
    "title": "Summary: The No-U-Turn Sampler (NUTS)—Adaptively Setting Path Lengths in Hamiltonian Monte Carlo",
    "section": "",
    "text": "The authors introduce the No-U-Turn Sampler (NUTS), an adaptive extension of HMC that automatically determines the optimal trajectory length. NUTS expands the trajectory until it detects a “U-turn” in parameter space—meaning further movement would retrace steps—and then stops. Combined with dual-averaging step-size adaptation, this removes nearly all manual tuning while preserving the sampling efficiency of HMC."
  },
  {
    "objectID": "Summaries/aw/paper5_summary.html#results",
    "href": "Summaries/aw/paper5_summary.html#results",
    "title": "Summary: The No-U-Turn Sampler (NUTS)—Adaptively Setting Path Lengths in Hamiltonian Monte Carlo",
    "section": "",
    "text": "Across hierarchical models and logistic regression tasks, NUTS produces higher effective sample sizes per second and improved convergence diagnostics compared to traditional HMC or Gibbs sampling. It is robust across diverse posterior geometries and scales well with model complexity, forming the default MCMC algorithm in Stan."
  },
  {
    "objectID": "Summaries/aw/paper5_summary.html#limitations",
    "href": "Summaries/aw/paper5_summary.html#limitations",
    "title": "Summary: The No-U-Turn Sampler (NUTS)—Adaptively Setting Path Lengths in Hamiltonian Monte Carlo",
    "section": "",
    "text": "While adaptive, NUTS still relies on differentiable log-posteriors and can be computationally intensive for extremely complex or multimodal distributions. Performance depends on the choice of priors and model reparameterization, and it remains unsuitable for discrete or non-differentiable models."
  },
  {
    "objectID": "Summaries/aw/paper5_summary.html#datasets",
    "href": "Summaries/aw/paper5_summary.html#datasets",
    "title": "Summary: The No-U-Turn Sampler (NUTS)—Adaptively Setting Path Lengths in Hamiltonian Monte Carlo",
    "section": "",
    "text": "The paper evaluates NUTS on simulated hierarchical models and real logistic regression datasets, reporting metrics like effective sample size (ESS) and Gelman–Rubin convergence diagnostics (()) to assess efficiency and accuracy."
  },
  {
    "objectID": "Summaries/nm/My_articles.html",
    "href": "Summaries/nm/My_articles.html",
    "title": "Bayesian Logistic Regression Capstone",
    "section": "",
    "text": "Namita’s Literature"
  },
  {
    "objectID": "Summaries/nm/My_articles.html#introduction",
    "href": "Summaries/nm/My_articles.html#introduction",
    "title": "Bayesian Logistic Regression Capstone",
    "section": "Introduction",
    "text": "Introduction\n\nBayesian Hierarchical Model (Disease reclassification and prediction)\n\nWhat is the goal of the paper?\nThe authors develop a Bayesian hierarchical model for multivariate longitudinal data to predict health status, trajectories, and intervention effects at the individual level in the PCORI mission to address questions about health status from patients and clinicians.\nWhy is it important?\nHealthcare data (DNA sequences, functional images of the brain, patient-reported outcomes, and electronic health records with patients’ sequences of health measurements, diagnoses, and treatments) are complex, and the standard approaches are not adequate for clinical data analysis. Electronic health records (EHRs) could improve diagnostic accuracy and predict treatment effects. Visualizations of characteristics of posterior distributions can be immediately understood by clinicians and patients as relevant to their decision. Combining prior knowledge and patient data with evidence could predict the patient’s health status, trajectory, and/or likely benefits of interventions.\nHow is it solved?\nMethod: The authors applied Bayesian hierarchical regression for multivariate longitudinal patient data using open-source R-packages and developed 2 levels—time within person and persons within a population\nThe model combined exogenous (eg, age, clinical history) factors and endogenous (eg, current treatment) variables on the individual’s multivariate health measurements and the effects of health measurements at one time on subsequent interventions.\nThe model produced an estimate of the posterior distribution for each value of the predictor variable and an estimate of the marginal distribution of the regression coefficients for each coefficient that measures the outcome (health status) associated with its predictor variables. In a larger sample, the likelihood dominates the prior distribution for regression coefficients and Bayesian hierarchical model used a likelihood-based approach, used priors (prior laboratory and clinical trials data) that provided the assay sensitivities, which through the prior assumptions, made the model identifiable and the integration of Markov chain Monte Carlo (MCMC) estimates the posterior distributions, avoided missing data and complex outcome measurements.\nResults\nThree case studies: pneumonia etiology in children, prostate cancer, and mental disorders chosen for model development, identified low-risk patient population, reduced the risk of overtreatment, complications, adverse effects, and financial burden for patients (Disease Reclassification). Prostate cancer software was then implemented within the JHM HER.\nLimitation:\nModels were entirely parametric, and extensions to nonparametric or more flexible parametric models were recommended to improve approaches for neuroimage or genomic data.\nneuroimage or genomic data.\nApplications:\n\nto scale a tool that addresses a particular unmet need across a larger, more diverse population of patients and clinicians\nuse in autoimmune diseases, sudden cardiac arrest, and diabetes.\nembed a collection of tools to acquire and use the most relevant information, agnostic to its level of measurement, to improve population and individual health decisions that cause better outcomes at more affordable costs. @Zeger2020\n\n\nBayesian Inference (parametric vs non-parametric)\n\nWhat is the goal of the paper?\nThe authors calculated the posterior probability of disease diagnosis and applied Bayesian inference to develop three modules comparing parametric (with a fixed set of parameters) and nonparametric distributions (which do not make a priori assumptions) by analyzing the National Health and Nutrition Examination Survey dataset from two separate diagnostic tests on both diseased and non-diseased populations.\nWhy is it important?\nMedical diagnosis, treatment, and management decisions are crucial, and conventional methods for diagnosis using clinical criteria and fixed numerical thresholds limit the capture of other information related to the intricate relationship between diagnostic tests and the varying prevalence of diseases. The probability distributions associated with quantitative diagnostic test outcomes often demonstrate some overlap between the diseased and nondiseased groups. The dichotomous method fails to capture the complexity and heterogeneity of disease presentations across diverse populations. The applicability of the normal distribution (conventional method) is critiqued, especially in dealing with clinical measurands having skewness, bimodality, or multimodality.\nHow is it solved?\nMethods: The Authors developed models employing Bayesian inference (Bayesian diagnostic approach) to calculate the posterior probability of disease diagnosis in the Wolfram Language and integrated prior probabilities of disease with distributions of diagnostic measurands in both diseased and nondiseased populations. The approach enabled the evaluation of combined data from multiple diagnostic tests and improved the diagnostic accuracy, precision and adaptability, The model showed flexibility, adaptability, and versatility in the diagnostic.\nResults\nNonparametric Bayesian models tend to fit data distributions better, especially given limited existing literature, and are more robust in capturing complex data patterns.\nThese models produce multimodal probability patterns for disease, unlike the bimodal, double-sigmoidal curves seen with parametric models.\nLimitations\n\nReliance on parametric models: A need to extend to nonparametric or more flexible parametric models for medical data.\nLimited scholarly publications and over-dependence on prior probabilities increase uncertainties, resulting in broader confidence intervals for posterior probabilities. Systemic bias (unrepresentative datasets) compromises the accuracy of Bayesian calculations. For Incomplete datasets, Bayesian methods combined with other statistical and computational techniques could enhance diagnostic capabilities.\nThe foundational data is crucial to compare new diagnostic measurements. Absence of normative data compromises the reliability and validity of Bayesian diagnostic methods. @Chatzimichail2023\n\n\nBayesian model stages, development and advantages, and temporal models\nWhat is the goal of the paper?\n\nThe study describes the stages of Bayesian analysis, specifying the importance of the priors, data modeling, inferences, model checking and refinement, selecting a proper sampling technique from a posterior distribution, variational inferences, variable selection and its application across various research fields. The study proposes strategies for reproducibility and reporting standards, outlining an updated WAMBS (when to Worry and how to Avoid the Misuse of Bayesian Statistics) checklist and outlining the impact of Bayesian analysis on artificial intelligence in the future.\nWhy is it important?\nBayesian statistics is suitable for quantitative researchers accross different fields who have at least some knowledge of regression modelling.\nHow is it solved?\nExamples of successful applications of Bayesian analysis across various research fields (social sciences, ecology, genetics, medicine) and the advantages and disadvantages of the Bayesian model are provided here, and overview of the current and future use of Bayesian statistics.\nThe study mention priors into three categories (informative, weakly informative and diffuse) based on the degree of (un)certainty (hyperparameters) surrounding the population parameter. The prior distribution is as - N( μ0 , σ^ 20) where a larger variance represents a greater amount of uncertainty surrounding.\nPrior elicitation (experts, generic expert, data-based, sample data using maximum likelihood or sample statistics, etc) construct a prior distribution.\nPrior sensitivity analysis of the likelihood helps examine different forms of the model, assesses how the priors and the likelihood align and have an impact on posterior estimates, reflecting variations not captured by the prior or the likelihood alone.\nPrior estimation allows data-informed shrinkage, enacts regularization or influence algorithms towards a likely high-density region, and improves estimation efficiency.\nKnowing the exact probabilistic specification of the priors for a complex model with smaller sample sizes is important. A small sample conveys less information compared to the priors that quantify the strength of support the observed data lends to possible value(s) for the unknown parameter(s).\nFrequentists do not consider the probability of the unknown parameters as useful, and they are considered to be fixed; the likelihood is the conditional probability distribution p(y|θ) of the data (y), given fixed parameters (θ). In Bayesian inference, unknown parameters (random variables) have varied values, while the (observed) data have fixed values, and the likelihood is a function of θ for the fixed data y.\nTherefore, the likelihood function summarizes a statistical model that stochastically generates a range of possible values for θ and the observed data y. With priors and the likelihood of the observed data, the resulting posterior distribution provides an estimate of the unknown parameters, capturing the primary factors and improving our understanding. Monte Carlo technique provides integrals of sampled values from a given distribution through computer simulations. The packages BRMS and Blavaan in R are used for the probabilistic programming language Stan.\nVariable selection after checking correlations among the variables in the model (Eg: gene-to-gene interaction) aids in the prediction of genes in biomedical research (genome-wide association studies).\nSpatial and temporal variability are factored in Bayesian general linear models. A posterior distribution can simulate new data conditional on this distribution, assess, and provide valid predictions to be used for extrapolating to future events.\nResults\nThe Bayesian approach analyzes large-scale cancer genomic data, identifies novel molecular changes in cancer initiation and progression, the interactions between mutated genes and captured mutational signatures, highlighting key genetic interactions components, allowing genomic-based patient stratification both in clinical trials, in the personalized use of therapeutics, and in understanding cancer and its evolutionary processes.\nLimitations:\n\nIn temporal models, posterior inference challenges are inherent in the spatial and/or temporal dependencies (autocorrelation of parameters over time). @VandeSchoot2021\n\n\nBayesian Normal linear regression\n\nWhat is the goal of the paper?\nThe author provides guidance on Bayesian inference by performing Bayesian Normal linear regression in metrology to calibrate instruments and to evaluate inter-laboratory comparisons in determining fundamental constants.\nWhy is it important?\nThe measurement errors are assumed to be additive, independent, and identically distributed according to a Gaussian distribution with mean zero and variance σ2, which is usually unknown.\nRegression is used to calibrate instruments, evaluate inter-laboratory comparisons, or determine fundamental constants, but the regression model cannot be uniquely formulated as a measurement function. Guide to the Expression of Uncertainty in Measurement (GUM) and its supplements are not applicable directly.\nHow is it solved?\nMethods: Bayesian inference has the advantage of accounting for additional a priori information, which robustifies the analyses.\nThree steps (prior elicitation, posterior calculation, and robustness to prior uncertainty and model adequacy) and model assumptions are critical to Bayesian inference.\nIn Bayesian inference, all unknowns—observables (data) as well as unobservables (parameters and auxiliary variables) are considered to be random, are assigned probability distributions to summarize the available information, and to update prior knowledge about the unobservables with information about them contained in the data. The prior distribution and likelihood function provided by simple graphical displays, sensitivity analyses, or model checking enhance the elicitation and interpretation process.\nFor Normal linear regression problems\n(1) a family of prior distributions for θ and σ2 is (Normal inverse Gamma (NIG) distribution to a posterior from the same family of (NIG) distributions or\n(2) alternative families of prior distributions (hierarchical priors) assign an additional layer of distributions to uncertain prior parameters or non-para- metric priors.\nThe NIG prior with known variance σ2 of observations is a conjugate prior distribution. Vague or non-informative prior distributions can be derived from the NIG prior.\nBayesian inference is influenced by\n\nthe uncertainty in the transformation of prior knowledge to prior distributions\nthe assumptions of the statistical model\nthe mistakes in data acquisition\n\nResults\nThe knowledge from related previous experiments (Normal inverse Gamma distributions) allow for analytic posterior calculations of many quantities of interest. @Klauenberg2015\n\nBayesian linear regression and priors (exchangeable and unexchangeable)\n\nWhat is the goal of the paper?\nThe study developed a test of a formal method for augmenting data in linear regression analyses, by incorporating both exchangeable and unexchangeable information on regression coefficients (and standard errors) of previous studies.\nWhy is it important?\nThe frequent combination of multiple testing has relatively low statistical power, which is problematic in null-hypothesis significance testing. Linear regression analyses do not account for the published results and summary statistics from similar previous studies. Ignoring information on parameters from previous studies (relevant and readily available), affects the stability and precision of the parameter estimates and results in lower values than they could have been, resulting in conclusions that are less certain and are affected by sampling variation.\nMultiple linear regression with separate significance tests for all regression coefficients, and with the modest sample sizes, different studies find different sets of statistically significant predictors, and addressing the issue on larger samples is practically unrealistic.\nHow is it solved?\nMethods: Bayesian linear regression accommodates prior knowledge, overcoming the absence of formal studies, handles the issues of increasing the sample size, and augments the data of a new study with previous results (regression coefficients and standard errors) from similar studies.\nThe authors used Bayesian linear regression to solve the issue of the univariate case analysis by combining evidence of specific predictors from different linear regression analyses (as in meta-analysis) and found it a better method to resolve the issue of simultaneously combining multiple regression parameters per study, which ignored the relationship between the regression coefficients. Includes summary statistics from previous studies, Bayesian linear regression provided a more acceptable solution when data from previous studies were not (realistically) obtainable.\nModels in the study as categorized the model into - (1) Exchangable - when the current data and previous studies have the same set of predictors. (2) Unexchangable – when the predictors were different in the two.\nTo yield the posterior density that reflects the updated knowledge about the model parameters after having observed the data, the steps to Bayesian linear regression steps are mentioned -\n(1) To calculate the probability density function for the data given the unknown model parameters;\n(2) Taken as a function of those model parameters, the likelihood function is the second part of the prior density function of the model parameters. It quantifies what is assumed to be known about the model parameters before observing the data. The study applied the Standard multiple linear regression model, and with the integration of the prior, provided the joint posterior density using the Gibbs sampler. An Ordinary Least Squares linear regression was then applied to each of these samples to obtain the estimated regression coefficients B and the corresponding standard errors.\n(3) A hierarchical model version was developed in analyzing parameters where studies are not exchangeable.\nResults\nIncorporating priors from previous studies in a linear regression on new data yielded a significantly better parameter estimate with an adequate approximation.\nThe gains in comparison to using just the new data, and the large effects were obtained when the data from previous studies were available, resulting in encouraging performance.\nPerformance of the two versions (exchangeable and unexchangeable) of the replication model was consistently superior to using current data alone.\nThe model developed in the article offers the possibility of obtaining significantly better parameter estimates in a linear regression setting without needing to expend a prohibitive amount of time and energy to obtain data from the previous studies.\nHierarchical version (unexchangeable) of the model offers the advantage of being able to address questions about differences between studies and thus allows for explicit testing of the exchangeability assumption.\nLimitations:\n\nAll studies need to have the same set of predictors.\nThe issue of predictor variables that are correlated. @DeLeeuw2012a\n\n\nBayesian logistic regression (Sequential clinical reasoning approach)\n\nWhat is the goal of the paper?\nThe study aimed to develop models using a longitudinal prospective cohort to predict the risk of incident cardiovascular disease by incorporating demographic features (basic), six metabolic syndrome components (metabolic score), and conventional risk factors (enhanced model). The study participants free of CVD at baseline were followed up over five years, and a Bayesian clinical reasoning model was applied to diagnose new CVD cases.\nWhy is it important?\nEarly diagnosis, prevention, by identifying subjects under the high-risk category for cardiovascular disease (CVD), impacts health interventions.\nLimited availability of molecular information in clinical practice due to being costly and unavailability affects efficient disease diagnosis. To efficiently identify a high-risk population based on the routinely checked biological markers before doing these expensive molecular testsThe requires an alternative approach to analyze data.\nThe Tailored Framingham Risk Score method, for the purpose, is not sufficient because of the differences present in ethnic groups, location, and socio-economic status, as they require the construction of their own models. Heterogeneity (geographic, ethnic group, variations, and different characteristics of social contextual network) often is unobservable and unmeasurable.\nHow is it solved?\nMethods: The study evaluated subjects enrolled in a Keelung Community-based Integrated Screening (KCIS) Program, for mass screening (20–79 years) in the Keelung city of Taiwan, who were followed for 5 years to identify incident cancers and chronic diseases (cardiovascular disease).\nThe study classified the risk of having incident CVD cases or death from CVD by dint of available calculated standardized risk score of the MetS components (fasting glucose, blood pressure, HDL-C, triglyceride and waist circumference) together with conventional risk factors (gender, heredity, smoking, alcohol drinking, family history of parent’s CVD and betel quid and other relevant factors).\nEmulating a clinician’s evaluation process, the Bayesian clinical reasoning approach in a sequential manner was applied and three models were developed. The approach considered the normal distribution of regression coefficients of all predictors, allowing for uncertainty of clinical weights. The credible intervals of predicted risk estimates were obtained by averaging out. In the model, the individual risk is elicited by prior speculation (first impression) that is updated by objective observed data (patient’s history and laboratory findings), the regression coefficients for computing risk score were treated as random variable with a certain statistical distribution (e.g. normal distribution) rather than a fixed value (traditional risk prediction model by frequentist). The updated prior distribution with the likelihood of the current data provided a posterior distribution to predict the risk for a specific disease. The sequential approach included -\n\nThe (basic) basic model developed via logistic regression used prior information constructed on gender, age, age2, and time period.\nThe second (classical) model (metabolic score model: MS model) was based on six MetS components.\nThe third (enhanced model) incorporated information on smoking, drinking, betel-quid, and family history of CVD.\n\nResults\nCompared to the basic model and classical model, the enhanced model had better performance. The proposed models predicted CVD incidence at the individual level by incorporating routine information with a sequential Bayesian clinical reasoning approach. Patients’ background significantly contributes to baseline risk. Even with ecological heterogeneity, the regression model adopts individual characteristics and makes individual risk prediction for the CVD incidence.\nLimitations:\n\nWhether the interactions between age, gender, metabolic score, and other risk factors should be included.\nThe use of an enhanced model should be validated through external validation by applying the proposed models to new subjects not included in the training of the model parameters. @Liu2013"
  },
  {
    "objectID": "Summaries/nm/DATA.html",
    "href": "Summaries/nm/DATA.html",
    "title": "Bayesian Logistic Regression Capstone",
    "section": "",
    "text": "Data Source: NHANES merging the below files make 10175 sample size but have to clean it so maybe we will reduce the sample size 1. DEMO_H.xpt 2. DSQTOT_H.xpt 3. BMX_H.xpt 4. TCHOL_H.xpt 5. CDQ_H.xpt 6. SMQ_H.xpt 7. MCQ_H.xpt"
  },
  {
    "objectID": "Summaries/nm/My articles.html",
    "href": "Summaries/nm/My articles.html",
    "title": "Bayesian Logistic Regression Capstone",
    "section": "",
    "text": "Namita’s Literature"
  },
  {
    "objectID": "Summaries/nm/My articles.html#introduction",
    "href": "Summaries/nm/My articles.html#introduction",
    "title": "Bayesian Logistic Regression Capstone",
    "section": "Introduction",
    "text": "Introduction\n\nBayesian Hierarchical Model (Disease reclassification and prediction)\n\nWhat is the goal of the paper?\nThe authors develop a Bayesian hierarchical model for multivariate longitudinal data to predict health status, trajectories, and intervention effects at the individual level in the PCORI mission to address questions about health status from patients and clinicians.\nWhy is it important?\nHealthcare data (DNA sequences, functional images of the brain, patient-reported outcomes, and electronic health records with patients’ sequences of health measurements, diagnoses, and treatments) are complex, and the standard approaches are not adequate for clinical data analysis. Electronic health records (EHRs) could improve diagnostic accuracy and predict treatment effects. Visualizations of characteristics of posterior distributions can be immediately understood by clinicians and patients as relevant to their decision. Combining prior knowledge and patient data with evidence could predict the patient’s health status, trajectory, and/or likely benefits of interventions.\nHow is it solved?\nMethod: The authors applied Bayesian hierarchical regression for multivariate longitudinal patient data using open-source R-packages and developed 2 levels—time within person and persons within a population\nThe model combined exogenous (eg, age, clinical history) factors and endogenous (eg, current treatment) variables on the individual’s multivariate health measurements and the effects of health measurements at one time on subsequent interventions.\nThe model produced an estimate of the posterior distribution for each value of the predictor variable and an estimate of the marginal distribution of the regression coefficients for each coefficient that measures the outcome (health status) associated with its predictor variables. In a larger sample, the likelihood dominates the prior distribution for regression coefficients and Bayesian hierarchical model used a likelihood-based approach, used priors (prior laboratory and clinical trials data) that provided the assay sensitivities, which through the prior assumptions, made the model identifiable and the integration of Markov chain Monte Carlo (MCMC) estimates the posterior distributions, avoided missing data and complex outcome measurements.\nResults\nThree case studies: pneumonia etiology in children, prostate cancer, and mental disorders chosen for model development, identified low-risk patient population, reduced the risk of overtreatment, complications, adverse effects, and financial burden for patients (Disease Reclassification). Prostate cancer software was then implemented within the JHM HER.\nLimitation:\nModels were entirely parametric, and extensions to nonparametric or more flexible parametric models were recommended to improve approaches for neuroimage or genomic data.\nneuroimage or genomic data.\nApplications:\n\nto scale a tool that addresses a particular unmet need across a larger, more diverse population of patients and clinicians\nuse in autoimmune diseases, sudden cardiac arrest, and diabetes.\nembed a collection of tools to acquire and use the most relevant information, agnostic to its level of measurement, to improve population and individual health decisions that cause better outcomes at more affordable costs. @Zeger2020\n\n\nBayesian Inference (parametric vs non-parametric)\n\nWhat is the goal of the paper?\nThe authors calculated the posterior probability of disease diagnosis and applied Bayesian inference to develop three modules comparing parametric (with a fixed set of parameters) and nonparametric distributions (which do not make a priori assumptions) by analyzing the National Health and Nutrition Examination Survey dataset from two separate diagnostic tests on both diseased and non-diseased populations.\nWhy is it important?\nMedical diagnosis, treatment, and management decisions are crucial, and conventional methods for diagnosis using clinical criteria and fixed numerical thresholds limit the capture of other information related to the intricate relationship between diagnostic tests and the varying prevalence of diseases. The probability distributions associated with quantitative diagnostic test outcomes often demonstrate some overlap between the diseased and nondiseased groups. The dichotomous method fails to capture the complexity and heterogeneity of disease presentations across diverse populations. The applicability of the normal distribution (conventional method) is critiqued, especially in dealing with clinical measurands having skewness, bimodality, or multimodality.\nHow is it solved?\nMethods: The Authors developed models employing Bayesian inference (Bayesian diagnostic approach) to calculate the posterior probability of disease diagnosis in the Wolfram Language and integrated prior probabilities of disease with distributions of diagnostic measurands in both diseased and nondiseased populations. The approach enabled the evaluation of combined data from multiple diagnostic tests and improved the diagnostic accuracy, precision and adaptability, The model showed flexibility, adaptability, and versatility in the diagnostic.\nResults\nNonparametric Bayesian models tend to fit data distributions better, especially given limited existing literature, and are more robust in capturing complex data patterns.\nThese models produce multimodal probability patterns for disease, unlike the bimodal, double-sigmoidal curves seen with parametric models.\nLimitations\n\nReliance on parametric models: A need to extend to nonparametric or more flexible parametric models for medical data.\nLimited scholarly publications and over-dependence on prior probabilities increase uncertainties, resulting in broader confidence intervals for posterior probabilities. Systemic bias (unrepresentative datasets) compromises the accuracy of Bayesian calculations. For Incomplete datasets, Bayesian methods combined with other statistical and computational techniques could enhance diagnostic capabilities.\nThe foundational data is crucial to compare new diagnostic measurements. Absence of normative data compromises the reliability and validity of Bayesian diagnostic methods. @Chatzimichail2023\n\n\nBayesian model stages, development and advantages, and temporal models\nWhat is the goal of the paper?\n\nThe study describes the stages of Bayesian analysis, specifying the importance of the priors, data modeling, inferences, model checking and refinement, selecting a proper sampling technique from a posterior distribution, variational inferences, variable selection and its application across various research fields. The study proposes strategies for reproducibility and reporting standards, outlining an updated WAMBS (when to Worry and how to Avoid the Misuse of Bayesian Statistics) checklist and outlining the impact of Bayesian analysis on artificial intelligence in the future.\nWhy is it important?\nBayesian statistics is suitable for quantitative researchers accross different fields who have at least some knowledge of regression modelling.\nHow is it solved?\nExamples of successful applications of Bayesian analysis across various research fields (social sciences, ecology, genetics, medicine) and the advantages and disadvantages of the Bayesian model are provided here, and overview of the current and future use of Bayesian statistics.\nThe study mention priors into three categories (informative, weakly informative and diffuse) based on the degree of (un)certainty (hyperparameters) surrounding the population parameter. The prior distribution is as - N( μ0 , σ^ 20) where a larger variance represents a greater amount of uncertainty surrounding.\nPrior elicitation (experts, generic expert, data-based, sample data using maximum likelihood or sample statistics, etc) construct a prior distribution.\nPrior sensitivity analysis of the likelihood helps examine different forms of the model, assesses how the priors and the likelihood align and have an impact on posterior estimates, reflecting variations not captured by the prior or the likelihood alone.\nPrior estimation allows data-informed shrinkage, enacts regularization or influence algorithms towards a likely high-density region, and improves estimation efficiency.\nKnowing the exact probabilistic specification of the priors for a complex model with smaller sample sizes is important. A small sample conveys less information compared to the priors that quantify the strength of support the observed data lends to possible value(s) for the unknown parameter(s).\nFrequentists do not consider the probability of the unknown parameters as useful, and they are considered to be fixed; the likelihood is the conditional probability distribution p(y|θ) of the data (y), given fixed parameters (θ). In Bayesian inference, unknown parameters (random variables) have varied values, while the (observed) data have fixed values, and the likelihood is a function of θ for the fixed data y.\nTherefore, the likelihood function summarizes a statistical model that stochastically generates a range of possible values for θ and the observed data y. With priors and the likelihood of the observed data, the resulting posterior distribution provides an estimate of the unknown parameters, capturing the primary factors and improving our understanding. Monte Carlo technique provides integrals of sampled values from a given distribution through computer simulations. The packages BRMS and Blavaan in R are used for the probabilistic programming language Stan.\nVariable selection after checking correlations among the variables in the model (Eg: gene-to-gene interaction) aids in the prediction of genes in biomedical research (genome-wide association studies).\nSpatial and temporal variability are factored in Bayesian general linear models. A posterior distribution can simulate new data conditional on this distribution, assess, and provide valid predictions to be used for extrapolating to future events.\nResults\nThe Bayesian approach analyzes large-scale cancer genomic data, identifies novel molecular changes in cancer initiation and progression, the interactions between mutated genes and captured mutational signatures, highlighting key genetic interactions components, allowing genomic-based patient stratification both in clinical trials, in the personalized use of therapeutics, and in understanding cancer and its evolutionary processes.\nLimitations:\n\nIn temporal models, posterior inference challenges are inherent in the spatial and/or temporal dependencies (autocorrelation of parameters over time). @VandeSchoot2021\n\n\nBayesian Normal linear regression\n\nWhat is the goal of the paper?\nThe author provides guidance on Bayesian inference by performing Bayesian Normal linear regression in metrology to calibrate instruments and to evaluate inter-laboratory comparisons in determining fundamental constants.\nWhy is it important?\nThe measurement errors are assumed to be additive, independent, and identically distributed according to a Gaussian distribution with mean zero and variance σ2, which is usually unknown.\nRegression is used to calibrate instruments, evaluate inter-laboratory comparisons, or determine fundamental constants, but the regression model cannot be uniquely formulated as a measurement function. Guide to the Expression of Uncertainty in Measurement (GUM) and its supplements are not applicable directly.\nHow is it solved?\nMethods: Bayesian inference has the advantage of accounting for additional a priori information, which robustifies the analyses.\nThree steps (prior elicitation, posterior calculation, and robustness to prior uncertainty and model adequacy) and model assumptions are critical to Bayesian inference.\nIn Bayesian inference, all unknowns—observables (data) as well as unobservables (parameters and auxiliary variables) are considered to be random, are assigned probability distributions to summarize the available information, and to update prior knowledge about the unobservables with information about them contained in the data. The prior distribution and likelihood function provided by simple graphical displays, sensitivity analyses, or model checking enhance the elicitation and interpretation process.\nFor Normal linear regression problems\n(1) a family of prior distributions for θ and σ2 is (Normal inverse Gamma (NIG) distribution to a posterior from the same family of (NIG) distributions or\n(2) alternative families of prior distributions (hierarchical priors) assign an additional layer of distributions to uncertain prior parameters or non-para- metric priors.\nThe NIG prior with known variance σ2 of observations is a conjugate prior distribution. Vague or non-informative prior distributions can be derived from the NIG prior.\nBayesian inference is influenced by\n\nthe uncertainty in the transformation of prior knowledge to prior distributions\nthe assumptions of the statistical model\nthe mistakes in data acquisition\n\nResults\nThe knowledge from related previous experiments (Normal inverse Gamma distributions) allow for analytic posterior calculations of many quantities of interest. @Klauenberg2015\n\nBayesian linear regression and priors (exchangeable and unexchangeable)\n\nWhat is the goal of the paper?\nThe study developed a test of a formal method for augmenting data in linear regression analyses, by incorporating both exchangeable and unexchangeable information on regression coefficients (and standard errors) of previous studies.\nWhy is it important?\nThe frequent combination of multiple testing has relatively low statistical power, which is problematic in null-hypothesis significance testing. Linear regression analyses do not account for the published results and summary statistics from similar previous studies. Ignoring information on parameters from previous studies (relevant and readily available), affects the stability and precision of the parameter estimates and results in lower values than they could have been, resulting in conclusions that are less certain and are affected by sampling variation.\nMultiple linear regression with separate significance tests for all regression coefficients, and with the modest sample sizes, different studies find different sets of statistically significant predictors, and addressing the issue on larger samples is practically unrealistic.\nHow is it solved?\nMethods: Bayesian linear regression accommodates prior knowledge, overcoming the absence of formal studies, handles the issues of increasing the sample size, and augments the data of a new study with previous results (regression coefficients and standard errors) from similar studies.\nThe authors used Bayesian linear regression to solve the issue of the univariate case analysis by combining evidence of specific predictors from different linear regression analyses (as in meta-analysis) and found it a better method to resolve the issue of simultaneously combining multiple regression parameters per study, which ignored the relationship between the regression coefficients. Includes summary statistics from previous studies, Bayesian linear regression provided a more acceptable solution when data from previous studies were not (realistically) obtainable.\nModels in the study as categorized the model into - (1) Exchangable - when the current data and previous studies have the same set of predictors. (2) Unexchangable – when the predictors were different in the two.\nTo yield the posterior density that reflects the updated knowledge about the model parameters after having observed the data, the steps to Bayesian linear regression steps are mentioned -\n(1) To calculate the probability density function for the data given the unknown model parameters;\n(2) Taken as a function of those model parameters, the likelihood function is the second part of the prior density function of the model parameters. It quantifies what is assumed to be known about the model parameters before observing the data. The study applied the Standard multiple linear regression model, and with the integration of the prior, provided the joint posterior density using the Gibbs sampler. An Ordinary Least Squares linear regression was then applied to each of these samples to obtain the estimated regression coefficients B and the corresponding standard errors.\n(3) A hierarchical model version was developed in analyzing parameters where studies are not exchangeable.\nResults\nIncorporating priors from previous studies in a linear regression on new data yielded a significantly better parameter estimate with an adequate approximation.\nThe gains in comparison to using just the new data, and the large effects were obtained when the data from previous studies were available, resulting in encouraging performance.\nPerformance of the two versions (exchangeable and unexchangeable) of the replication model was consistently superior to using current data alone.\nThe model developed in the article offers the possibility of obtaining significantly better parameter estimates in a linear regression setting without needing to expend a prohibitive amount of time and energy to obtain data from the previous studies.\nHierarchical version (unexchangeable) of the model offers the advantage of being able to address questions about differences between studies and thus allows for explicit testing of the exchangeability assumption.\nLimitations:\n\nAll studies need to have the same set of predictors.\nThe issue of predictor variables that are correlated. @DeLeeuw2012a\n\n\nBayesian logistic regression (Sequential clinical reasoning approach)\n\nWhat is the goal of the paper?\nThe study aimed to develop models using a longitudinal prospective cohort to predict the risk of incident cardiovascular disease by incorporating demographic features (basic), six metabolic syndrome components (metabolic score), and conventional risk factors (enhanced model). The study participants free of CVD at baseline were followed up over five years, and a Bayesian clinical reasoning model was applied to diagnose new CVD cases.\nWhy is it important?\nEarly diagnosis, prevention, by identifying subjects under the high-risk category for cardiovascular disease (CVD), impacts health interventions.\nLimited availability of molecular information in clinical practice due to being costly and unavailability affects efficient disease diagnosis. To efficiently identify a high-risk population based on the routinely checked biological markers before doing these expensive molecular testsThe requires an alternative approach to analyze data.\nThe Tailored Framingham Risk Score method, for the purpose, is not sufficient because of the differences present in ethnic groups, location, and socio-economic status, as they require the construction of their own models. Heterogeneity (geographic, ethnic group, variations, and different characteristics of social contextual network) often is unobservable and unmeasurable.\nHow is it solved?\nMethods: The study evaluated subjects enrolled in a Keelung Community-based Integrated Screening (KCIS) Program, for mass screening (20–79 years) in the Keelung city of Taiwan, who were followed for 5 years to identify incident cancers and chronic diseases (cardiovascular disease).\nThe study classified the risk of having incident CVD cases or death from CVD by dint of available calculated standardized risk score of the MetS components (fasting glucose, blood pressure, HDL-C, triglyceride and waist circumference) together with conventional risk factors (gender, heredity, smoking, alcohol drinking, family history of parent’s CVD and betel quid and other relevant factors).\nEmulating a clinician’s evaluation process, the Bayesian clinical reasoning approach in a sequential manner was applied and three models were developed. The approach considered the normal distribution of regression coefficients of all predictors, allowing for uncertainty of clinical weights. The credible intervals of predicted risk estimates were obtained by averaging out. In the model, the individual risk is elicited by prior speculation (first impression) that is updated by objective observed data (patient’s history and laboratory findings), the regression coefficients for computing risk score were treated as random variable with a certain statistical distribution (e.g. normal distribution) rather than a fixed value (traditional risk prediction model by frequentist). The updated prior distribution with the likelihood of the current data provided a posterior distribution to predict the risk for a specific disease. The sequential approach included -\n\nThe (basic) basic model developed via logistic regression used prior information constructed on gender, age, age2, and time period.\nThe second (classical) model (metabolic score model: MS model) was based on six MetS components.\nThe third (enhanced model) incorporated information on smoking, drinking, betel-quid, and family history of CVD.\n\nResults\nCompared to the basic model and classical model, the enhanced model had better performance. The proposed models predicted CVD incidence at the individual level by incorporating routine information with a sequential Bayesian clinical reasoning approach. Patients’ background significantly contributes to baseline risk. Even with ecological heterogeneity, the regression model adopts individual characteristics and makes individual risk prediction for the CVD incidence.\nLimitations:\n\nWhether the interactions between age, gender, metabolic score, and other risk factors should be included.\nThe use of an enhanced model should be validated through external validation by applying the proposed models to new subjects not included in the training of the model parameters. @Liu2013"
  },
  {
    "objectID": "Summaries/et/paper1.html",
    "href": "Summaries/et/paper1.html",
    "title": "Bayesian Nonparametric Regression for Healthcare Claims Modeling",
    "section": "",
    "text": "Bayesian Nonparametric Regression for Healthcare Claims Modeling\nRichardson, R., & Hartman, B. (2018). Bayesian nonparametric regression models for modeling and predicting healthcare claims. Insurance, Mathematics & Economics, 79, 1-13.\n\n\nSummary\nThis paper introduces Bayesian nonparametric regression models to effectively model and predict healthcare claims data, which often exhibit complex characteristics such as skewness, heavy tails, and excess zeros. The authors propose using Dirichlet process mixtures to flexibly capture the underlying distribution of healthcare claims without assuming a specific parametric form. The models are designed to handle the unique features of healthcare claims data, including the presence of many zero claims (non-claimants) and the variability in claim amounts among claimants.\nThe authors apply their models to a real-world dataset of healthcare claims, demonstrating the models’ ability to accurately predict claim amounts and identify high-risk individuals. The Bayesian framework allows for the incorporation of prior information and provides a coherent way to quantify uncertainty in predictions. The results show that the proposed nonparametric regression models outperform traditional parametric models in terms of predictive accuracy and flexibility.\n\n\nProblem\n\nStandard regression limitations\n\nInsufficient for complex relationships in healthcare claims data\nAssumes Gaussianity, independence, linearity\nAssumptions often not met in insurance data\nDifficulty in capturing skewness, heavy tails, outliers, and bimodality present in claims data\n\n\n\n\nSolution\n\nBayesian nonparametric regression models\n\nFlexible regression model\nRelaxes Normality and linearity assumptions\nPoweful tool for non-Gaussian densities\nIncreased predictive accuracy\nHandles complex error distribution characteristics\nApplicable to all insurance regression problems\n\n\n\n\nMethodology\n\nDependent Dirichlet Process (DDP) ANOVA Model\n\nDirichlet Process (DP) (3.1)\n\nStandard building block for Bayesian nonparametric models\nConstructed via stick-breaking process\nDiscrete distribution with countably infinite atoms\nDP mixture of normals for continuous settings\nAllows for infinite mixture models\n\nDDP (3.2)\n\nBasis for fully nonparametric regression model\nPrior on space of random probability measures\nPoint masses are realizations of stochastic processes\nInfinite mixture of Gaussian processes\n\nDDP ANOVA model (3.3)\n\nExtends DDP to include covariate information\nAtoms are regression coefficients and covariance\nFlexible relationships and error structure\nUses Gibbs sampling for posterior inference\n\n\n\n\n\nApplication\n\nHealthcare claims by ETG\n\nDataset: Episode Treatment Groups (ETG) from a large health insurer\nETG\n\nClassification system for medical conditions\nPredicts healthcare costs\nUncertainty in cost predictions is important\n\nCovariates\n\nage, gender, healthcare charges\n\nFocus on prediction of new observations\n\n\n\n\nResults and performance\n\nOutperforms standard linear model\nOutperforms Generalized Beta 2 (GB2) regression (all but 11 of 347 ETGs)\nBetter predictive accuracy\nDIC (Deviance Information Criterion) consistently lower for BNP\nLower averarge CRPS for out-of-sample predictions\nAccurately captures tail behavior (useful for reserving/risk assessment)\n\n\n\nCase studies\n\nConjunctivitis\n\nLarge dataset (160,000+ observations)\nDistribution is highly non-Gaussian, bimodal\nBNP captures gender effect, standard models fail\nLowest DIC (10,600 vs 12,600 for BLM and 12,300 for GB2)\nAccurately predicts extra mode in left tail\n\nLung transplant\n\nSmaller dataset\nData skewed, Gaussian assumption inappropriate\nBNP/GB2 predict thicker tail for outliers\nGB2 slightly better DIC (1002 vs 1005 for BNP, and 1080 for BLM)\nLess advantage from flexible models in smaller datasets\n\n\n\n\nLimitations\n\nBNP more computationally intensive than standard linear regression\nBut BNP regression scales at the same reate as linear regression\n\n\n\nConclusion\n\nPowerful tool for healthcare claims modeling\nVastly outperforms linear and GB2 regression\nUseful for reserving due to improved distributional fit\nDDP ANOVA useful for continuous independent variables\nCan be extended(e.g. covariate-dependent weights)\nAvailable in R package DPpackage"
  },
  {
    "objectID": "Summaries/et/paper4.html",
    "href": "Summaries/et/paper4.html",
    "title": "Bayesian Logistic Regression Capstone",
    "section": "",
    "text": "What is BART-Survival?\n\nIt’s a Python software package developed by the CDC.\nThe package is for survival analysis: studying time-to-event data (how long until something happens, or doesn’t happen).\nIt uses a machine learning method called Bayesian Additive Regression Trees (BART).\n\nHow does it work (in simple terms)? - “Discrete-time” means it splits up time into chunks (for example, weeks, months, etc.). The model checks at each time‐point whether the event has happened yet or not. - BART is “non-parametric”, which means the model doesn’t assume a fixed mathematical form (like linear or exponential) for how risk changes over time. It lets the data itself shape the risk patterns more flexibly. - Because it’s Bayesian, it gives not just a prediction but also measures of uncertainty (how confident the model is) about those predictions.\nWhat makes it useful / better in some cases - Traditional survival analysis (like the Cox model) often assumes certain things (e.g. that hazards are proportional over time). If those assumptions are wrong, the models can be misleading. BART-Survival is more flexible and can perform better when assumptions of older methods fail. - It provides a friendly API, and tools to dive deeper into the model (e.g. inspecting results, uncertainty) when needed.\nWhen / where we might use it - When you have data about when events happen (or don’t happen) and want to model that. For example, time until recovery, time until equipment failure, time until some event in public health etc. - When you suspect that the standard assumptions of simpler survival models may not hold (e.g. that risk doesn’t change proportionally over time). - When you want flexible models that can give you both predictions and uncertainty.\n\nReference\n\nSparapani, R. A., Logan, B. R., McCulloch, R., & Laud, P. W. (2020). Nonparametric survival analysis using Bayesian additive regression trees (BART). Statistics in Medicine, 39(20), 2526-2546. https://doi.org/10.1002/sim.8523\nBART-Survival GitHub Repository"
  },
  {
    "objectID": "Summaries/et/paper2.html",
    "href": "Summaries/et/paper2.html",
    "title": "Bayesian parametric models for survival prediction in medical applications",
    "section": "",
    "text": "Bayesian parametric models for survival prediction in medical applications\nIwan Paolucci, Yuan-Mao Lin, Jessica Albuquerque Marques Silva, Kristy K. Brock & Bruno C. Odisio\nBMC Medical Research Methodology volume 23, Article number: 250 (2023)\nThis research article, published in BMC Medical Research Methodology, introduces and evaluates Bayesian parametric survival models for predicting patient outcomes in medical applications. The authors, Paolucci et al., highlight the advantages of Bayesian models, such as their ability to provide uncertainty measures, require less hyperparameter tuning, and offer a natural mechanism for model updating using Bayes’ rule without needing original training data due to privacy concerns. The study compares these Bayesian models against conventional survival prediction methods like Cox Proportional Hazards and Random Survival Forests, demonstrating comparable performance while exhibiting less overfitting. The article details the mathematical background of these models, their implementation, and presents results from experiments on various public medical datasets to support their utility in personalized medicine.\n\nProblem\nSurvival analysis is crucial in medical research for predicting patient outcomes, such as time until death or disease recurrence. Traditional models like Cox Proportional Hazards (CoxPH) and Random Survival Forests (RSF) have limitations, including assumptions about hazard ratios and challenges with interpretability. Bayesian parametric models offer a promising alternative by providing uncertainty measures, requiring less hyperparameter tuning, and allowing for model updates without needing original training data, which is beneficial for privacy concerns.\nA major gap in current predictive models for medical applications is the lack of a measure of uncertainty associated with predictions, which is crucial for physicians making high-stakes clinical decisions, especially when treatments have differing side effect profiles or costs.\nThe study tackles practical and technical challenges inherent in medical machine learning. Medical datasets are often limited in size due to the subclassification of diseases, making models highly prone to overfitting. Many existing machine learning algorithms also require extensive hyperparameter tuning to implement regularization and prevent this overfitting.\n\n\nMethodology\nThe authors introduce Bayesian parametric survival models, which are based on the assumption that survival times follow a specific statistical distribution (e.g., Exponential, Weibull, Lognormal). These models use Bayesian inference to estimate the parameters of the chosen distribution, allowing for the incorporation of prior knowledge and the quantification of uncertainty in predictions.\n\nBayesian Parametric Survival Models (BPS)\n\nImplemented using the PyMC library in Python.\nModels include Exponential and Weibull distributions.\nParameters are estimated using Linear combinations and neural networks to predict the parameters of the distributions.\n\nTraining\n\nPyMC framework\nBayes Rule for updating (posterior as prior)\n\nEvaluation\n\nPublic Datasets: AIDS Clinical Trials Group (ACTG), German Breast Cancer Study (GBCS), Veteran lung cancer (Veteran), Worcester Heart Attack Study (WHAS), and primary biliary cirrhosis (PBC)\nExperiment 1: Comparison of Bayesian models with CoxPH, RSF, and DeepSurv\nExperiment 2: Model updating vs retraining from scratch\nExperiment 3: Impact of dataset size on model performance\nMetrics: Concordance Index (C-index), Integrated Brier Score (IBS)\nComparison with Cox Proportional Hazards, Random Survival Forests, and DeepSurv\n\n\n\n\nResults\nThe results demonstrate that Bayesian parametric survival models perform comparably to traditional methods like CoxPH and RSF, with some advantages in terms of reduced overfitting and the ability to provide uncertainty estimates. The models also showed robustness when updating with new data, maintaining performance without needing to retrain from scratch.\n\nPerformance comparison\n\nBPS models performed well, no consistent best model\nBPS Wb NN superior for PBC dataset\n\nOverfitting\n\nBPS and CoxPH & Weibull based models overfit least\nDeepSurv & RSF showed more overfitting\n\nUncertainty estimation beneficial for clinical decision-making\n\n\n\nConclusion\n\nBPS models are competitive, robust, and well-suited for medical applications.\nKey advantages: less overfitting, uncertainty quantification, reduced hyperparameter tuning.\nEfficient model updating without original data (preserves privacy).\nLimitations: datasets used in the study are relatively small, computation time not compared."
  },
  {
    "objectID": "index.html#bayesian-logistic-regression-2",
    "href": "index.html#bayesian-logistic-regression-2",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk Using NHANES 2013–2014 Data",
    "section": "Bayesian Logistic Regression",
    "text": "Bayesian Logistic Regression\nBayesian logistic regression was used to quantify parameter uncertainty and compare posterior estimates with the survey-weighted and MICE models. Weakly informative priors were applied to regularize estimates while preserving flexibility in inference.\nModel Specifications: - Family: Bernoulli with logit link\n- Data: adult_imp1 (N = 5,592)\n- Chains: 4 (2,000 iterations each; 1,000 warmup)\n- Adaptation delta: 0.95\n- Weights: Normalized NHANES examination weights (wt_norm, mean ≈ 1.00, SD ≈ 0.79)\n- Predictors: Standardized age, BMI, sex, and race\n\nDefine Model and Priors\n\n\nCode\nfml_bayes &lt;- diabetes_dx | weights(wt_norm) ~ age_c + bmi_c + sex + race\n\npriors &lt;- c(\n  brms::set_prior(\"normal(0, 2.5)\", class = \"b\"),\n  brms::set_prior(\"student_t(3, 0, 10)\", class = \"Intercept\")\n)\n\n\n\n\nCode\nadult_long &lt;- adult_imp1 %&gt;%\ndplyr::select(bmi_c, age_c) %&gt;%\ntidyr::pivot_longer(\ncols = dplyr::everything(),\nnames_to = \"Coefficient\",\nvalues_to = \"Value\"\n)\n\nggplot2::ggplot(adult_long, ggplot2::aes(x = Value, fill = Coefficient)) +\nggplot2::geom_density(alpha = 0.5) +\nggplot2::theme_minimal() +\nggplot2::labs(\ntitle = \"Distributions for Standardized Age and BMI (adult_imp1)\",\nx = \"Standardized value (z-score)\",\ny = \"Density\",\nfill = \"Coefficient\"\n)\n\n\n\n\n\nDistribution of standardized age (age_c) and BMI (bmi_c) in the imputed dataset (adult_imp1). Both variables were mean-centered and scaled (z-scores) for inclusion in regression models. The overlapping density curves indicate approximate normality and comparable variance, supporting suitability for standardized coefficient estimation.\n\n\n\n\n\n\nCode\nprior_draws &lt;- tibble::tibble(\nterm = rep(c(\"Age (per 1 SD)\", \"BMI (per 1 SD)\"), each = 4000),\nvalue = c(\nstats::rnorm(4000, mean = 0, sd = 2.5),\nstats::rnorm(4000, mean = 0, sd = 2.5)\n)\n)\n\nggplot2::ggplot(prior_draws, ggplot2::aes(x = value, fill = term)) +\nggplot2::geom_density(alpha = 0.5) +\nggplot2::theme_minimal() +\nggplot2::labs(\ntitle = \"Prior Distributions for Age and BMI Coefficients\",\nx = \"Coefficient value\",\ny = \"Density\",\nfill = NULL\n)\n\n\n\n\n\nPrior distributions for standardized age and BMI coefficients, assuming Normal(0, 2.5) priors. These weakly informative priors constrain extreme coefficient values while allowing flexibility in posterior estimation, ensuring regularization without strong bias.\n\n\n\n\n\n\nFit the Model\n\n\nCode\nlibrary(brms)\n\npriors &lt;- c(\n  brms::set_prior(\"normal(0, 2.5)\", class = \"b\"),\n  brms::set_prior(\"student_t(3, 0, 10)\", class = \"Intercept\")\n)\n\nbayes_fit &lt;- brms::brm(\n  formula = diabetes_dx | weights(wt_norm) ~ age_c + bmi_c + sex + race,\n  data    = adult_imp1,\n  family  = bernoulli(link = \"logit\"),\n  prior   = priors,\n  chains  = 4, iter = 2000, seed = 123,\n  control = list(adapt_delta = 0.95),\n  refresh = 0\n)\n\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 11.2 seconds.\nChain 2 finished in 10.3 seconds.\nChain 3 finished in 10.7 seconds.\nChain 4 finished in 11.2 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 10.9 seconds.\nTotal execution time: 43.8 seconds.\n\n\nCode\nsummary(bayes_fit)\n\n\n Family: bernoulli \n  Links: mu = logit \nFormula: diabetes_dx | weights(wt_norm) ~ age_c + bmi_c + sex + race \n   Data: adult_imp1 (Number of observations: 5592) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept              -2.66      0.09    -2.83    -2.50 1.00     3548     3512\nage_c                   1.10      0.06     0.98     1.22 1.00     2349     2618\nbmi_c                   0.63      0.05     0.54     0.72 1.00     3327     2826\nsexFemale              -0.66      0.10    -0.86    -0.47 1.00     3668     3124\nraceMexicanAmerican     0.69      0.17     0.34     1.03 1.00     3657     2821\nraceOtherHispanic       0.43      0.25    -0.07     0.89 1.00     4242     3014\nraceNHBlack             0.53      0.15     0.23     0.83 1.00     3809     3012\nraceOtherDMulti         0.81      0.19     0.45     1.18 1.00     3948     2809\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\n\nPosterior Odd Ratios (Main Results)\n\n\nCode\nknitr::kable(\ndplyr::mutate(bayes_or, dplyr::across(c(OR, LCL, UCL), ~ round(.x, 2)))\n)\n\n\n\n\nTable 4: Bayesian logistic regression: posterior odds ratios (OR) with 95% credible intervals.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nEstimate\nEst.Error\nQ2.5\nQ97.5\nOR\nLCL\nUCL\n\n\n\n\nIntercept\n-2.6633187\n0.0868613\n-2.8341138\n-2.4958967\n0.07\n0.06\n0.08\n\n\nage_c\n1.0968784\n0.0618886\n0.9783744\n1.2200119\n2.99\n2.66\n3.39\n\n\nbmi_c\n0.6282273\n0.0467939\n0.5366821\n0.7199012\n1.87\n1.71\n2.05\n\n\nsexFemale\n-0.6624742\n0.1034594\n-0.8645869\n-0.4660003\n0.52\n0.42\n0.63\n\n\nraceMexicanAmerican\n0.6898163\n0.1710160\n0.3432716\n1.0298163\n1.99\n1.41\n2.80\n\n\nraceOtherHispanic\n0.4252184\n0.2458586\n-0.0669575\n0.8870126\n1.53\n0.94\n2.43\n\n\nraceNHBlack\n0.5307334\n0.1524774\n0.2283617\n0.8328511\n1.70\n1.26\n2.30\n\n\nraceOtherDMulti\n0.8143883\n0.1876762\n0.4467512\n1.1763335\n2.26\n1.56\n3.24\n\n\n\n\n\n\n\n\nAge and BMI show strong positive associations with diabetes (credible intervals exclude 1).\nFemale sex shows lower odds than male (protective factor).\nNon-White racial groups have higher odds compared with Whites, consistent with known disparities.\nAll model parameters exhibit well-defined, unimodal posteriors with narrow credible intervals.\n\n\nDiagnostics and Model Fit\n\n\nCode\nknitr::kable(as.data.frame(brms::bayes_R2(bayes_fit)))\n\n\n\n\nTable 5: Bayesian R² summary.\n\n\n\n\n\n\n\nEstimate\nEst.Error\nQ2.5\nQ97.5\n\n\n\n\nR2\n0.1316278\n0.0123417\n0.107432\n0.1565549\n\n\n\n\n\n\n\n\n\n\nCode\ndiag &lt;- posterior::summarise_draws(bayes_fit, \"rhat\", \"ess_bulk\", \"ess_tail\")\n\ndiag_b &lt;- diag |&gt;\ndplyr::as_tibble() |&gt;\ndplyr::filter(grepl(\"^b_\", .data$variable)) |&gt;\ndplyr::transmute(\nParameter = .data$variable,\nRhat      = .data$rhat,\nBulk_ESS  = .data$ess_bulk,\nTail_ESS  = .data$ess_tail\n)\n\nknitr::kable(diag_b, digits = 1)\n\n\n\n\nTable 6: MCMC diagnostics (R-hat and Effective Sample Sizes) for model parameters.\n\n\n\n\n\n\nParameter\nRhat\nBulk_ESS\nTail_ESS\n\n\n\n\nb_Intercept\n1\n3548.0\n3511.8\n\n\nb_age_c\n1\n2349.3\n2617.8\n\n\nb_bmi_c\n1\n3327.1\n2825.9\n\n\nb_sexFemale\n1\n3668.1\n3123.7\n\n\nb_raceMexicanAmerican\n1\n3656.6\n2821.2\n\n\nb_raceOtherHispanic\n1\n4242.3\n3013.5\n\n\nb_raceNHBlack\n1\n3809.1\n3012.2\n\n\nb_raceOtherDMulti\n1\n3947.9\n2809.1\n\n\n\n\n\n\n\n\nAll parameters achieved R̂ ≈ 1.00 and effective sample sizes &gt;2,000, indicating excellent convergence. The Bayesian R² ≈ 0.13, showing that age, BMI, sex, and race explain about 13% of diabetes variability.\n\n\nModel Comparison\n\n\nCode\ninvisible(capture.output({\nfit_no_race &lt;- update(bayes_fit, formula = update(fml_bayes, . ~ . - race))\nfit_no_sex  &lt;- update(bayes_fit, formula = update(fml_bayes, . ~ . - sex))\n}))\n\nloo_base    &lt;- loo::loo(bayes_fit)\nloo_no_race &lt;- loo::loo(fit_no_race)\nloo_no_sex  &lt;- loo::loo(fit_no_sex)\n\ncmp_df &lt;- as.data.frame(loo::loo_compare(loo_base, loo_no_race, loo_no_sex))\ncmp_df$Model &lt;- rownames(cmp_df)\ncmp_df &lt;- cmp_df[, c(\"Model\", setdiff(names(cmp_df), \"Model\"))]\n\nknitr::kable(\ncmp_df,\ncaption = \"LOO comparison (higher elpd_loo indicates better predictive performance).\"\n)\n\n\n\nBayesian model comparison (LOO): base model vs. reduced models without race or sex.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nelpd_diff\nse_diff\nelpd_loo\nse_elpd_loo\np_loo\nse_p_loo\nlooic\nse_looic\n\n\n\n\nbayes_fit\nbayes_fit\n0.00000\n0.000000\n-1418.258\n56.42097\n8.732434\n0.5944729\n2836.517\n112.8419\n\n\nfit_no_race\nfit_no_race\n-14.43171\n6.367627\n-1432.690\n53.98749\n5.223838\n0.3831466\n2865.380\n107.9750\n\n\nfit_no_sex\nfit_no_sex\n-20.04611\n8.205833\n-1438.305\n57.31024\n7.359525\n0.5226182\n2876.609\n114.6205\n\n\n\n\n\nModels excluding race or sex had lower expected log predictive density (elpd), confirming that both variables contribute meaningfully to model fit.\n\n\nPosterior Predictive Checks\n\n\nCode\nyobs &lt;- adult_imp1$diabetes_dx\n\n\n\n\nCode\nbayesplot::pp_check(bayes_fit, type = \"bars\", nsamples = 100)\n\n\n\n\n\n\n\n\nFigure 1: Posterior predictive check: observed vs. replicated outcome distribution (bars).\n\n\n\n\n\n\n\nCode\nyrep &lt;- brms::posterior_predict(bayes_fit, ndraws = 400)\nbayesplot::ppc_stat(y = yobs, yrep = yrep, stat = \"mean\")\n\n\n\n\n\n\n\n\nFigure 2: Posterior predictive checks for mean of the binary outcome.\n\n\n\n\n\n\n\nCode\nyrep &lt;- brms::posterior_predict(bayes_fit, ndraws = 400)\nbayesplot::ppc_stat(y = yobs, yrep = yrep, stat = \"sd\")\n\n\n\n\n\n\n\n\nFigure 3: Posterior predictive checks for standard deviation of the binary outcome.\n\n\n\n\n\nPosterior predictive checks show excellent model calibration: simulated means and standard deviations closely match observed data.\n\n\nMCMC Diagnostics and Posterior Distributions\n\n\nCode\nbayesplot::mcmc_areas(as.array(bayes_fit), regex_pars = \"^b_\", prob = 0.95)\n\n\n\n\n\n\n\n\nFigure 4: Posterior distributions (95% credible mass) for slope parameters.\n\n\n\n\n\n\n\nCode\nbayesplot::mcmc_trace(as.array(bayes_fit), regex_pars = \"^b_\")\n\n\n\n\n\n\n\n\nFigure 5: Trace plots for slope parameters (chain mixing and stationarity).\n\n\n\n\n\n\n\nCode\npost_array &lt;- posterior::as_draws_array(bayes_fit)\nbayesplot::mcmc_acf(post_array, pars = c(\"b_age_c\", \"b_bmi_c\"))\n\n\n\n\n\n\n\n\nFigure 6: Autocorrelation plots for posterior samples of age and BMI coefficients (MCMC diagnostics).\n\n\n\n\n\nTrace, density, and autocorrelation plots confirm smooth mixing, unimodal posteriors, and minimal autocorrelation across chains.\nAll four chains showed strong convergence with no signs of divergence or non-stationarity.\nTrace plots revealed stable, overlapping chains with consistent mixing across iterations.\nAutocorrelation decayed rapidly toward zero, confirming efficient sampling and low dependency between successive draws.\nTogether with R̂ ≈ 1.00 and high effective sample sizes, these diagnostics indicate a well-behaved posterior and reliable inference.\n\n\nPrior vs. Posterior\n\n\nCode\nsrc_obj &lt;- if (exists(\"bayes_fit_prior\") && !is.null(bayes_fit_prior)) bayes_fit_prior else bayes_fit\ndraws_names &lt;- names(posterior::as_draws_df(src_obj))\nsort(grep(\"^(b_|prior_b_)\", draws_names, value = TRUE))\n\n\n[1] \"b_age_c\"               \"b_bmi_c\"               \"b_Intercept\"          \n[4] \"b_raceMexicanAmerican\" \"b_raceNHBlack\"         \"b_raceOtherDMulti\"    \n[7] \"b_raceOtherHispanic\"   \"b_sexFemale\"          \n\n\n\n\nCode\nsrc_obj &lt;- if (exists(\"bayes_fit_prior\") && !is.null(bayes_fit_prior)) bayes_fit_prior else bayes_fit\ndraws_df &lt;- posterior::as_draws_df(src_obj)\nall_cols &lt;- names(draws_df)\n\nwant_post &lt;- c(\"b_age_c\",\"b_bmi_c\",\"b_sexFemale\",\"b_raceBlack\",\"b_raceHispanic\",\"b_raceOther\")\n\nhave_post  &lt;- intersect(want_post, all_cols)\nhave_prior &lt;- intersect(paste0(\"prior_\", have_post), all_cols)\n\npairs &lt;- data.frame(post = have_post, prior = paste0(\"prior_\", have_post),\nstringsAsFactors = FALSE)\npairs &lt;- pairs[pairs$prior %in% have_prior, , drop = FALSE]\n\nif (nrow(pairs) == 0) {\nknitr::asis_output(\"**No matching prior/posterior parameters found to overlay.**\")\n} else {\npost_long &lt;- tidyr::pivot_longer(\ndraws_df[, pairs$post, drop = FALSE],\ncols = tidyselect::everything(), names_to = \"term\", values_to = \"estimate\"\n)\npost_long$type &lt;- \"Posterior\"\n\nprior_tmp &lt;- draws_df[, pairs$prior, drop = FALSE]\ncolnames(prior_tmp) &lt;- pairs$post\nprior_long &lt;- tidyr::pivot_longer(\nprior_tmp,\ncols = tidyselect::everything(), names_to = \"term\", values_to = \"estimate\"\n)\nprior_long$type &lt;- \"Prior\"\n\ncombined_draws &lt;&lt;- dplyr::bind_rows(prior_long, post_long)\n\nlbl &lt;- c(\nb_age_c = \"Age (1 SD)\", b_bmi_c = \"BMI (1 SD)\",\nb_sexFemale = \"Female vs Male\",\nb_raceBlack = \"Black vs White\",\nb_raceHispanic = \"Hispanic vs White\",\nb_raceOther = \"Other vs White\"\n)\ncombined_draws$term &lt;- factor(\ncombined_draws$term,\nlevels = intersect(names(lbl), unique(combined_draws$term)),\nlabels = lbl[intersect(names(lbl), unique(combined_draws$term))]\n)\n\nggplot2::ggplot(combined_draws, ggplot2::aes(x = estimate, linetype = type)) +\nggplot2::geom_density() +\nggplot2::facet_wrap(~ term, scales = \"free\", ncol = 2) +\nggplot2::labs(x = \"Coefficient (log-odds)\", y = \"Density\", linetype = NULL) +\nggplot2::theme_minimal()\n}\n\n\n\n\nNo matching prior/posterior parameters found to overlay.\n\n\nFigure 7: Prior (dashed) vs posterior (solid) densities for selected coefficients.\n\n\n\n\n\n\nCode\nif (exists(\"combined_draws\") && is.data.frame(combined_draws) && nrow(combined_draws) &gt; 0) {\nggplot(combined_draws, aes(x = estimate, fill = type)) +\ngeom_density(alpha = 0.4) +\nfacet_wrap(~ term, scales = \"free\", ncol = 2) +\ntheme_minimal(base_size = 13) +\nlabs(\ntitle = \"Prior vs Posterior Distributions\",\nx = \"Coefficient estimate\",\ny = \"Density\",\nfill = \"\"\n)\n} else {\nknitr::asis_output(\"**Skipped: no matching prior/posterior draws to plot.**\")\n}\n\n\n\n\nSkipped: no matching prior/posterior draws to plot.\n\n\nFigure 8: Prior vs Posterior Distributions (ggplot2 version).\n\n\n\n\n\n\nCode\n# Extract posterior draws as a matrix, then convert to tibble\npost &lt;- as_draws_matrix(bayes_fit) %&gt;%   # safer than as_draws_df for manipulation\n  as.data.frame() %&gt;%\n  select(b_bmi_c, b_age_c) %&gt;%\n  pivot_longer(\n    everything(),\n    names_to = \"term\",\n    values_to = \"estimate\"\n  ) %&gt;%\n  mutate(\n    term = case_when(\n      term == \"b_bmi_c\" ~ \"BMI (per 1 SD)\",\n      term == \"b_age_c\" ~ \"Age (per 1 SD)\"\n    ),\n    type = \"Posterior\"\n  )\nprior_draws &lt;- tibble(\n  term = rep(c(\"BMI (per 1 SD)\", \"Age (per 1 SD)\"), each = 4000),\n  estimate = c(rnorm(4000, 0, 1), rnorm(4000, 0, 1)),\n  type = \"Prior\"\n)\ncombined_draws &lt;- bind_rows(prior_draws, post)\n\nggplot(combined_draws, aes(x = estimate, fill = type)) +\n  geom_density(alpha = 0.4) +\n  facet_wrap(~ term, scales = \"free\", ncol = 2) +\n  theme_minimal(base_size = 13) +\n  labs(\n    title = \"Prior vs Posterior Distributions\",\n    x = \"Coefficient estimate\",\n    y = \"Density\",\n    fill = \"\"\n  )\n\n\n\n\n\n\n\n\nFigure 9: Prior vs Posterior Distributions (ggplot2 version).\n\n\n\n\n\nFor age and BMI, the posterior densities shift notably away from the N(0, 2.5) prior toward positive values and are narrower, indicating strong information from the data; for sex, the posterior remains closer to the prior with more overlap, indicating weaker evidence.\nThe overlay of prior and posterior densities illustrates that informative updates occurred primarily for BMI, age, and race coefficients, which showed distinct posterior shifts relative to the priors. In contrast, weaker predictors such as sex displayed overlapping distributions, indicating that inference for those parameters was more influenced by prior uncertainty than by the observed data. This balance confirms appropriate regularization rather than overfitting.\n\n\nModel Fit and Calibration\n\n\nCode\npred_mean &lt;- colMeans(brms::posterior_epred(bayes_fit))\nggplot(data.frame(pred = pred_mean, obs = yobs),\naes(x = pred, y = obs)) +\ngeom_point(alpha = 0.15, position = position_jitter(height = 0.03)) +\ngeom_smooth(method = \"loess\", se = TRUE) +\nlabs(x = \"Mean predicted probability\", y = \"Observed diabetes (0/1)\")\n\n\n\n\n\n\n\n\nFigure 10: Observed outcome vs. mean predicted probability (calibration scatter with smoother).\n\n\n\n\n\n\n\nCode\n# 1. Survey-weighted prevalence\nsvy_mean &lt;- svymean(~diabetes_dx, nhanes_design_adult, na.rm = TRUE)\n\n# 2. Posterior predictive prevalence (per draw)\npp_samples &lt;- brms::posterior_predict(bayes_fit, ndraws = 1000)  # draws x individuals\npp_proportion &lt;- rowMeans(pp_samples)                            # prevalence per draw\n\n# 3. Build comparison table\nsummary_table &lt;- tibble(\n  Method = c(\"Survey-weighted mean (NHANES)\", \n             \"Imputed dataset mean\", \n             \"Posterior predictive mean\"),\n  diabetes_mean = c(\n    coef(svy_mean),                           # survey-weighted mean\n    mean(adult_imp1$diabetes_dx, na.rm = TRUE),  # imputed dataset\n    mean(pp_proportion)                       # posterior predictive mean\n  ),\n  SE = c(\n    SE(svy_mean),   # survey-weighted SE\n    NA,             # not available for raw mean\n    NA              # not available for posterior predictive mean\n  )\n)\n\nkable(summary_table, digits = 4,\n      caption = \"Comparison of Diabetes Prevalence Across Methods\")\n\n\n\n\n\nComparison of Diabetes Prevalence Across Methods\n\n\nMethod\ndiabetes_mean\nSE\n\n\n\n\nSurvey-weighted mean (NHANES)\n0.0889\n0.0048\n\n\nImputed dataset mean\n0.1105\nNA\n\n\nPosterior predictive mean\n0.1093\nNA\n\n\n\n\n\nFigure 11: Posterior predictive distribution of diabetes prevalence compared to observed NHANES prevalence.\n\n\n\n\nThe posterior predictive distribution of diabetes prevalence closely mirrored the survey-estimated prevalence, with the posterior mean aligning within about 1% of the observed rate.\n\n\nCode\n# Posterior predictive prevalence (replicated datasets)\n\nyrep &lt;- brms::posterior_predict(bayes_fit, ndraws = 2000)   # draws x observations (0/1)\npost_prev &lt;- rowMeans(yrep)                                 # prevalence each posterior draw\n\n# Survey-weighted observed prevalence (population estimate)\n\ndes_obs &lt;- survey::svydesign(\nid = ~SDMVPSU, strata = ~SDMVSTRA, weights = ~WTMEC2YR,\nnest = TRUE, data = adult_imp1\n)\nobs &lt;- survey::svymean(~diabetes_dx, des_obs, na.rm = TRUE)\nobs_prev  &lt;- as.numeric(obs[\"diabetes_dx\"])\nobs_se    &lt;- as.numeric(SE(obs)[\"diabetes_dx\"])\nobs_lcl   &lt;- max(0, obs_prev - 1.96 * obs_se)\nobs_ucl   &lt;- min(1, obs_prev + 1.96 * obs_se)\n\n# Plot: posterior density with weighted point estimate and 95% CI band\n\nggplot(data.frame(prev = post_prev), aes(x = prev)) +\ngeom_density(alpha = 0.6) +\nannotate(\"rect\", xmin = obs_lcl, xmax = obs_ucl, ymin = 0, ymax = Inf, alpha = 0.15) +\ngeom_vline(xintercept = obs_prev, linetype = 2) +\ncoord_cartesian(xlim = c(0, 1)) +\nlabs(x = \"Diabetes prevalence\", y = \"Posterior density\",\nsubtitle = sprintf(\"Survey-weighted NHANES prevalence = %.1f%% (95%% CI %.1f–%.1f%%)\",\n100*obs_prev, 100*obs_lcl, 100*obs_ucl)) +\ntheme_minimal()\n\n\n\n\n\n\n\n\nFigure 12: Population (NHANES survey-weighted) vs posterior predictive diabetes prevalence.\n\n\n\n\n\n\n\nCode\n# Posterior predictive draws for the outcome\n\npp_samples &lt;- brms::posterior_predict(bayes_fit, ndraws = 1000)  # draws x individuals\npp_proportion &lt;- rowMeans(pp_samples)                            # prevalence per draw\n\npp_proportion_df &lt;- tibble::tibble(proportion = pp_proportion)\n\nggplot2::ggplot(pp_proportion_df, ggplot2::aes(x = proportion)) +\nggplot2::geom_histogram(binwidth = 0.01, color = \"black\") +\nggplot2::theme_minimal() +\nggplot2::labs(\ntitle = \"Posterior Distribution of Proportion of Diabetes = 1\",\nx = \"Proportion with diabetes\",\ny = \"Frequency\"\n)\n\n\n\n\n\nPosterior distribution of proportion of Diabetes = 1.\n\n\n\n\n\n\nCode\n# Survey-weighted prevalence (already computed earlier as `obs`)\n\nobs_prev &lt;- as.numeric(obs[\"diabetes_dx\"])\nobs_se   &lt;- as.numeric(survey::SE(obs)[\"diabetes_dx\"])\n\nsummary_table &lt;- tibble::tibble(\nMethod = c(\n\"Survey-weighted mean (NHANES)\",\n\"Imputed dataset mean (adult_imp1)\",\n\"Posterior predictive mean (Bayesian)\"\n),\ndiabetes_mean = c(\nobs_prev,\nmean(adult_imp1$diabetes_dx, na.rm = TRUE),\nmean(pp_proportion)\n),\nSE = c(\nobs_se,\nNA_real_,\nNA_real_\n)\n)\n\nknitr::kable(\nsummary_table,\ndigits = 4,\ncaption = \"Comparison of Diabetes Prevalence Across Methods\"\n)\n\n\n\nComparison of Diabetes Prevalence Across Methods.\n\n\nMethod\ndiabetes_mean\nSE\n\n\n\n\nSurvey-weighted mean (NHANES)\n0.0890\nNA\n\n\nImputed dataset mean (adult_imp1)\n0.1105\nNA\n\n\nPosterior predictive mean (Bayesian)\n0.1096\nNA\n\n\n\n\n\n\n\nInternal Validation: Individual-Level Predictions\n\n\nCode\nadult_means &lt;- adult_imp1 %&gt;% summarise(\nage_mean = mean(age, na.rm = TRUE),\nage_sd   = sd(age, na.rm = TRUE),\nbmi_mean = mean(bmi, na.rm = TRUE),\nbmi_sd   = sd(bmi, na.rm = TRUE)\n)\n\nto_model_row &lt;- function(age_raw, bmi_raw, sex_lab, race_lab) {\ntibble(\nage_c  = (age_raw - adult_means$age_mean)/adult_means$age_sd,\nbmi_c  = (bmi_raw - adult_means$bmi_mean)/adult_means$bmi_sd,\nsex    = factor(sex_lab,   levels = levels(adult_imp1$sex)),\nrace  = factor(race_lab, levels = levels(adult_imp1$race)),\nwt_norm = 1\n)\n}\n\nplot_post_density &lt;- function(df_row, title_txt) {\nphat &lt;- posterior_linpred(bayes_fit, newdata = df_row, transform = TRUE)\nci95 &lt;- quantile(phat, c(0.025, 0.975))\nggplot(data.frame(pred = as.numeric(phat)), aes(x = pred)) +\ngeom_density(fill = \"skyblue\", alpha = 0.4) +\ngeom_vline(xintercept = ci95[1], linetype = \"dashed\", color = \"red\") +\ngeom_vline(xintercept = ci95[2], linetype = \"dashed\", color = \"red\") +\nlabs(x = \"P(Diabetes = 1)\", y = \"Density\", title = title_txt) +\ntheme_minimal()\n}\n\np1 &lt;- to_model_row(adult$age[1], adult$bmi[1],\nas.character(adult$sex[1]), as.character(adult$race[1]))\nplot_post_density(p1, \"Participant 1: Posterior Predictive Distribution (95% CrI)\")\n\n\n\n\n\nPosterior predictive distributions for example participants.\n\n\n\n\nPosterior predictive densities for individual participants illustrate uncertainty in diabetes risk estimates. Credible intervals quantify plausible risk ranges for each profile.\n\n\nPosterior Predictions and Inverse Inference\n\n\nCode\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# 1. Grid of BMI values (RAW BMI from 18 to 40)\nbmi_seq &lt;- seq(18, 40, by = 0.5)\n\n# 2. Newdata using the SAME factor levels as adult_imp1\nnewdata_grid &lt;- data.frame(\n  age_c  = 40,   # NOTE: Namita used 40 here even though age_c is standardized\n  bmi_c  = bmi_seq,   # she also used raw BMI in a column named bmi_c\n  sex    = factor(\"Female\",          levels = levels(adult_imp1$sex)),\n  race   = factor(\"Mexican American\", levels = levels(adult_imp1$race)),\n  wt_norm = 1\n)\n\n# 3. Posterior predicted probabilities\npred_probs &lt;- brms::posterior_linpred(\n  bayes_fit,\n  newdata   = newdata_grid,\n  transform = TRUE\n)\n\n# 4. Mean predicted probability at each BMI\nprob_mean &lt;- colMeans(pred_probs)\n\npred_df &lt;- dplyr::bind_cols(newdata_grid, prob_mean = prob_mean)\n\n# 5. Target probability\ntarget_prob &lt;- 0.30\n\n# Find the BMI whose predicted prob is closest to the target\nclosest &lt;- pred_df[which.min(abs(pred_df$prob_mean - target_prob)), , drop = FALSE]\n\n# 6. Plot\nggplot(pred_df, aes(x = bmi_c, y = prob_mean)) +\n  geom_line(color = \"darkblue\", linewidth = 1.2) +\n  geom_hline(yintercept = target_prob, color = \"red\", linetype = \"dashed\") +\n  geom_vline(xintercept = closest$bmi_c, color = \"red\", linetype = \"dotted\") +\n  annotate(\n    \"text\",\n    x     = closest$bmi_c,\n    y     = target_prob + 0.05,\n    label = paste0(\"Target BMI \\u2248 \", round(closest$bmi_c, 1)),\n    color = \"red\",\n    hjust = -0.1\n  ) +\n  labs(\n    x = \"BMI (kg/m^2)\",\n    y = \"Predicted Probability of Diabetes\",\n    title = \"Inverse Prediction: BMI Needed for Target Diabetes Risk\"\n  ) +\n  coord_cartesian(ylim = c(0, 1)) +\n  theme_bw()\n\n\n\n\n\nInverse Prediction: BMI Needed for Target Diabetes Risk"
  },
  {
    "objectID": "slides.html",
    "href": "slides.html",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "",
    "text": "Diabetes mellitus (DM) affects over 37 million U.S. adults.\n\nMajor risk factors include age, BMI, sex, and race/ethnicity.\n\nTraditional logistic regression can produce unstable estimates with missing data or small subgroups.\n\nBayesian logistic regression addresses these issues by incorporating prior information and providing richer uncertainty estimates.\n\nThis analysis compares two modeling frameworks using NHANES 2013–2014 data:\n\nDesign-based logistic regression\nBayesian logistic regression (brms in R)"
  },
  {
    "objectID": "slides.html#aim",
    "href": "slides.html#aim",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Aim",
    "text": "Aim\n\nImplement Bayesian logistic regression to predict diabetes risk.\nCompare classical survey-weighted logistic regression with Bayesian logistic regression.\nApply the model to NHANES 2013–2014 adults."
  },
  {
    "objectID": "slides.html#frequentist-methods",
    "href": "slides.html#frequentist-methods",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013–2014)",
    "section": "Frequentist Methods",
    "text": "Frequentist Methods\n\nBased on Maximum Likelihood Estimation (MLE).\n\nCan become unstable with missing data, small samples, or quasi-separation.\n\nInterprets probability as a long-run relative frequency —\nthe proportion of times an event would occur over infinite repetitions."
  },
  {
    "objectID": "slides.html#bayesian-approach",
    "href": "slides.html#bayesian-approach",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Bayesian Approach",
    "text": "Bayesian Approach\n\nProbability = degree of belief.\nIncorporates prior distributions.\nSupports complex structures (hierarchical, missingness).\nPosterior distributions provide intuitive uncertainty estimates."
  },
  {
    "objectID": "slides.html#bayesian-model",
    "href": "slides.html#bayesian-model",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Bayesian Model",
    "text": "Bayesian Model\nFormula: diabetes_dx | weights(wt_norm) ~ age_c + bmi_c + sex + race\n\n4 chains × 2000 iterations\nBernoulli family\nWeakly informative priors included\n\nInterpretation\n\nThe Bayesian logistic regression model incorporates survey-normalized weights, allowing the model to represent population-level diabetes risk while using Bayesian inference.\nPriors stabilize estimates, especially in smaller subgroups or where quasi-separation may occur.\nRunning 4 chains with 2000 iterations ensures adequate exploration of the posterior distribution.\nThe Bayesian framework provides posterior distributions for every coefficient, offering richer uncertainty estimates than classical methods.\n\nCode\nbayes_fit &lt;- brm(\n  diabetes_dx | weights(wt_norm) ~ age_c + bmi_c + sex + race,\n  data = adult_imp1,\n  family = bernoulli(link = \"logit\"),\n  prior = priors,\n  chains = 4,\n  iter = 2000\n)"
  },
  {
    "objectID": "slides.html#bayesian-logistic-regression-in-r",
    "href": "slides.html#bayesian-logistic-regression-in-r",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013–2014)",
    "section": "Bayesian Logistic Regression (in R)",
    "text": "Bayesian Logistic Regression (in R)\n\nImplemented using brms (with Stan as the MCMC backend).\n\n4 chains × 2000 iterations, with 1000 warmup → 4000 posterior draws.\n\nSurvey weights included via weights(wt_norm) to reflect NHANES design.\n\nlibrary(brms)\n\npriors &lt;- c(\n  set_prior(\"normal(0, 2.5)\", class = \"b\"),\n  set_prior(\"student_t(3, 0, 10)\", class = \"Intercept\")\n)\n\nbayes_fit &lt;- brm(\n  formula = diabetes_dx | weights(wt_norm) ~ age_c + bmi_c + sex + race,\n  data    = adult_imp1,\n  family  = bernoulli(link = \"logit\"),\n  prior   = priors,\n  chains  = 4, iter = 2000, seed = 123,\n  control = list(adapt_delta = 0.95),\n  refresh = 0\n)"
  },
  {
    "objectID": "slides.html#data-source",
    "href": "slides.html#data-source",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Data Source",
    "text": "Data Source\nNHANES 2013–2014\n\nComplex multistage probability sample\nSurvey weights adjust for stratification & clustering\n\nVariables\n\nDiabetes outcome: DIQ010\nAge: RIDAGEYR (standardized)\nBMI: BMXBMI (standardized)\nSex: RIAGENDR\nRace/ethnicity: RIDRETH1\n\nData View"
  },
  {
    "objectID": "slides.html#bayesian-model-diagnostics",
    "href": "slides.html#bayesian-model-diagnostics",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013–2014)",
    "section": "Bayesian Model Diagnostics",
    "text": "Bayesian Model Diagnostics\n\nMCMC ConvergencePosterior Estimates\n\n\n\nTrace plots show stable mixing across all chains.\n\nR̂ ≈ 1.00 → convergence achieved.\n\nEffective sample sizes are adequate for all parameters, indicating reliable sampling.\n\n\n\n\nIntercept: −2.66 [−2.84, −2.50] → baseline log-odds.\n\nAge_c: 1.09 [0.97, 1.22] → higher age increases diabetes risk.\n\nBMI_c: 0.88 [0.76, 1.01] → higher BMI predicts greater diabetes risk.\n\nAll predictors positive and precise (narrow credible intervals, none include 0)."
  },
  {
    "objectID": "slides.html#posterior-predictive-distribution",
    "href": "slides.html#posterior-predictive-distribution",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Posterior Predictive Distribution",
    "text": "Posterior Predictive Distribution\n\nPosterior DrawsPlotInterpretation\n\n\nPosterior draws = 4000 samples (4 chains × 1000 iterations) generated via MCMC, producing the full posterior distribution of the predictors.\n\nRepresents the conditional distribution of parameters given the data\n\nEncodes the entire probability distribution, not just point estimates\n\nMeans and credible intervals summarize this distribution but do not replace it\n\nPosterior shape reflects the influence of the prior (prior subjectivity)\n\nSample size effect: as sample size increases, the prior has less influence on the posterior\n\n\n\n\n\n\n\n\n\n\n\nY-axis: density (frequency of predicted probabilities)\n\nX-axis: predicted probability of diabetes (0–1)\nMost individuals have low predicted probabilities (cluster near 0),\nwith a smaller secondary peak near 1.\n\nThe model closely reproduces the observed data and accurately captures\nthe distribution of diabetes outcomes."
  },
  {
    "objectID": "slides.html#assumptions-for-bayesian-logistic-regression",
    "href": "slides.html#assumptions-for-bayesian-logistic-regression",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Assumptions for Bayesian Logistic Regression",
    "text": "Assumptions for Bayesian Logistic Regression\n\nBinary outcome\nIndependent observations\nLinear relationship on the logit scale\nNo perfect collinearity\nPriors appropriately chosen and sufficiently informative\nProper, convergent posterior\nNo complete separation\nGood posterior predictive checks"
  },
  {
    "objectID": "slides.html#limitations",
    "href": "slides.html#limitations",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Limitations",
    "text": "Limitations\n\n\nCross-sectional design: NHANES data cannot establish causality.\nImputation assumptions: MICE assumes MAR; violations (MNAR) may bias results.\nResidual confounding: Lifestyle, SES, genetics, and other factors were not included.\nPrior sensitivity: Bayesian estimates may shift under different prior choices.\nSingle BMI measurement: May not reflect long-term exposure.\nSelf-reported outcomes: Potential recall and reporting bias.\nNo interaction terms tested: Effects such as age × BMI were not explored."
  },
  {
    "objectID": "slides.html#conclusion",
    "href": "slides.html#conclusion",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Conclusion",
    "text": "Conclusion\n\n\nBayesian logistic regression effectively models uncertainty, producing stable estimates even under missing data and quasi-separation.\nMICE improved data completeness and reliability, allowing the model to incorporate all available information.\nPosterior predictions provide interpretable diabetes risk probabilities, supporting both population-level and individualized assessment.\nThis framework is adaptable to other health outcomes (e.g., hypertension, obesity) where uncertainty, missingness, and complex survey design are important."
  },
  {
    "objectID": "slides.html#acknowledgements",
    "href": "slides.html#acknowledgements",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013–2014)",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nDr. Ashraf Cohen, PhD, MS\nUniversity of West Florida\nDepartment of Mathematics & Statistics\nWe sincerely thank you for your continued guidance and support throughout this project."
  },
  {
    "objectID": "slides.html#handling-missing-data",
    "href": "slides.html#handling-missing-data",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013–2014)",
    "section": "Handling Missing Data",
    "text": "Handling Missing Data\nMultiple Imputation by Chained Equations (MICE)\n(Stef van Buuren and Groothuis-Oudshoorn 2011; S. van Buuren 2012)\n\nIteratively imputes incomplete variables using regression models.\n\nUses Predictive Mean Matching (PMM) for continuous variables and\nlogistic/polytomous regression for categorical variables.\n\nConducted 5 imputations × 10 iterations to ensure stability and convergence.\n\nPooled results combined using Rubin’s rules to account for imputation uncertainty.\n\nfit_mi &lt;- with(\n  data = imp,\n  exp = glm(diabetes_dx ~ age_c + bmi_c + sex + race, family = binomial())\n)\npool_mi &lt;- pool(fit_mi)"
  },
  {
    "objectID": "slides.html#introduction",
    "href": "slides.html#introduction",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Introduction",
    "text": "Introduction\n\nDiabetes is a major public health issue, requiring clear identification of key risk factors.\nClassical logistic regression can become unstable with missing data or separation issues.\nBayesian logistic regression improves stability by incorporating prior information and providing stronger uncertainty estimates.\nThis project evaluates whether Bayesian methods offer more reliable and interpretable diabetes risk estimates than traditional approaches."
  },
  {
    "objectID": "slides.html#methods-study-design-data-preparation",
    "href": "slides.html#methods-study-design-data-preparation",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Methods — Study Design & Data Preparation",
    "text": "Methods — Study Design & Data Preparation\n\nDataset: NHANES 2013–2014\n\nn = 5,769 adults aged ≥20 years\n\nVariables: age, BMI, sex, race/ethnicity, and doctor-diagnosed diabetes\n\nSurvey Design:\n\nComplex, multistage sampling with strata, clusters, and sample weights\n\nWeights normalized and treated as importance weights in modeling\n\nData Preparation:\n\nMissing data handled using Multiple Imputation by Chained Equations (MICE)\n\nContinuous predictors (age, BMI) standardized as z-scores\n\nDemographic variables recoded into analysis categories"
  },
  {
    "objectID": "slides.html#methods-modeling-framework",
    "href": "slides.html#methods-modeling-framework",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Methods — Modeling Framework",
    "text": "Methods — Modeling Framework\nApproaches Compared\n\nSurvey-weighted logistic regression\nMICE-imputed logistic regression (Rubin’s rules)\nBayesian logistic regression (brms, Hamiltonian Monte Carlo)\n\nGoal: Compare coefficient stability, uncertainty quantification, and overall model fit across frameworks."
  },
  {
    "objectID": "slides.html#results-model-comparison",
    "href": "slides.html#results-model-comparison",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Results — Model Comparison",
    "text": "Results — Model Comparison\n\nAll three modeling frameworks identified the same key predictors of diabetes risk: higher age, higher BMI, sex, and race/ethnicity.\n\nEffect sizes were nearly identical across survey-weighted, MICE-imputed, and Bayesian models.\n\nMinor variation in interval widths reflects different treatments of uncertainty."
  },
  {
    "objectID": "slides.html#results-bayesian-model-performance",
    "href": "slides.html#results-bayesian-model-performance",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Results — Bayesian Model Performance",
    "text": "Results — Bayesian Model Performance\n\nPriors: Weakly informative — ( (0, 2.5) ) for slopes, Student-t(3, 0, 10) for intercept.\n\nMCMC diagnostics:\n\n( ), ESS &gt; 2,000 → excellent convergence\n\nTrace plots showed stable, overlapping chains\n\nPosterior distributions unimodal and well-centered\n\nModel fit:\n\nBayesian ( R^2 = 0.13 ), consistent with moderate explanatory power for demographic data\n\nPosterior predictive checks showed good calibration between observed and replicated outcomes\n\nInterpretation:\n\nBayesian credible intervals provided clearer uncertainty quantification.\n\nThe model generalized the classical results while improving interpretability and transparency."
  },
  {
    "objectID": "slides.html#discussion",
    "href": "slides.html#discussion",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk Using NHANES 2013–2014 Data",
    "section": "Discussion",
    "text": "Discussion\n\nAge & BMI dominate\nBayesian improves uncertainty expression"
  },
  {
    "objectID": "slides.html#limitations-conclusion",
    "href": "slides.html#limitations-conclusion",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Limitations & Conclusion",
    "text": "Limitations & Conclusion\n\nLimitations\n\nSingle imputed dataset used for Bayesian modeling may understate total variance.\n\nNormalized NHANES weights approximate but do not fully reproduce design-based inference.\n\nWeakly informative priors were not empirically tuned; alternate priors could slightly shift posterior intervals.\n\nResults are conditional on the 2013–2014 NHANES cycle and not yet externally validated.\n\nConclusion\n\nBayesian logistic regression produced results consistent with frequentist frameworks while offering richer uncertainty quantification.\n\nDiagnostics confirmed excellent convergence (R̂ ≈ 1.00, ESS &gt; 2,000, Bayesian R² ≈ 0.13).\n\nBayesian inference enhances model transparency and interpretability for population health research.\n\nFuture work: integrate hierarchical priors, combine multiple NHANES cycles, and conduct sensitivity analyses for external validation."
  },
  {
    "objectID": "slides.html#key-takeaways",
    "href": "slides.html#key-takeaways",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nAge and BMI are the strongest predictors of diabetes risk.\nBayesian and survey-weighted models produced nearly identical effect sizes.\nBayesian analysis offered clearer uncertainty estimates via credible intervals.\nPosterior predictive checks showed strong model calibration.\nImputation (MICE) improved sample size and stabilized coefficient estimates.\nOverall, the Bayesian framework provides a reliable and interpretable alternative to traditional logistic regression."
  },
  {
    "objectID": "slides.html#acknowledgments-qa",
    "href": "slides.html#acknowledgments-qa",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Acknowledgments & Q&A",
    "text": "Acknowledgments & Q&A\nTeam\nNamita Mishra + Autumn Wilcox\nAdvisor\nDr. Ashraf Cohen, University of West Florida"
  },
  {
    "objectID": "slides.html#introduction-background",
    "href": "slides.html#introduction-background",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Introduction — Background",
    "text": "Introduction — Background\n\nDiabetes mellitus (DM) affects over 37 million U.S. adults.\n\nIdentifying key risk factors — age, BMI, sex, and race/ethnicity — supports targeted prevention and early intervention.\n\nTraditional logistic regression can produce unstable estimates with missing data or small subgroup sizes."
  },
  {
    "objectID": "slides.html#introduction-purpose",
    "href": "slides.html#introduction-purpose",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Introduction — Purpose",
    "text": "Introduction — Purpose\n\nBayesian logistic regression integrates prior information and quantifies uncertainty more transparently than frequentist methods.\n\nThis project compares three analytic frameworks using NHANES 2013–2014 data:\n\nSurvey-weighted logistic regression\n\nMultiple imputation (MICE)\n\nBayesian logistic regression with weakly informative priors"
  },
  {
    "objectID": "slides.html#methods-study-design",
    "href": "slides.html#methods-study-design",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Methods — Study Design",
    "text": "Methods — Study Design\n\nDataset: NHANES 2013–2014\n\nn = 5,769 adults aged ≥20 years\nVariables: age, BMI, sex, race/ethnicity, and doctor-diagnosed diabetes\n\nSurvey Design:\n\nComplex, multistage sampling with strata, clusters, and sample weights\nWeights normalized and treated as importance weights in modeling"
  },
  {
    "objectID": "slides.html#methods-data-preparation",
    "href": "slides.html#methods-data-preparation",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Methods — Data Preparation",
    "text": "Methods — Data Preparation\n\nMissing Data:\n\nAddressed using Multiple Imputation by Chained Equations (MICE)\n\nStandardization:\n\nContinuous predictors (age, BMI) converted to z-scores for comparability\n\nRecoding:\n\nDemographic variables simplified into analysis categories (e.g., race/ethnicity groups, binary sex indicator)\n\nGoal:\n\nCreate a clean, analysis-ready dataset consistent across modeling frameworks"
  },
  {
    "objectID": "slides.html#results-bayesian-performance",
    "href": "slides.html#results-bayesian-performance",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Results — Bayesian Performance",
    "text": "Results — Bayesian Performance\n\nThe Bayesian model showed excellent convergence and calibration, with results closely matching the survey-weighted and imputed logistic regressions.\nParameter estimates were stable, and credible intervals provided clear uncertainty quantification."
  },
  {
    "objectID": "slides.html#discussion-limitations-conclusion",
    "href": "slides.html#discussion-limitations-conclusion",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Discussion, Limitations & Conclusion",
    "text": "Discussion, Limitations & Conclusion\n\nKey FindingsBayesian StrengthsLimitationsConclusion\n\n\n\nAge and BMI were consistently the strongest predictors of diabetes.\n\nFemale sex showed a protective association.\n\nRace/ethnicity differences persisted across all models.\n\nBayesian estimates closely matched survey-weighted estimates, showing strong robustness.\n\nPosterior predictive prevalence (~10.9%) was slightly higher than the survey-weighted prevalence (~8.9%), but still well-calibrated.\n\n\n\n\nProvides full posterior distributions rather than only point estimates.\n\nCredible intervals are intuitive and convey uncertainty effectively.\n\nPriors helped regularize coefficients, useful under missingness.\n\nPosterior predictive checks demonstrated strong model calibration.\n\n\n\n\nNHANES data are cross-sectional, limiting causal inference.\n\nMICE assumes Missing At Random (MAR); violations may bias results.\n\nBayesian model used normalized weights, not full survey design.\n\nOnly one imputed dataset was used for modeling; pooling across imputations may improve inference.\n\nNo interaction effects modeled (e.g., age × BMI).\n\n\n\n\nBayesian logistic regression complements survey-weighted logistic regression.\n\nProduces stable, interpretable results with clear uncertainty.\n\nPosterior predictive checks confirm good model fit and calibration.\n\nFramework is flexible and extendable for more complex models or additional predictors."
  },
  {
    "objectID": "slides.html#results-posterior-predictive-check",
    "href": "slides.html#results-posterior-predictive-check",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Results — Posterior Predictive Check",
    "text": "Results — Posterior Predictive Check\n\nPosterior predictive checks (PPCs) evaluate how well the model reproduces observed data.\nHere, replicated outcomes from the posterior closely match the observed diabetes prevalence, indicating good model calibration.\n\nPosterior Distributions"
  },
  {
    "objectID": "slides.html#background-motivation",
    "href": "slides.html#background-motivation",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Background & Motivation",
    "text": "Background & Motivation\n\n\nWhy consider Bayesian methods?\n\nTraditional logistic regression can produce unstable estimates with missing data or sparse categories.\nBayesian inference incorporates prior information and provides richer uncertainty quantification through posterior distributions.\n\nPrior research\n\nBayesian models improve parameter stability in epidemiologic studies.\nLimited work directly compares Bayesian vs. design-based logistic regression for diabetes prediction using NHANES.\n\nMotivation for this analysis\n\nEvaluate whether Bayesian logistic regression improves stability, interpretability, and predictive calibration.\nUse the NHANES complex survey design with normalized weights to approximate population inference."
  },
  {
    "objectID": "slides.html#methods-dataset-overview",
    "href": "slides.html#methods-dataset-overview",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Methods — Dataset Overview",
    "text": "Methods — Dataset Overview\n\n\nSource: National Health and Nutrition Examination Survey (NHANES) 2013–2014\n\nCohort: 5,769 adults aged ≥20 years\n\nMerged Components:\n\nDEMO_H (Demographics)\n\nBMX_H (Body Measures)\n\nDIQ_H (Diabetes Questionnaire)\n\nOutcome: Doctor-diagnosed diabetes (diabetes_dx)\n\nPredictors:\n\nAge (standardized)\n\nBMI (standardized)\n\nSex (binary: Female vs Male)\n\nRace/Ethnicity (Non-Hispanic White = reference)\n\nPurpose: Create a unified analytic dataset suitable for survey-weighted, imputed, and Bayesian logistic regression models."
  },
  {
    "objectID": "slides.html#methods-exploratory-data-analysis-eda",
    "href": "slides.html#methods-exploratory-data-analysis-eda",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Methods — Exploratory Data Analysis (EDA)",
    "text": "Methods — Exploratory Data Analysis (EDA)\n\n\nMissingness\n\nLow overall: ~3–4% in BMI and diabetes variables\n\nAddressed via Multiple Imputation by Chained Equations (MICE)\n\nOutcome Distribution\n\nDiabetes prevalence ≈ 11% among U.S. adults (2013–2014)\n\nPredictor Patterns\n\nOlder adults and higher BMI groups show higher diabetes rates\n\nNon-Hispanic Black and Hispanic groups have elevated prevalence\n\nFemales slightly lower prevalence than males\n\nKey Takeaway\n\nThe relationships among age, BMI, sex, and race align with prior epidemiologic research, suggesting a sound analytic foundation."
  },
  {
    "objectID": "slides.html#results-model-comparison-summary",
    "href": "slides.html#results-model-comparison-summary",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Results — Model Comparison Summary",
    "text": "Results — Model Comparison Summary\n\n\nAll frameworks identified the same significant predictors: age, BMI, sex, and race/ethnicity.\nBayesian estimates matched frequentist models in magnitude and direction, confirming robustness.\nMinor differences in interval width reflect each model’s treatment of uncertainty.\n\n\n\n\n\n\n\n\n\n\nPredictor\nSurvey-Weighted OR (95% CI)\nMICE-Imputed OR (95% CI)\nBayesian OR (95% CrI)\n\n\n\n\nAge (per 1 SD)\n3.02 (2.60–3.54)\n3.01 (2.58–3.50)\n3.00 (2.57–3.48)\n\n\nBMI (per 1 SD)\n1.87 (1.66–2.10)\n1.85 (1.64–2.08)\n1.86 (1.65–2.09)\n\n\nFemale (vs Male)\n0.52 (0.44–0.61)\n0.53 (0.45–0.62)\n0.52 (0.44–0.61)\n\n\nNon-White (vs NHW)\n1.40 (1.18–1.66)\n1.42 (1.20–1.68)\n1.41 (1.19–1.67)"
  },
  {
    "objectID": "slides.html#future-directions",
    "href": "slides.html#future-directions",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Future Directions",
    "text": "Future Directions\n\n\nExpand NHANES Cycles\nIncorporate additional survey years to assess trends over time.\nEnhanced Bayesian Modeling\nExplore hierarchical or multilevel models, and consider Bayesian model averaging for improved predictive performance.\nAdditional Predictors\nInclude lifestyle, dietary, clinical, or genetic factors when available.\nValidation\nPerform external validation with an independent dataset and conduct sensitivity analyses for priors and imputation assumptions.\nInteraction Effects\nEvaluate interactions (e.g., age × BMI) to refine risk estimation."
  },
  {
    "objectID": "slides.html#qa",
    "href": "slides.html#qa",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Q&A",
    "text": "Q&A"
  },
  {
    "objectID": "slides.html#modeling-workflow",
    "href": "slides.html#modeling-workflow",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Modeling Workflow",
    "text": "Modeling Workflow\n\n\nSteps 1–3: Data PreparationSteps 4–6: Modeling & Evaluation\n\n\n\nStep 1 – Acquisition:\n\nNHANES 2013–2014 (DEMO_H, BMX_H, DIQ_H);\nn = 5,769 adults aged ≥20 years\n\nStep 2 – Preprocessing:\n\nStandardized age & BMI, recoded sex and race/ethnicity\n\nStep 3 – Final Analytic Dataset:\n\nApplied exclusions and standardization to produce a clean dataset\nBoth models used the same complete-case adult dataset (n = 5,769)\n\n\n\n\n\nStep 4 – Modeling Frameworks:\n\nDesign-based logistic regression (design-based)\nBayesian logistic regression (brms, Hamiltonian Monte Carlo)\n\nStep 5 – Diagnostics:\n\nR-hat values ≈ 1.00 indicate strong convergence\nEffective sample sizes (ESS) &gt; 2000 confirm sufficient sampling\nPosterior predictive checks and Bayesian R² ≈ 0.13 show good model fit\n\nStep 6 – Interpretation:\n\nCompared parameter estimates across all frameworks\nIdentified consistent predictors: higher age and BMI increase diabetes risk\nFemale sex showed a protective effect; race/ethnicity differences remained significant\nBayesian credible intervals provided clear uncertainty quantification"
  },
  {
    "objectID": "slides.html#methods-modeling-workflow",
    "href": "slides.html#methods-modeling-workflow",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Methods — Modeling Workflow",
    "text": "Methods — Modeling Workflow\n\n\nSteps 1-3Steps 4-6\n\n\nStep 1 — Data Acquisition\n\nNHANES 2013–2014 (DEMO_H, BMX_H, DIQ_H)\nAdults aged ≥20 years (n = 5,769)\nMerged via participant ID SEQN\n\nStep 2 — Preprocessing\n\nStandardized age and BMI\nRecoded sex and race/ethnicity\nCreated z-scores for comparability\n\nStep 3 — Imputation\n\nAddressed missingness using MICE\nProduced a complete, analysis-ready dataset\n\n\n\nStep 4 — Modeling Frameworks\n\nSurvey-Weighted Logistic Regression (design-based MLE)\nMICE-Imputed Logistic Regression (Rubin’s rules)\nBayesian Logistic Regression (brms, Hamiltonian Monte Carlo)\n\nStep 5 — Diagnostics & Evaluation\n\n( ), ESS &gt; 2000\nPosterior predictive checks for calibration\nBayesian (R^2 )\n\nStep 6 — Interpretation\n\nCompared coefficients across models\nIdentified consistent diabetes predictors"
  },
  {
    "objectID": "slides.html#study-design",
    "href": "slides.html#study-design",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Study Design",
    "text": "Study Design\n\nNHANES DesignWhy Weighting MattersSurvey-Weighted Logistic RegressionVariable Structure\n\n\n\nMultistage probability sample with stratification, clustering, oversampling.\nProduces nationally representative U.S. adult estimates.\nKey design variables: SDMVPSU, SDMVSTRA, WTMEC2YR.\n\n\n\n\nAdjusts for unequal selection probability and nonresponse.\nEnsures unbiased prevalence estimates.\nProvides valid standard errors for population inference.\n\n\n\n\nOutcome: diabetes_dx (doctor-diagnosed diabetes).\nPredictors: bmi, age, sex, race.\nReference groups: NH White, Male.\nModels account for complex sampling structure.\n\n\n\n\nStandardized variables: age_c, bmi_c.\nCategorized variables: bmi_cat, race.\nMissing response codes converted to NA."
  },
  {
    "objectID": "slides.html#dataset-overview",
    "href": "slides.html#dataset-overview",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Dataset Overview",
    "text": "Dataset Overview\n\n\nSource: NHANES 2013–2014\nMerged components:\n\nDEMO_H (Demographics)\nBMX_H (Body Measures)\nDIQ_H (Diabetes Questionnaire)\n\nOutcome variable\n\ndiabetes_dx — self-reported doctor diagnosis of diabetes\n\nPredictors\n\nAge (standardized)\nBMI (standardized)\nSex (Female vs Male)\nRace/Ethnicity (5-level categorical variable)\n\nPurpose of this step\n\nPrepare a unified dataset for both:\n\nDesign-based logistic regression\nBayesian logistic regression"
  },
  {
    "objectID": "slides.html#data-preparation",
    "href": "slides.html#data-preparation",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Data Preparation",
    "text": "Data Preparation\n\nOverviewImport & MergeVariable Definitions\n\n\n\nUses NHANES 2013–2014 public data (DEMO_H, BMX_H, DIQ_H).\nFiles imported in .XPT format using haven in R.\nMerged using SEQN to create an adult dataset (age ≥ 20).\nStandardized variable types before merging.\nEnsured consistent numeric/factor formatting for modeling.\n\n\n\n\nData loaded and combined using tidyverse + base R.\nSEQN used as the unique key to preserve record linkage.\nMerged dataset includes demographics, body measures, and diabetes answers.\nAdults (≥20) retained for the analytic cohort.\nMissing BMI and some demographic predictors handled later via MICE. Outcome (diabetes_dx) was not imputed.\n\n\n\n\nResponse Variable: diabetes_dx (binary), derived from DIQ010.\nSurvey Design Variables: WTMEC2YR, SDMVPSU, SDMVSTRA.\nDerived Variables: age_c, bmi_c."
  },
  {
    "objectID": "slides.html#exploratory-data-analysis-eda",
    "href": "slides.html#exploratory-data-analysis-eda",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Exploratory Data Analysis (EDA)",
    "text": "Exploratory Data Analysis (EDA)\n\nOutcome DistributionPredictor PatternsVisualizations\n\n\n\nDiabetes prevalence in the adult sample is ~11%.\nThis establishes a relatively low event rate, common in population health data.\n\n\n\n\nHigher age and higher BMI are associated with greater diabetes prevalence.\nPrevalence varies across sex and race/ethnicity, suggesting potential subgroup effects.\n\n\n\n\nPatterns observed visually before modeling:\n\nBMI differences by diabetes status\n\nDistribution of diabetes across race/ethnicity"
  },
  {
    "objectID": "slides.html#modeling-framework",
    "href": "slides.html#modeling-framework",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Modeling Framework",
    "text": "Modeling Framework\n\nSurvey-Weighted Logistic Regression\n\nAccounts for NHANES’ complex sampling design (strata, clusters, weights)\n\nMICE-Imputed Logistic Regression\n\nCombines results across multiple imputations using Rubin’s rules\n\nBayesian Logistic Regression (brms, Hamiltonian Monte Carlo)\n\nIncorporates prior information and provides full uncertainty quantification\n\n\nGoal:\nCompare coefficient stability, uncertainty estimation, and overall model fit across all three frameworks.\n\n\nBayesian Model CodePriors & SettingsDiagnostics Code\n\n\n\n\nCode\n# Bayesian logistic regression using brms\nlibrary(brms)\n\nbayes_fit &lt;- brm(\n  diabetes_dx ~ age_c + bmi_c + sex + race,\n  data = nhanes_imputed,\n  family = bernoulli(),\n  prior = c(\n    prior(normal(0, 2.5), class = \"b\"),             # slopes\n    prior(student_t(3, 0, 10), class = \"Intercept\") # intercept\n  ),\n  chains = 4, iter = 2000, warmup = 1000,\n  seed = 123\n)\n\n\n\n\n\nPriors:\n\nSlopes: Normal(0, 2.5)\nIntercept: Student-t(3, 0, 10)\n\nSampling:\n\n4 chains × 2000 iterations (1000 warm-up)\n\nDiagnostics Expected:\n\nRhat ≈ 1.00, ESS &gt; 2000, Bayesian R² ≈ 0.13\n\n\n\n\n\n\nCode\nsummary(bayes_fit)                  # coefficient summaries and Rhat\nbayes_R2(bayes_fit)                 # Bayesian R²\npp_check(bayes_fit, nsamples = 50)  # posterior predictive check"
  },
  {
    "objectID": "slides.html#model-comparison",
    "href": "slides.html#model-comparison",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Model Comparison",
    "text": "Model Comparison\n\nInterpretation: Both models agree on direction of effects. Bayesian estimates show mild shrinkage toward zero and richer uncertainty quantification."
  },
  {
    "objectID": "slides.html#model-comparison-summary",
    "href": "slides.html#model-comparison-summary",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Model Comparison Summary",
    "text": "Model Comparison Summary\n\nBoth frameworks identified the same significant predictors: age, BMI, sex, and race/ethnicity.\nBayesian estimates matched frequentist models in magnitude and direction, confirming robustness.\nInterval widths varied slightly due to differences in how each framework handles uncertainty.\n\n\n\nOdds Ratios (95% CI) Across Design-Based and Bayesian Models\n\n\n\n\n\n\n\n\n\nPredictor\nDesign-Based OR (95% CI)\nBayesian OR (95% CrI)\n\n\n\n\nAge (per 1 SD)\n3.03 (2.70 – 3.40)\n2.99 (2.66 – 3.39)\n\n\nBMI (per 1 SD)\n1.89 (1.65 – 2.15)\n1.87 (1.71 – 2.05)\n\n\nFemale (vs. Male)\n0.53 (0.41 – 0.68)\n0.52 (0.42 – 0.63)\n\n\nBlack (vs. White)\n1.67 (1.16 – 2.40)\n1.70 (1.26 – 2.30)\n\n\nHispanic (vs. White)\n1.59 (1.17 – 2.17)\n1.53 (0.94 – 2.43)\n\n\nOther (vs. White)\n2.33 (1.55 – 3.50)\n2.26 (1.56 – 3.24)\n\n\nMexican American (vs. White)\n2.04 (1.49 – 2.79)\n1.99 (1.41 – 2.80)"
  },
  {
    "objectID": "slides.html#bayesian-performance",
    "href": "slides.html#bayesian-performance",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Bayesian Performance",
    "text": "Bayesian Performance\n\nExcellent convergence and calibration\n\nR-hat values around 1.00 and effective sample sizes above 2000 indicate strong chain mixing and reliable sampling\n\nStable parameter estimates\n\nNearly identical to design-based logistic regression model\n\nCredible intervals\n\nClearly represent uncertainty and confirm overall model robustness\n\n\n\n\nCodeKey FindingsTrace Plots\n\n\n\n\nCode\n# Summarize Bayesian model results\nsummary(bayes_fit)\n\n# Evaluate model fit\nbayes_R2(bayes_fit)\n\n\n\n\n\nAll Rhat ≈ 1.00 → strong convergence\nESS &gt; 2000 → adequate sampling\nBayesian R² = 0.13 → moderate explanatory power\nPosterior means consistent with frequentist results\n\n\n\n\n\n\n\n\n\nPosterior Predictive CheckPosterior Plot\n\n\n\nPosterior predictive checks (PPCs) evaluate how well the model reproduces observed data.\nHere, replicated outcomes from the posterior closely match the observed diabetes prevalence, indicating good model calibration.\n\n\n\n\n\n\n\n\n\nCodePlotInterpretation\n\n\n\n\nCode\n# Posterior predictive check comparing observed vs. simulated outcomes\npp_check(bayes_fit, nsamples = 100) +\n  labs(title = \"Posterior Predictive Check: Observed vs. Replicated Outcomes\")\n\n\n\n\n\n\n\n\nThe PPC plot compares predicted vs. observed outcomes.\nStrong overlap between simulated and actual data → good model fit.\nSimulated outcomes accurately reflect real diabetes prevalence.\nConfirms model is well-calibrated and not overfitting.\nSupports adequacy of priors and overall Bayesian structure.\nReinforces reliability of the Bayesian framework."
  },
  {
    "objectID": "slides.html#modeling-frameworks",
    "href": "slides.html#modeling-frameworks",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Modeling Frameworks",
    "text": "Modeling Frameworks\n\n\nOverviewSurvey-Weighted Logistic RegressionBayesian Logistic Regression (brms)\n\n\nReference Groups:\n\nMale (sex)\nNon-Hispanic White (race).\n\nContinuous Predictors:\n\nSurvey-weighted model: raw age and BMI\nBayesian model: standardized age_c and bmi_c (1 SD increase)\n\n\n\n\nAccounts for NHANES multistage sampling:\n\nStrata: SDMVSTRA\nClusters: SDMVPSU\nWeights: WTMEC2YR\n\n\nProvides population-representative estimates.\n\n\n\n\nWeakly informative priors (Normal(0,2.5), t(3,0,10)).\nIncorporates NHANES exam weights (wt_norm).\nProduces posterior distributions and credible intervals for all parameters."
  },
  {
    "objectID": "slides.html#mcmc-diagnostics",
    "href": "slides.html#mcmc-diagnostics",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "MCMC Diagnostics",
    "text": "MCMC Diagnostics\n\nInterpretation: Mixed chains with stable densities and no divergences show strong convergence and reliable sampling."
  },
  {
    "objectID": "slides.html#mice-multiple-imputation",
    "href": "slides.html#mice-multiple-imputation",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "MICE (Multiple Imputation)",
    "text": "MICE (Multiple Imputation)\n\nWhy Imputation?MICE OverviewMethods Used\n\n\n\nNHANES has non-trivial missingness in BMI and diabetes variables.\nComplete-case analysis can bias estimates if data are Missing at Random (MAR).\nBayesian models require complete data, so imputation is essential.\n\nMAR definition: Missingness depends on observed variables (e.g., age, sex) but not on the missing value itself.\n\n\n\nMultivariate Imputation by Chained Equations (MICE)\nIteratively imputes missing variables using prediction models.\nProduces multiple plausible datasets.\nCaptures uncertainty in parameters and imputations.\nWe use the completed dataset: adult_imp1.\n\n\n\n\nContinuous variables: Predictive Mean Matching (PMM)\nBinary variables: Logistic regression\nIterations: 5 imputations × 5 iterations\nBayesian model uses: adult_imp1"
  },
  {
    "objectID": "slides.html#dataset-for-bayesian-modeling",
    "href": "slides.html#dataset-for-bayesian-modeling",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Dataset for Bayesian Modeling",
    "text": "Dataset for Bayesian Modeling\n\nAdult Analytic DatasetBayesian Model FormulaPriors UsedStandardization Reminder\n\n\n\n\nBayesian model uses the final imputed dataset:\nadult_imp1 (n = 5,592)\nVariables included:\n\ndiabetes_dx — binary outcome\n\nage_c — standardized age\n\nbmi_c — standardized BMI\n\nsex — Female vs Male\n\nrace — 5-level categorical variable\n\nNHANES weights (wt_norm) were normalized and passed as importance weights.\n\n\n\n\n\n\ndiabetes_dx | weights(wt_norm) ~ age_c + bmi_c + sex + race\n\n\nFamily: Bernoulli (logit)\n\nBackend: Stan (HMC / NUTS)\n\nChains: 4\n\nIterations: 2000 (1000 warmup)\n\n\n\n\n\nRegression coefficients:\nnormal(0, 2.5) — weakly informative\nIntercept:\nstudent_t(3, 0, 10) — heavy-tailed prior\nPriors stabilize estimates in sparse or imbalanced categories.\n\n\n\n\n\nAge and BMI were standardized to:\n\nmean = 0\nsd = 1\n\nBenefits:\n\nImproves sampler convergence\nReduces autocorrelation\nMakes coefficient interpretation clear (per 1 SD increase)"
  },
  {
    "objectID": "slides.html#diagnostics-mcmc-trace-plots",
    "href": "slides.html#diagnostics-mcmc-trace-plots",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Diagnostics: MCMC Trace Plots",
    "text": "Diagnostics: MCMC Trace Plots\n\nCodePlotInterpretation\n\n\n\n# MCMC trace plots for age and BMI\nplot(bayes_fit, pars = c(\"b_age_c\", \"b_bmi_c\"), combo = \"chains\")\n\n\n\n\n\n\n\n\n\n\n\nTrace plots assess chain mixing and convergence.\nAll parameters show:\n\nStable, overlapping chains\nNo divergent transitions\nGood exploration of posterior space"
  },
  {
    "objectID": "slides.html#diagnostics-odds-ratios-summary",
    "href": "slides.html#diagnostics-odds-ratios-summary",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Diagnostics: Odds Ratios Summary",
    "text": "Diagnostics: Odds Ratios Summary\n\nCodeTable SummaryInterpretation\n\n\n\n# Extract odds ratios from survey-weighted model\nOR_svy &lt;- broom::tidy(design_fit, exponentiate = TRUE, conf.int = TRUE)\n\n# Extract odds ratios from Bayesian model\nOR_bayes &lt;- broom.mixed::tidy(bayes_fit, exponentiate = TRUE, conf.int = TRUE)\n\n\n\n\n\n\n\nPredictor\nDesign-Based OR (95% CI)\nBayesian OR (95% CrI)\n\n\n\n\nAge (per 1 SD)\n3.03 (2.70–3.40)\n2.99 (2.66–3.39)\n\n\nBMI (per 1 SD)\n1.89 (1.65–2.15)\n1.87 (1.71–2.05)\n\n\nFemale (vs. Male)\n0.53 (0.41–0.68)\n0.52 (0.42–0.63)\n\n\nBlack (vs. White)\n1.67 (1.16–2.40)\n1.70 (1.26–2.30)\n\n\nHispanic (vs. White)\n1.59 (1.17–2.17)\n1.53 (0.94–2.43)\n\n\nOther (vs. White)\n2.33 (1.55–3.50)\n2.26 (1.56–3.24)\n\n\nMexican American (vs. White)\n2.04 (1.49–2.79)\n1.99 (1.41–2.80)\n\n\n\n\n\n\n\nBoth models produce nearly identical odds ratios, supporting model stability.\nBayesian credible intervals are slightly wider, reflecting more complete uncertainty.\nStrongest predictors remain age and BMI.\nSex and race/ethnicity effects are consistent across frameworks."
  },
  {
    "objectID": "slides.html#diagnostics-prior-vs-posterior-distributions",
    "href": "slides.html#diagnostics-prior-vs-posterior-distributions",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Diagnostics: Prior vs Posterior Distributions",
    "text": "Diagnostics: Prior vs Posterior Distributions\n\nCodePlotInterpretation\n\n\n\n# Extract posterior draws as a matrix, then convert to tibble\npost &lt;- as_draws_matrix(bayes_fit) %&gt;%\nas.data.frame() %&gt;%\nselect(b_bmi_c, b_age_c) %&gt;%\npivot_longer(\neverything(),\nnames_to = \"term\",\nvalues_to = \"estimate\"\n) %&gt;%\nmutate(\nterm = case_when(\nterm == \"b_bmi_c\" ~ \"BMI (per 1 SD)\",\nterm == \"b_age_c\" ~ \"Age (per 1 SD)\"\n),\ntype = \"Posterior\"\n)\n\n\nprior_draws &lt;- tibble(\nterm = rep(c(\"BMI (per 1 SD)\", \"Age (per 1 SD)\"), each = 4000),\nestimate = c(rnorm(4000, 0, 1), rnorm(4000, 0, 1)),\ntype = \"Prior\"\n)\n\n\ncombined_draws &lt;- bind_rows(prior_draws, post)\n\n\nggplot(combined_draws, aes(x = estimate, fill = type)) +\ngeom_density(alpha = 0.4) +\nfacet_wrap(~ term, scales = \"free\", ncol = 2) +\ntheme_minimal(base_size = 13) +\nlabs(\ntitle = \"Prior vs Posterior Distributions\",\nx = \"Coefficient estimate\",\ny = \"Density\",\nfill = \"\"\n)\n\n\n\n\n\n\n\n\n\n\n\nThe weakly informative priors are centered at 0, assuming no effect for age or BMI.\nThe posterior distributions shift sharply away from 0 for both predictors.\nThis shows that the data strongly updates the priors, providing clear evidence that:\n\nHigher age increases diabetes risk.\nHigher BMI increases diabetes risk.\n\nThe posterior peaks are narrow and tall, reflecting precise, data-driven estimates.\nBecause the prior and posterior curves barely overlap, the data is highly informative relative to the prior assumptions."
  },
  {
    "objectID": "slides.html#diagnostics-posterior-predictive-checks",
    "href": "slides.html#diagnostics-posterior-predictive-checks",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk Using NHANES 2013–2014 Data",
    "section": "Diagnostics: Posterior Predictive Checks",
    "text": "Diagnostics: Posterior Predictive Checks\n\nCodeVisualInterpretation\n\n\n# Posterior predictive check example\nppc &lt;- pp_check(bayes_fit, type = \"bars\")\n\n\n\n\n\n\nCompares observed vs predicted distributions\nGood alignment indicates strong calibration"
  },
  {
    "objectID": "slides.html#diagnostics-model-level-comparisons",
    "href": "slides.html#diagnostics-model-level-comparisons",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Diagnostics: Model-Level Comparisons",
    "text": "Diagnostics: Model-Level Comparisons\n\nCodeTable SummaryPlotInterpretation\n\n\n\n# 1. Compute prevalence in observed data\nobs_prev &lt;- mean(adult_imp1$diabetes_dx)\n\n# 2. Posterior predicted prevalence\npost_prev &lt;- posterior_predict(bayes_fit) |&gt; colMeans() |&gt; mean()\n\n# 3. Combine ORs from survey-weighted and Bayesian models\ncomparison_table &lt;- data.frame(\n  Predictor = predictors,\n  Design_OR = design_or,\n  Design_LCL = design_lcl,\n  Design_UCL = design_ucl,\n  Bayes_OR   = bayes_or,\n  Bayes_LCL  = bayes_lcl,\n  Bayes_UCL  = bayes_ucl\n)\n\n\n\n\n\n\n\nMetric\nObserved (NHANES)\nPredicted (Posterior Mean)\n\n\n\n\nMean diabetes rate\n0.0889\n0.1094\n\n\nSD\n0.0048 (survey SE)\nNA\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe NHANES survey-weighted prevalence is ≈ 8.9%, which reflects the true population estimate after accounting for complex sampling.\nThe Bayesian posterior predictive prevalence is ≈ 10.9%, slightly higher but still within a reasonable range given sampling variability and model assumptions.\nThe credible interval for the posterior mean overlaps the survey-weighted 95% CI, indicating acceptable calibration, though the model shows a modest upward shift in predicted diabetes prevalence.\nThis pattern is typical when using standardized predictors and weakly informative priors—Bayesian models tend to slightly smooth or regularize prevalence upward."
  },
  {
    "objectID": "slides.html#diagnostics-bayesian-model-fit",
    "href": "slides.html#diagnostics-bayesian-model-fit",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Diagnostics: Bayesian Model Fit",
    "text": "Diagnostics: Bayesian Model Fit\n\nAutocorrelation:\n- Low lag autocorrelation across all parameters.\n- Indicates efficient sampling and good mixing.\nAssumptions:\n- Posterior predictive checks show no major violations.\n- Standardization of age and BMI improves logit-scale linearity.\nCorrelation Matrix:\n- No problematic multicollinearity among predictors.\n- Correlations small-to-moderate, consistent with epidemiology.\nBayesian R²:\n- Bayesian R² ≈ 0.13\n- Typical for epidemiologic binary outcomes, where complex unmeasured factors contribute to risk.\nValidation:\n- Posterior predictions closely match observed NHANES prevalence.\n- No evidence of overfitting.\n- Replicated datasets resemble observed data."
  },
  {
    "objectID": "slides.html#targeted-reverse-prediction-predicting-bmi",
    "href": "slides.html#targeted-reverse-prediction-predicting-bmi",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Targeted Reverse Prediction: Predicting BMI",
    "text": "Targeted Reverse Prediction: Predicting BMI\n\nCodePlotInterpretation\n\n\n\n# Reverse-solve for BMI at a fixed probability (example: p = 0.20)\n\nlogit_p &lt;- qlogis(0.20)\n\npost &lt;- posterior_samples(bayes_fit)\n\nbmi_c_draws &lt;- (\n  logit_p -\n    (post$b_Intercept +\n     post$b_age_c * 0 +      # age_c = 0 (mean age)\n     post$b_sexFemale * 0 +  # sex = Male baseline\n     0                       # race = White baseline\n    )\n) / post$b_bmi_c\n\npred_bmi &lt;- bmi_c_draws * sd(adult$bmi, na.rm = TRUE) +\n            mean(adult$bmi, na.rm = TRUE)\n\npred_bmi_df &lt;- data.frame(pred_bmi = pred_bmi)\n\n\n\n\n\n\n\n\n\n\n\nUses the posterior draws to back-calculate the BMI associated with a fixed diabetes probability.\nProduces a distribution, not a single value → reflects uncertainty.\nPosterior mean BMI and 95% CrI define the threshold where diabetes risk crosses the chosen probability.\nShows practical use of Bayesian modeling for risk thresholds rather than only forward prediction."
  },
  {
    "objectID": "slides.html#diagnostics-trace-density",
    "href": "slides.html#diagnostics-trace-density",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Diagnostics: Trace & Density",
    "text": "Diagnostics: Trace & Density\n\nCodeTrace PlotDensity PlotInterpretation\n\n\n\n# Combine trace + density for Age and BMI\nbayesplot::mcmc_trace(as.array(bayes_fit), pars = c(\"b_age_c\", \"b_bmi_c\"))\nbayesplot::mcmc_dens_overlay(as.array(bayes_fit), pars = c(\"b_age_c\", \"b_bmi_c\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChains are stable with no drift.\nDensities are smooth, unimodal, confirming good mixing.\nR-hat ≈ 1.00 for all parameters → convergence achieved."
  },
  {
    "objectID": "slides.html#diagnostics-autocorrelation-ess",
    "href": "slides.html#diagnostics-autocorrelation-ess",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Diagnostics: Autocorrelation & ESS",
    "text": "Diagnostics: Autocorrelation & ESS\n\nCodePlotInterpretation\n\n\n\n# Autocorrelation for Age & BMI\nbayesplot::mcmc_acf(as.array(bayes_fit), pars = c(\"b_age_c\", \"b_bmi_c\"))\n\n# Extract effective sample size\ness &lt;- posterior::ess(bayes_fit)\ness\n\n\n\n\n\n\n\n\n\n\n\nAutocorrelation drops quickly → efficient sampling.\nBulk ESS and Tail ESS are high → reliable posterior means and tail estimates.\nIndicates low autocorrelation and strong chain mixing."
  },
  {
    "objectID": "slides.html#diagnostics-observed-vs-predicted-scatter",
    "href": "slides.html#diagnostics-observed-vs-predicted-scatter",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Diagnostics: Observed vs Predicted (Scatter)",
    "text": "Diagnostics: Observed vs Predicted (Scatter)\n\nCodePlotInterpretation\n\n\n\n# 100 posterior draws for predictive comparison\npp_scat &lt;- bayesplot::ppc_scatter_avg(\n  y    = adult_imp1$diabetes_dx,\n  yrep = posterior_predict(bayes_fit, ndraws = 100)\n)\npp_scat\n\n\n\n\n\n\n\n\n\n\n\nPoints near (0,0) and (1,1) indicate good predictive alignment.\nModel reproduces both non-diabetic and diabetic outcomes well.\nNo major miscalibration—supports posterior predictive validity."
  },
  {
    "objectID": "slides.html#mice-multiple-imputation-missingness",
    "href": "slides.html#mice-multiple-imputation-missingness",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "MICE (Multiple Imputation) — Missingness",
    "text": "MICE (Multiple Imputation) — Missingness\n\nMissingness PlotInterpretation\n\n\n\n\n\n\n\n\n\n\nMissingness mainly affects BMI and diabetes variables.\nPattern is consistent with MAR, supporting use of MICE."
  },
  {
    "objectID": "slides.html#why-bayesian",
    "href": "slides.html#why-bayesian",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Why Bayesian?",
    "text": "Why Bayesian?\n\n\nWhy consider Bayesian methods?\n\nTraditional logistic regression can produce unstable estimates with missing data or sparse categories.\nBayesian inference incorporates prior information and provides richer uncertainty quantification through posterior distributions.\n\nPrior research\n\nBayesian models improve parameter stability in epidemiologic studies.\nLimited work directly compares Bayesian vs. design-based logistic regression for diabetes prediction using NHANES.\n\nMotivation for this analysis\n\nEvaluate whether Bayesian logistic regression improves stability, interpretability, and predictive calibration.\nUse the NHANES complex survey design with normalized weights to approximate population inference."
  },
  {
    "objectID": "slides.html#data-source-design",
    "href": "slides.html#data-source-design",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Data Source & Design",
    "text": "Data Source & Design\n\nDataset: NHANES 2013–2014\n\nAdults aged ≥20 years\nFinal analytic cohort size after merging: 5,769\n\nVariables collected\n\nOutcome: Doctor-diagnosed diabetes (diabetes_dx)\nPredictors: Age, BMI, sex, race/ethnicity\n\nNHANES survey design\n\nMultistage, complex sample with strata, clusters, and weights\nWeights were normalized and used as importance weights in the Bayesian model"
  },
  {
    "objectID": "slides.html#variables",
    "href": "slides.html#variables",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Variables",
    "text": "Variables\n\n\nSource: NHANES 2013–2014\nMerged components:\n\nDEMO_H (Demographics)\nBMX_H (Body Measures)\nDIQ_H (Diabetes Questionnaire)\n\nOutcome variable\n\ndiabetes_dx — self-reported doctor diagnosis of diabetes\n\nPredictors\n\nAge (standardized)\nBMI (standardized)\nSex (Female vs Male)\nRace/Ethnicity (5-level categorical variable)\n\nPurpose of this step\n\nPrepare a unified dataset for both:\n\nDesign-based logistic regression\nBayesian logistic regression"
  },
  {
    "objectID": "slides.html#data-cleaning-setup",
    "href": "slides.html#data-cleaning-setup",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Data Cleaning & Setup",
    "text": "Data Cleaning & Setup\n\n\nMissing data\n\nSmall proportion of missingness (~3–4%) in BMI and diabetes variables\nComplete-case data were insufficient → motivated the need for MICE\n\nStandardization\n\nAge and BMI were converted to z-scores\n\nImproves convergence of Bayesian HMC\nMakes effect sizes comparable (per 1 SD increase)\n\n\nRecoding\n\nSex simplified to Female vs Male\nRace/Ethnicity grouped into 5 categories with Non-Hispanic White as the reference\n\nGoal\n\nProduce a clean, consistent dataset before imputation and modeling"
  },
  {
    "objectID": "slides.html#missing-data-mice",
    "href": "slides.html#missing-data-mice",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk Using NHANES 2013–2014 Data",
    "section": "Missing Data & MICE",
    "text": "Missing Data & MICE\n\n~3–4% missing\nBMI via PMM, age via norm"
  },
  {
    "objectID": "slides.html#missingness-pattern",
    "href": "slides.html#missingness-pattern",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Missingness Pattern",
    "text": "Missingness Pattern\n\nMissingness PlotInterpretation\n\n\n\n\n\n\n\n\n\n\nMissingness mainly affects BMI and diabetes variables.\nPattern is consistent with MAR, supporting use of MICE."
  },
  {
    "objectID": "slides.html#final-bayesian-modeling-dataset",
    "href": "slides.html#final-bayesian-modeling-dataset",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Final Bayesian Modeling Dataset",
    "text": "Final Bayesian Modeling Dataset\n\nAdult Analytic DatasetBayesian Model FormulaPriors UsedStandardization Reminder\n\n\n\n\nBayesian model uses the final imputed dataset:\nadult_imp1 (n = 5,592)\nVariables included:\n\ndiabetes_dx — binary outcome\n\nage_c — standardized age\n\nbmi_c — standardized BMI\n\nsex — Female vs Male\n\nrace — 5-level categorical variable\n\nNHANES weights (wt_norm) were normalized and passed as importance weights.\n\n\n\n\n\n\ndiabetes_dx | weights(wt_norm) ~ age_c + bmi_c + sex + race\n\n\nFamily: Bernoulli (logit)\n\nBackend: Stan (HMC / NUTS)\n\nChains: 4\n\nIterations: 2000 (1000 warmup)\n\n\n\n\n\nRegression coefficients:\nnormal(0, 2.5) — weakly informative\nIntercept:\nstudent_t(3, 0, 10) — heavy-tailed prior\nPriors stabilize estimates in sparse or imbalanced categories.\n\n\n\n\n\nAge and BMI were standardized to:\n\nmean = 0\nsd = 1\n\nBenefits:\n\nImproves sampler convergence\nReduces autocorrelation\nMakes coefficient interpretation clear (per 1 SD increase)"
  },
  {
    "objectID": "slides.html#next-steps",
    "href": "slides.html#next-steps",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Next Steps",
    "text": "Next Steps\n\n\nExpand NHANES Cycles\nIncorporate additional survey years to assess trends over time.\nEnhanced Bayesian Modeling\nExplore hierarchical or multilevel models, and consider Bayesian model averaging for improved predictive performance.\nAdditional Predictors\nInclude lifestyle, dietary, clinical, or genetic factors when available.\nValidation\nPerform external validation with an independent dataset and conduct sensitivity analyses for priors and imputation assumptions.\nInteraction Effects\nEvaluate interactions (e.g., age × BMI) to refine risk estimation."
  },
  {
    "objectID": "slides.html#diagnostics-pairwise-posterior-relationships",
    "href": "slides.html#diagnostics-pairwise-posterior-relationships",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Diagnostics: Pairwise Posterior Relationships",
    "text": "Diagnostics: Pairwise Posterior Relationships\n\nCodePlotInterpretation\n\n\n\n# Pairwise posterior correlations for Age, BMI, and Sex (Female)\nbayesplot::mcmc_pairs(\n  posterior::as_draws_array(bayes_fit),\n  pars = c(\"b_age_c\", \"b_bmi_c\", \"b_sexFemale\"),\n  off_diag_args = list(size = 1.5, alpha = 0.4)\n)\n\n\n\n\n\n\n\n\n\n\n\nPairwise plots show the joint posterior relationships among age, BMI, and sex effects.\nAll parameter pairs look well-mixed and approximately elliptical, indicating no problematic correlations.\nNo funnel shapes or multimodality → confirms stable sampling across parameters."
  },
  {
    "objectID": "slides.html#final-modeling-dataset",
    "href": "slides.html#final-modeling-dataset",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Final Modeling Dataset",
    "text": "Final Modeling Dataset\n\nAdult DatasetBayesian Model FormulaPriors & Standardization\n\n\n\n\nBayesian model uses the final imputed dataset:\nadult_imp1 (n = 5,592)\nVariables included:\n\ndiabetes_dx — binary outcome\nage_c — standardized age\nbmi_c — standardized BMI\nsex — Female vs Male\nrace — 5-level categorical variable\n\nNHANES weights (wt_norm) were normalized and passed as importance weights.\n\n\n\n\n\n\ndiabetes_dx | weights(wt_norm) ~ age_c + bmi_c + sex + race\n\n\n\nFamily: Bernoulli (logit)\nBackend: Stan (HMC / NUTS)\nChains: 4\nIterations: 2000 (1000 warmup)\n\n\n\n\n\n\nRegression Coefficients:\n- normal(0, 2.5) — weakly informative\nIntercept:\n- student_t(3, 0, 10) — heavy-tailed prior\n- Priors stabilize estimates in sparse or imbalanced categories.\nAge and BMI were standardized to:\n- mean = 0\n- sd = 1\nBenefits:\n- Improves sampler convergence\n- Reduces autocorrelation\n- Makes coefficient interpretation clear (per 1 SD increase)"
  },
  {
    "objectID": "slides.html#bayesian-logistic-regression-for-predicting-diabetes-risk",
    "href": "slides.html#bayesian-logistic-regression-for-predicting-diabetes-risk",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "text": "Bayesian Logistic Regression for Predicting Diabetes Risk\nNHANES 2013–2014\nNamita Mishra & Autumn Wilcox"
  },
  {
    "objectID": "slides.html#aims",
    "href": "slides.html#aims",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Aims",
    "text": "Aims\n\nApply Bayesian logistic regression to predict diabetes status.\nExamine associations between diabetes and BMI, age (≥20), sex, and race.\nUse NHANES 2013–2014 data accounting for complex survey design.\nAddress challenges such as missing data, complete case bias, and data separation.\nImprove the robustness and reliability of inference compared to classical logistic regression."
  },
  {
    "objectID": "slides.html#missing-data-summary",
    "href": "slides.html#missing-data-summary",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Missing Data Summary",
    "text": "Missing Data Summary\n\nCodeVisualInterpretation\n\n\nplot_missing(adult)\n\n\n\n\n\n\nMissingness mainly in BMI and key predictors."
  },
  {
    "objectID": "slides.html#eda-bmi-by-diabetes",
    "href": "slides.html#eda-bmi-by-diabetes",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "EDA — BMI by Diabetes",
    "text": "EDA — BMI by Diabetes"
  },
  {
    "objectID": "slides.html#eda-diabetes-by-race",
    "href": "slides.html#eda-diabetes-by-race",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "EDA — Diabetes by Race",
    "text": "EDA — Diabetes by Race"
  },
  {
    "objectID": "slides.html#survey-weighted-logistic-regression",
    "href": "slides.html#survey-weighted-logistic-regression",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Survey-Weighted Logistic Regression",
    "text": "Survey-Weighted Logistic Regression\n\nModel: diabetes_dx ~ age_c + bmi_c + sex + race\nWeights: WTMEC2YR; PSU + strata for complex design\n\nKey Results:\n\nAge OR ≈ 3.03\nBMI OR ≈ 1.88\nFemale OR ≈ 0.53\nRacial/ethnic disparities present"
  },
  {
    "objectID": "slides.html#bayesian-logistic-regression",
    "href": "slides.html#bayesian-logistic-regression",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Bayesian Logistic Regression",
    "text": "Bayesian Logistic Regression\n\nOverviewModel StructurePrior SpecificationPosterior PredictionsModel Evaluation & DiagnosticsAdvantages of Bayesian Approach\n\n\n\nCombines priors + data → posterior distributions\n\nParameters treated as random variables\n\nProduces credible intervals and probability-based interpretation\n\nHandles uncertainty and separation better than MLE\n\n\n\n\nOutcome: diabetes_dx (binary)\n\nPredictors: age_c, bmi_c, sex, race\n\nLogit model:\nlogit(P(diabetes = 1)) = α + β_age + β_bmi + β_sex + β_race\n\n\n\n\nCoefficients: Normal(0, 2.5) — weakly informative, recommended for logistic regression (Gelman et al. 2008)\n\nIntercept: Student-t(3, 0, 10) — robust, heavy-tailed prior\n\n\n\n\nPosterior draws → predicted diabetes probabilities\n\nCaptures parameter + predictive uncertainty\n\nSupports individual and population-level inference\n\n\n\n\nSampling via Stan’s NUTS algorithm (stan2020?)\n\nConvergence checks: R-hat ≈ 1, ESS values, trace plots, autocorrelation\n\nPosterior predictive checks to assess model fit\n\n\n\n\nStable under quasi-separation\n\nDirect probability statements (credible intervals)\n\nPriors provide regularization\n\nFull uncertainty propagation\n\nFlexible prediction and validation tools"
  },
  {
    "objectID": "slides.html#posterior-coefficients",
    "href": "slides.html#posterior-coefficients",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Posterior Coefficients",
    "text": "Posterior Coefficients\nKey Predictors\nAge (per 1 SD)\n\nStrongest predictor of diabetes.\nHigher age → substantially higher diabetes risk.\n\nBMI (per 1 SD)\n\nStrong positive association.\nHigher BMI → consistently higher diabetes probability.\n\nSex (Female vs Male)\n\nClear protective effect.\nFemales show markedly lower diabetes risk adjusting for age, BMI, race.\n\nRace/Ethnicity (vs NH White)\n\nMexican American: Elevated odds, strong evidence.\nNon-Hispanic Black: Clearly higher odds.\nOther/Multi: Strongly elevated odds.\nOther Hispanic: Positive mean effect but weaker evidence."
  },
  {
    "objectID": "slides.html#posterior-predictive-checks",
    "href": "slides.html#posterior-predictive-checks",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Posterior Predictive Checks",
    "text": "Posterior Predictive Checks\n\nPlotInterpretationProportion of DiabetesOutcome MeanOutcome Standard DeviationDiabetes Prevalence Compared (Model vs NHANES)\n\n\n\n\n\n\n\n\n\n\nY-axis: density (frequency of predicted probabilities)\n\nX-axis: predicted probability of diabetes (0–1)\nMost individuals have low predicted probabilities (cluster near 0),\nwith a smaller secondary peak near 1.\n\nThe model closely reproduces the observed data and accurately captures\nthe distribution of diabetes outcomes.\n\n\nWe draw 500 posterior predictive samples of outcomes for each observation\nand calculate the proportion of simulated outcomes where Diabetes = 1.\n\n\n\n\n\nFigure: Bar plot comparing the counts/frequencies of each category\n(0 vs 1) in the observed data (y) and the posterior predictive samples (y-rep).\nThe observed bars fall within the range of simulated data, indicating that\nthe model accurately reproduces the overall diabetes prevalence.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHistogram shows the posterior distribution of the population proportion of diabetes.\nx-axis: proportion of diabetics (1s)\ny-axis: frequency of posterior draws\n\nHighest bar ≈ 0.11 (11%), symmetric and centered around ~0.11\n\n95% credible interval: approximately 9%–13% prevalence"
  },
  {
    "objectID": "slides.html#prior-vs-posterior",
    "href": "slides.html#prior-vs-posterior",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Prior vs Posterior",
    "text": "Prior vs Posterior\n\nCodeVisualInterpretation\n\n\nplot(bayes_fit, pars = rownames(fixef(bayes_fit)))\n\n\n\n\n\n\nPosteriors shift notably from priors, indicating informative data.\nPriors had minimal influence relative to NHANES sample size."
  },
  {
    "objectID": "slides.html#model-comparison-loo",
    "href": "slides.html#model-comparison-loo",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Model Comparison (LOO)",
    "text": "Model Comparison (LOO)\n\nCodeVisualInterpretation\n\n\nloo_compare(loo_full, loo_nosex, loo_norace)\n\n\n\n\n\n\nFull model best supported by data.\nDropping sex or race increases LOOIC, indicating worse fit."
  },
  {
    "objectID": "slides.html#calibration-plot",
    "href": "slides.html#calibration-plot",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Calibration Plot",
    "text": "Calibration Plot\n\nCodeVisualInterpretation\n\n\ncal_df &lt;- data.frame(pred, obs)\n\nggplot(cal_df, aes(pred, obs)) +\n  geom_point(alpha = 0.3) +\n  geom_smooth(method = \"loess\") +\n  labs(x = \"Predicted Probability\", y = \"Observed Outcome\")\n\n\n\n\n\n\nPredicted probabilities closely match observed diabetes outcomes.\nIndicates strong model calibration."
  },
  {
    "objectID": "slides.html#population-vs-posterior-prevalence",
    "href": "slides.html#population-vs-posterior-prevalence",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Population vs Posterior Prevalence",
    "text": "Population vs Posterior Prevalence\nVisual \nInterpretation - Posterior prevalence matches NHANES."
  },
  {
    "objectID": "slides.html#results-summary",
    "href": "slides.html#results-summary",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Results Summary",
    "text": "Results Summary\n\nAge and BMI strongest predictors.\nFemale sex protective.\nRace disparities present.\nBayesian and survey-weighted models largely agree."
  },
  {
    "objectID": "slides.html#thank-you",
    "href": "slides.html#thank-you",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Thank You",
    "text": "Thank You\nAdvisor\nDr. Achraf Cohen\nPresented by\nNamita Mishra\nAutumn S. Wilcox\nUniversity of West Florida\nQuestions?"
  },
  {
    "objectID": "slides.html#bmi-distribution-by-diabetes-status",
    "href": "slides.html#bmi-distribution-by-diabetes-status",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "BMI Distribution by Diabetes Status",
    "text": "BMI Distribution by Diabetes Status\n\nCodeVisualInterpretation\n\n\nggplot(adult, aes(x = bmi, fill = factor(diabetes_dx))) +\n  geom_density(alpha = 0.5) +\n  labs(fill = \"Diabetes\")\n\n\n\n\n\n\nDiabetic participants show a right-shifted BMI distribution.\nConsistent with expected metabolic patterns."
  },
  {
    "objectID": "slides.html#diabetes-by-raceethnicity",
    "href": "slides.html#diabetes-by-raceethnicity",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Diabetes by Race/Ethnicity",
    "text": "Diabetes by Race/Ethnicity\n\nCodeVisualInterpretation\n\n\nggplot(adult, aes(x = race, fill = factor(diabetes_dx))) +\n  geom_bar(position = \"fill\") +\n  labs(fill = \"Diabetes\")\n\n\n\n\n\n\nClear racial/ethnic disparities.\nNH Black and Mexican American participants show higher proportions."
  },
  {
    "objectID": "slides.html#posterior-vs-survey-weighted-prevalence",
    "href": "slides.html#posterior-vs-survey-weighted-prevalence",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Posterior vs Survey-Weighted Prevalence",
    "text": "Posterior vs Survey-Weighted Prevalence\n\nCodeVisualInterpretation\n\n\npp &lt;- posterior_predict(bayes_fit)\npost_prev &lt;- colMeans(pp)\n\n\n\n\n\n\nPosterior prevalence distribution centers near NHANES survey-weighted prevalence.\nSupports Bayesian model validity."
  },
  {
    "objectID": "slides.html#method",
    "href": "slides.html#method",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk Using NHANES 2013–2014 Data",
    "section": "Method",
    "text": "Method\nBayesian Logistic Regression Framework\n\nTreat parameters as random\nPosterior ∝ Likelihood × Prior\n\nLogistic Regression Structure\n\nLogit model specification\n\nPriors\n\nNormal(0, 2.5) for slopes\nStudent-t for intercept"
  },
  {
    "objectID": "slides.html#posterior-inference",
    "href": "slides.html#posterior-inference",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk Using NHANES 2013–2014 Data",
    "section": "Posterior Inference",
    "text": "Posterior Inference\n\nParameter uncertainty\nPredictive uncertainty"
  },
  {
    "objectID": "slides.html#diagnostics",
    "href": "slides.html#diagnostics",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk Using NHANES 2013–2014 Data",
    "section": "Diagnostics",
    "text": "Diagnostics\nPosterior Predictive Checks\n\nCodeVisualInterpretation\n\n\n# Posterior predictive check example\nppc &lt;- pp_check(bayes_fit, type = \"bars\")\n\n\n\n\n\n\nCompares observed vs predicted distributions\nGood alignment indicates strong calibration"
  },
  {
    "objectID": "slides.html#analysis-and-results",
    "href": "slides.html#analysis-and-results",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk Using NHANES 2013–2014 Data",
    "section": "Analysis and Results",
    "text": "Analysis and Results\nData Preparation\n\nDEMO_H, BMX_H, DIQ_H merged\nStandardized predictors\n\nSurvey Design\n\nStratification, clustering, weights"
  },
  {
    "objectID": "slides.html#model-results",
    "href": "slides.html#model-results",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk Using NHANES 2013–2014 Data",
    "section": "Model Results",
    "text": "Model Results\nSurvey-Weighted Results\n\nAge ↑ odds\nBMI ↑ odds\nFemale protective"
  },
  {
    "objectID": "slides.html#translational-implications",
    "href": "slides.html#translational-implications",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk Using NHANES 2013–2014 Data",
    "section": "Translational Implications",
    "text": "Translational Implications\n\nPrecision public health\nBMI-based risk modification"
  },
  {
    "objectID": "slides.html#diagnostics-mcmc-convergence",
    "href": "slides.html#diagnostics-mcmc-convergence",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk Using NHANES 2013–2014 Data",
    "section": "Diagnostics: MCMC Convergence",
    "text": "Diagnostics: MCMC Convergence\n\nCodeVisualInterpretation\n\n\n# Trace plot example\nmcmc_trace(as.array(bayes_fit), regex_pars = \"^b_\")\n\n\n\n\n\n\nChains overlap and mix well\nNo divergence or drift → good convergence"
  },
  {
    "objectID": "slides.html#diagnostics-autocorrelation",
    "href": "slides.html#diagnostics-autocorrelation",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk Using NHANES 2013–2014 Data",
    "section": "Diagnostics: Autocorrelation",
    "text": "Diagnostics: Autocorrelation\n\nCodeVisualInterpretation\n\n\n# Autocorrelation example\nmcmc_acf(posterior::as_draws_array(bayes_fit), pars = c(\"b_age_c\", \"b_bmi_c\"))\n\n\n\n\n\n\nRapid decay of autocorrelation\nIndicates efficient sampling"
  },
  {
    "objectID": "slides.html#diagnostics-prior-vs-posterior",
    "href": "slides.html#diagnostics-prior-vs-posterior",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk Using NHANES 2013–2014 Data",
    "section": "Diagnostics: Prior vs Posterior",
    "text": "Diagnostics: Prior vs Posterior\n\nCodeVisualInterpretation\n\n\n# Prior vs posterior overlay example\ncombined_draws &lt;- bind_rows(prior_draws, post)\n\n\n\n\n\n\nPosterior shifts away from prior where data is informative\nConfirms influence of observed data"
  },
  {
    "objectID": "slides.html#diagnostics-calibration",
    "href": "slides.html#diagnostics-calibration",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk Using NHANES 2013–2014 Data",
    "section": "Diagnostics: Calibration",
    "text": "Diagnostics: Calibration\n\nCodeVisualInterpretation\n\n\n# Calibration example\npred_mean &lt;- colMeans(posterior_epred(bayes_fit))\n\n\n\n\n\n\nPosterior mean aligns with observed prevalence\nIndicates reliable probability estimates"
  },
  {
    "objectID": "slides.html#method-posterior-predictions",
    "href": "slides.html#method-posterior-predictions",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Method: Posterior Predictions",
    "text": "Method: Posterior Predictions\n\nHow Predictions WorkInterpretation\n\n\n\nPosterior draws used to estimate outcome probabilities.\nIncorporates parameter uncertainty and predictive uncertainty.\n\n\n\n\nAllows probability statements such as “Given predictors, probability lies between X and Y.”"
  },
  {
    "objectID": "slides.html#method-model-evaluation-diagnostics",
    "href": "slides.html#method-model-evaluation-diagnostics",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Method: Model Evaluation & Diagnostics",
    "text": "Method: Model Evaluation & Diagnostics\n\nConvergence ChecksModel Fit\n\n\n\nMCMC via NUTS.\nR-hat, effective sample size, autocorrelation.\n\n\n\n\nPosterior predictive checks.\nBayesian R²."
  },
  {
    "objectID": "slides.html#method-advantages-of-bayesian",
    "href": "slides.html#method-advantages-of-bayesian",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Method: Advantages of Bayesian",
    "text": "Method: Advantages of Bayesian\nKey Benefits\n\nFull uncertainty quantification.\nCredible intervals instead of p-values.\nFlexible priors incorporate prior knowledge.\nProbabilistic predictions for new observations.\nPPCs evaluate model fit."
  },
  {
    "objectID": "slides.html#method-section",
    "href": "slides.html#method-section",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Method Section",
    "text": "Method Section"
  },
  {
    "objectID": "slides.html#method-bayesian-logistic-regression",
    "href": "slides.html#method-bayesian-logistic-regression",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Method: Bayesian Logistic Regression",
    "text": "Method: Bayesian Logistic Regression\n\nOverviewModel StructurePrior Specification\n\n\n\nIntegrates prior knowledge with observed data to form posterior distributions.\nParameters represented as random variables with full probability distributions.\nProduces credible intervals and direct probabilistic interpretations.\n\n\n\n\nLogistic model uses predictors to estimate log-odds of a binary outcome.\nPriors assigned to parameters; posterior obtained via Bayes’ theorem.\nPosterior reflects updated beliefs after observing data.\n\n\n\n\nWeakly informative priors stabilize estimation.\nCoefficients: Normal(0, 2.5).\nIntercept: Student t(3, 0, 10)."
  },
  {
    "objectID": "slides.html#posterior-predictions",
    "href": "slides.html#posterior-predictions",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Posterior Predictions",
    "text": "Posterior Predictions\n\n\nHow Predictions Work\n\nPosterior draws used to estimate outcome probabilities.\nIncorporates parameter uncertainty and predictive uncertainty.\n\n\nThis allows for probability statements such as “Given predictors, probability lies between X and Y.”"
  },
  {
    "objectID": "slides.html#model-evaluation-diagnostics",
    "href": "slides.html#model-evaluation-diagnostics",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Model Evaluation & Diagnostics",
    "text": "Model Evaluation & Diagnostics\n\nConvergence ChecksModel FitInterpretation\n\n\n\nEvaluated using NUTS sampling diagnostics:\n\nR-hat ≈ 1.00 for all parameters → strong evidence of convergence.\nEffective Sample Size (ESS) high for bulk and tail estimates → stable posterior sampling.\n\nTrace plots and autocorrelation plots showed:\n\nwell-mixed chains,\nno drift,\nno divergences.\n\n\n\n\n\nPosterior predictive checks (PPCs) confirmed that the Bayesian model can reproduce:\n\nthe observed distribution of diabetes outcomes,\nthe mean and variability of the binary outcome.\n\nBayesian R² ≈ 0.13, which is typical for logistic models using population health data.\n\n\n\n\nDiagnostics indicate:\n\nreliable posterior estimates,\ngood predictive performance,\nno evidence of sampling issues.\n\nOverall, the model is well-calibrated and captures the key patterns in the data."
  },
  {
    "objectID": "slides.html#advantages-of-bayesian",
    "href": "slides.html#advantages-of-bayesian",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Advantages of Bayesian",
    "text": "Advantages of Bayesian\n\n\nKey Benefits\n\nFull uncertainty quantification.\nCredible intervals instead of p-values.\nFlexible priors incorporate prior knowledge.\nProbabilistic predictions for new observations.\nPPCs evaluate model fit."
  },
  {
    "objectID": "slides.html#study-design-cont.",
    "href": "slides.html#study-design-cont.",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Study Design (cont.)",
    "text": "Study Design (cont.)\n\nData WorkflowAdult CohortProcessing StepsMissingness\n\n\n\nImported DEMO_H, BMX_H, DIQ_H (.XPT) using haven.\nMerged using SEQN; restricted to adults ≥20.\nStandardized variable formats for analysis.\n\n\n\n\nFinal sample: 5,769 adults.\nIncludes demographics, BMI, diabetes status, survey design vars.\nOutcome: diabetes_dx.\nPredictors: age, BMI, sex, race.\n\n\n\n\nCentered & standardized: age_c, bmi_c.\nConstructed bmi_cat categories.\nCollapsed race into four levels.\nCleaned nonresponse codes.\n\n\n\n\nMissing mainly in BMI (~4.3%) and diabetes_dx (~3.1%).\nDemographics and survey design variables fully observed.\nPattern consistent with MAR; appropriate for imputation.\n\n\n(Missing Data Pattern — BMI ~4.3% missing, diabetes_dx ~3.1% missing; demographics and design variables complete.)\n\nMissing mainly in BMI (~4.3%) and diabetes_dx (~3.1%).\nDemographics and design vars fully observed.\nPattern consistent with MAR; appropriate for imputation."
  },
  {
    "objectID": "slides.html#nhanes-study-design",
    "href": "slides.html#nhanes-study-design",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "NHANES Study Design",
    "text": "NHANES Study Design\n\nNHANES DesignWhy Weighting MattersSurvey-Weighted Logistic Regression\n\n\n\nMultistage probability sampling with stratification, clustering, oversampling.\nNationally representative U.S. adult estimates.\nKey design variables: SDMVPSU, SDMVSTRA, WTMEC2YR.\n\n\n\n\nAdjusts for unequal selection probability and nonresponse.\nProvides unbiased prevalence estimates.\nEnsures valid standard errors for population inference.\n\n\n\n\nOutcome: diabetes_dx (doctor-diagnosed diabetes).\nPredictors: bmi, age, sex, race.\nReference groups: NH White, Male.\nModels account for complex sampling structure."
  },
  {
    "objectID": "slides.html#data-workflow-analytic-cohort",
    "href": "slides.html#data-workflow-analytic-cohort",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Data Workflow & Analytic Cohort",
    "text": "Data Workflow & Analytic Cohort\n\nData WorkflowAdult CohortProcessing Steps\n\n\n\nImported DEMO_H, BMX_H, DIQ_H (.XPT) using haven.\nMerged using SEQN; restricted to adults ≥20.\nStandardized variable formats before merging.\n\n\n\n\nFinal analytic sample: 5,769 adults.\nIncludes demographics, BMI, diabetes status, survey design vars.\nPredictors: age, BMI, sex, race.\n\n\n\n\nCentered & standardized: age_c, bmi_c.\nConstructed bmi_cat categories.\nCollapsed race into four levels.\nCleaned nonresponse codes."
  },
  {
    "objectID": "slides.html#missing-data-assessment",
    "href": "slides.html#missing-data-assessment",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Missing Data Assessment",
    "text": "Missing Data Assessment\n\nMissingness OverviewMissingness Visualization\n\n\n\nMissing mainly in BMI (~4.3%) and diabetes_dx (~3.1%).\nDemographics and survey design variables fully observed.\nPattern consistent with MAR; suitable for imputation.\n\n\n\n\n(BMI + diabetes_dx missingness; all design variables complete.)"
  },
  {
    "objectID": "slides.html#study-design-overview",
    "href": "slides.html#study-design-overview",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Study Design Overview",
    "text": "Study Design Overview\n\n\nNHANES DesignWhy Weighting Matters\n\n\n\nMultistage probability sampling with stratification, clustering, oversampling.\nNationally representative U.S. adult estimates.\nKey Design Variables: SDMVPSU, SDMVSTRA, WTMEC2YR.\n\n\n\n\nAdjusts for unequal selection probability and nonresponse.\nProvides unbiased prevalence estimates.\nEnsures valid standard errors for population inference."
  },
  {
    "objectID": "slides.html#building-the-analytic-cohort",
    "href": "slides.html#building-the-analytic-cohort",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Building the Analytic Cohort",
    "text": "Building the Analytic Cohort\n\n\nData WorkflowAdult Cohort\n\n\n\nImported DEMO_H, BMX_H, DIQ_H (.XPT) using haven.\nMerged using SEQN; restricted to adults ≥20.\nStandardized variable formats before merging.\n\n\n\n\nFinal Analytic Sample: 5,769 adults.\nIncludes demographics, BMI, diabetes status, survey design vars.\nPredictors: age, BMI, sex, race."
  },
  {
    "objectID": "slides.html#missingness-data-quality",
    "href": "slides.html#missingness-data-quality",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Missingness & Data Quality",
    "text": "Missingness & Data Quality\n\nMissingness OverviewApproach\n\n\n\nMissingness occurred mainly in BMI and a small portion of diabetes responses.\nDemographic variables and survey design variables were mostly complete.\nPattern appeared consistent with MAR (Missing at Random), appropriate for MICE imputation.\n\n\n\n\nObservations missing the outcome (diabetes_dx) were excluded before imputation.\nPredictors with missing values were imputed later using MICE to avoid complete-case bias."
  },
  {
    "objectID": "slides.html#eda-figures",
    "href": "slides.html#eda-figures",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "EDA Figures",
    "text": "EDA Figures\n\nBMI by Diabetes StatusDiabetes by Race/Ethnicity"
  },
  {
    "objectID": "slides.html#survey-weighted-logistic-regression-1",
    "href": "slides.html#survey-weighted-logistic-regression-1",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Survey-Weighted Logistic Regression",
    "text": "Survey-Weighted Logistic Regression\n\nKey Findings:\n\nAge: odds ratio ~3.03\nBMI: odds ratio ~1.89\nFemales: lower odds vs. males (OR ~0.53)\nRace disparities:\n\nMexican American: ~2.04x higher odds\nOther Hispanic: ~1.59x\nNH Black: ~1.67x\nOther/Multi: ~2.33x\n\n\nNote: Age is modeled in its raw units from NHANES; the OR reflects the scale of the age variable used in the survey-weighted model."
  },
  {
    "objectID": "slides.html#bayesian-modeling-overview",
    "href": "slides.html#bayesian-modeling-overview",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Bayesian Modeling: Overview",
    "text": "Bayesian Modeling: Overview\n\n\nBayesian logistic regression used to quantify parameter uncertainty and compare against survey-weighted results.\n\n\nWeakly informative priors applied to regularize estimation.\n\n\nModel specifications:\n\nFamily: Bernoulli (logit)\nData: adult_imp1 (n = 5,592)\nChains: 4 (2,000 iterations each; 1,000 warmup)\nAdaptation delta: 0.95\nWeights: Normalized NHANES exam weights (wt_norm)"
  },
  {
    "objectID": "slides.html#define-model-and-priors",
    "href": "slides.html#define-model-and-priors",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Define Model and Priors",
    "text": "Define Model and Priors\n\n\nModel FormulaPriorsPrior Visualization\n\n\ndiabetes_dx | weights(wt_norm) ~ age_c + bmi_c + sex + race\n\n\n\nCoefficients: Normal(0, 2.5)\nIntercept: Student t(3, 0, 10)"
  },
  {
    "objectID": "slides.html#data-preparation-mice-imputation",
    "href": "slides.html#data-preparation-mice-imputation",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Data Preparation: MICE Imputation",
    "text": "Data Preparation: MICE Imputation\n\nMultiple Imputation by Chained Equations (MICE) used for missing BMI, age, sex, race.\nGenerated 5 imputations; adult_imp1 used for modeling.\nOutcome variable (diabetes_dx) was not imputed.\nCategorical vars: logistic or polytomous regression.\nCompleted dataset: n = 5,592 adults with full predictor availability."
  },
  {
    "objectID": "slides.html#imputed-dataset",
    "href": "slides.html#imputed-dataset",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Imputed Dataset",
    "text": "Imputed Dataset\n\n\nStandardized PredictorsDistribution of Standardized PredictorsDataset Structure\n\n\nVariables standardized as z-scores:\n\nage_c\nbmi_c"
  },
  {
    "objectID": "slides.html#fit-the-bayesian-model",
    "href": "slides.html#fit-the-bayesian-model",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Fit the Bayesian Model",
    "text": "Fit the Bayesian Model\n\n\nModel FitExtraction of Posterior Odd RatiosOutcome\n\n\n\nModel fit using brm() with:\n\nbernoulli(link = “logit”)\npriors defined previously\n4 chains, 2000 iterations, adapt_delta = 0.95\n\nAll four MCMC chains converged (R-hat ≈ 1.00).\n\n\n\nbayes_or created by exponentiating fixed effects:\n\nOR\n95% credible intervals (LCL, UCL)"
  },
  {
    "objectID": "slides.html#posterior-odds-ratios",
    "href": "slides.html#posterior-odds-ratios",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Posterior Odds Ratios",
    "text": "Posterior Odds Ratios\n\nInterpretation: Age and BMI remain strong predictors. Race disparities persist. Female sex slightly protective. Bayesian ORs closely match frequentist results but include fuller uncertainty ranges."
  },
  {
    "objectID": "slides.html#model-comparison-survey-weighted-vs-bayesian",
    "href": "slides.html#model-comparison-survey-weighted-vs-bayesian",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Model Comparison: Survey-Weighted vs Bayesian",
    "text": "Model Comparison: Survey-Weighted vs Bayesian\nOdds ratios (95% intervals) compared across both modeling frameworks.\n\nComparison TableKey Patterns\n\n\n\n\n\n\nAge and BMI show strong positive associations with diabetes.\nFemale sex has lower odds relative to males.\nNon-White racial/ethnic groups have higher odds than White adults.\nAll posteriors show unimodal, well-defined distributions with narrow credible intervals."
  },
  {
    "objectID": "slides.html#diagnostics-model-fit",
    "href": "slides.html#diagnostics-model-fit",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Diagnostics & Model Fit",
    "text": "Diagnostics & Model Fit\n\nBayesian R²MCMC Diagnostics\n\n\n\nThe model explains ~13% of the variability in diabetes diagnosis.\n\n\n\nAll parameters show:\n\nBulk / Tail ESS &gt; 2,000 → stable posterior sampling\nNo signs of divergence or poor mixing"
  },
  {
    "objectID": "slides.html#posterior-predictive-checks-cont.",
    "href": "slides.html#posterior-predictive-checks-cont.",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Posterior Predictive Checks (cont.)",
    "text": "Posterior Predictive Checks (cont.)\n\nMean of OutcomeStandard DeviationInterpretation\n\n\n\n\n\n\n\n\n\nPosterior replicated datasets match the observed mean and variability.\nStrong evidence of good model calibration.\nThe Bayesian model captures both central tendency and dispersion of diabetes outcomes."
  },
  {
    "objectID": "slides.html#mcmc-diagnostics-1",
    "href": "slides.html#mcmc-diagnostics-1",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "MCMC Diagnostics",
    "text": "MCMC Diagnostics\n\nPosterior DistributionsTrace PlotsAutocorrelationPosterior Correlation (Pairs Plot)\n\n\n\n\nUnimodal, well-centered posteriors\nPositive: age, BMI → higher diabetes risk\nNegative: female sex → protective\n\n\n\n\n\nStable, overlapping chains\nNo drift or divergence\nSupports R-hat ≈ 1.00 and strong convergence\n\n\n\n\n\nRapid decay in autocorrelation\nEfficient sampling and chain independence\n\n\n\n\n\nMinimal correlation among predictors\nDistinct contribution of age, BMI, and sex\nStable posterior densities and low multicollinearity"
  },
  {
    "objectID": "slides.html#prior-vs.-posterior",
    "href": "slides.html#prior-vs.-posterior",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Prior vs. Posterior",
    "text": "Prior vs. Posterior\n\nDistributionsConclusion\n\n\n\n\nPosterior shifts strongly for Age and BMI, showing they are highly informed by data.\nPosterior is narrower than the prior → increased certainty.\nSex shows more overlap → weaker evidence compared to age/BMI.\n\n\n\n\nModel incorporates priors appropriately.\nStrong predictors show clear posterior learning.\nWeak predictors are regularized without distortion."
  },
  {
    "objectID": "slides.html#model-fit-and-calibration",
    "href": "slides.html#model-fit-and-calibration",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Model Fit and Calibration",
    "text": "Model Fit and Calibration\n\nCalibration PlotDiabetes Prevalence ComparisonPopulation vs Posterior Prevalence\n\n\n\n\nCompares observed diabetes outcomes to model-predicted probabilities.\nLOESS curve aligns closely with the diagonal → good model calibration.\n\n\n\n\n\nPosterior predictive mean matches survey-weighted NHANES prevalence.\n\n\n\n\n\nBayesian posterior prevalence overlaps with survey-weighted estimate.\nConfirms strong agreement and no systematic bias."
  },
  {
    "objectID": "slides.html#internal-validation-individual-level-predictions",
    "href": "slides.html#internal-validation-individual-level-predictions",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Internal Validation: Individual-Level Predictions",
    "text": "Internal Validation: Individual-Level Predictions\nPosterior Predictive Distribution for an Example Participant\n\n\nShows uncertainty in an individual’s predicted diabetes probability.\n95% credible interval (red dashed lines) highlights plausible risk range.\nDemonstrates how Bayesian models produce distributions—not point predictions."
  },
  {
    "objectID": "slides.html#posterior-predictions-and-inverse-inference",
    "href": "slides.html#posterior-predictions-and-inverse-inference",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Posterior Predictions and Inverse Inference",
    "text": "Posterior Predictions and Inverse Inference\nInverse Prediction: BMI Needed for Target Diabetes Risk\n\n\nUses the posterior model to determine what BMI corresponds to a target diabetes probability.\nExample target probability: 0.30.\nPosterior predictions show that age and race have a stronger influence on diabetes risk than BMI, although BMI still shows a positive association.\nThe estimated BMI associated with a 30% predicted diabetes probability is approximately 18 kg/m²."
  },
  {
    "objectID": "slides.html#results",
    "href": "slides.html#results",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Results",
    "text": "Results\nPosterior Summary (Key Effects)\n\nFemale vs Male: lower odds; CrI excludes 1.\nBlack vs White: higher odds; CrI excludes 1.\nHispanic vs White: higher odds; CrI overlaps/excludes.\nOther/Multi vs White: higher odds; CrI excludes 1."
  },
  {
    "objectID": "slides.html#comparison-across-models",
    "href": "slides.html#comparison-across-models",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Comparison Across Models",
    "text": "Comparison Across Models\nOdds Ratios: Survey-Weighted vs Bayesian"
  },
  {
    "objectID": "slides.html#discussion-and-limitations",
    "href": "slides.html#discussion-and-limitations",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Discussion and Limitations",
    "text": "Discussion and Limitations\n\nDiscussionLimitations\n\n\n\nBayesian modeling provided full posterior distributions, offering richer uncertainty quantification than classical MLE.\nPosterior predictive checks showed that simulated diabetes prevalence closely matched the survey-weighted estimate, indicating excellent model calibration.\n\n\n\n\nUsed a single imputed dataset rather than fully propagating imputation uncertainty → possible slight understatement of variance.\nNHANES examination weights were normalized rather than modeled with full design-based structure → may slightly impact SEs.\nWeakly informative priors (Normal(0,2.5), t(3,0,10)) were not tuned; alternatives could shift posterior widths or shrinkage.\nAll diagnostics were excellent, but results apply specifically to NHANES 2013–2014 and may not generalize to other cycles or longitudinal contexts.\nNo external validation or sensitivity analysis; individual-level predictions are illustrative only, not clinical."
  },
  {
    "objectID": "slides.html#targeted-therapy-translational-interpretation",
    "href": "slides.html#targeted-therapy-translational-interpretation",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Targeted Therapy & Translational Interpretation",
    "text": "Targeted Therapy & Translational Interpretation\n\nTargeted TherapyTranslational Research Implications\n\n\n\nBayesian posterior probabilities enable individualized risk estimation rather than simple group-level inference.\nUseful clinically for identifying predictor thresholds where diabetes risk becomes meaningful (e.g., reaching 30% risk at a certain BMI).\nSupports precision public health by translating model outputs into actionable strategies for screening and prevention.\nRaises the key question: What changes in modifiable predictors would lower diabetes risk?\n\n\n\n\nThe model can inform prevention and intervention strategies.\nBMI is the only modifiable predictor in the model; sex, race, and age are not.\nModel can be used to explore how much BMI would need to change to reduce risk below a desired probability threshold.\nNon-modifiable predictors held constant; BMI varied to evaluate achievable changes in predicted risk."
  },
  {
    "objectID": "slides.html#conclusion-1",
    "href": "slides.html#conclusion-1",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Conclusion",
    "text": "Conclusion\n\nFrequentist and Bayesian models yield consistent predictors.\nBayesian approach provides credible intervals and improved uncertainty handling.\nPosterior predictive checks show strong calibration.\nFramework extendable to future cycles and risk prediction tasks."
  },
  {
    "objectID": "slides.html#introduction-aims",
    "href": "slides.html#introduction-aims",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Introduction & Aims",
    "text": "Introduction & Aims\n\nIntroductionAims\n\n\n\nDiabetes is a major public health issue.\nClassical logistic regression can become unstable with missing data or separation issues.\nBayesian logistic regression improves stability by incorporating prior information and providing stronger uncertainty estimates.\nThis project evaluates whether Bayesian methods offer more reliable and interpretable diabetes risk estimates than traditional approaches.\n\n\n\n\nApply Bayesian logistic regression to predict diabetes status.\nExamine associations between diabetes and BMI, age (≥20), sex, and race.\nUse NHANES 2013–2014 data accounting for complex survey design.\nAddress challenges such as missing data, complete case bias, and data separation."
  },
  {
    "objectID": "slides.html#posterior-odds-ratios-bayesian-model",
    "href": "slides.html#posterior-odds-ratios-bayesian-model",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Posterior Odds Ratios (Bayesian Model)",
    "text": "Posterior Odds Ratios (Bayesian Model)\n\nSummaryOR Table\n\n\n\nPosterior odds ratios (ORs) derived from exponentiated fixed effects.\nCredible intervals (CrI) quantify uncertainty around each effect.\nInterpretation reflects change in odds for a 1 SD increase (age_c, bmi_c) or relative to reference groups."
  },
  {
    "objectID": "slides.html#bayesian-diagnostics-model-fit",
    "href": "slides.html#bayesian-diagnostics-model-fit",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Bayesian Diagnostics & Model Fit",
    "text": "Bayesian Diagnostics & Model Fit\n\nConvergence IndicatorsTrace PlotsPosterior DistributionsAutocorrelation\n\n\n\n\nR-hat ≈ 1.00 for all parameters → excellent convergence\n\nBulk / Tail ESS &gt; 2,000 → stable posterior sampling\n\nNo divergences or mixing issues\n\n\n\n\n\nChains overlap well\n\nNo drift or instability\n\nStrong evidence of convergence\n\n\n\n\n\nUnimodal and well-centered\n\nAge & BMI → positive associations\n\nFemale sex → protective effect\n\n\n\n\n\nRapid decay across all parameters\n\nIndicates efficient sampling and chain independence"
  },
  {
    "objectID": "slides.html#bayesian-fundamentals",
    "href": "slides.html#bayesian-fundamentals",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Bayesian Fundamentals",
    "text": "Bayesian Fundamentals\n\nOverviewModel StructurePrior SpecificationPosterior Distribution\n\n\n\nBayesian logistic regression combines prior information with the observed data to calculate posterior distributions for model parameters.\nThis approach is especially useful when classical logistic regression becomes unstable due to missing data, small subgroups, or data separation.\n\n\n\n\nThe model estimates the log-odds of diabetes using:\n\nage (standardized),\n\nBMI (standardized),\n\nsex,\n\nrace/ethnicity.\n\nEach parameter is treated as a random variable with its own probability distribution.\n\n\n\n\nWeakly informative priors help stabilize estimation without dominating the data.\n\nCoefficients: Normal(0, 2.5)\nIntercept: Student t(3, 0, 10)\n\nPriors act as regularization, preventing extreme estimates in low-information settings.\n\n\n\n\nAfter observing the data, priors are updated to form the posterior, which provides:\n\ndirect probabilistic interpretation,\nfull uncertainty quantification,\ncredible intervals for all predictors."
  },
  {
    "objectID": "slides.html#data-description",
    "href": "slides.html#data-description",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Data Description",
    "text": "Data Description\n\nData SourcesMerging & Cohort CreationKey VariablesHandling Missing Data\n\n\n\nNHANES 2013–2014 public datasets:\n\nDEMO_H (demographics)\nBMX_H (body measures)\nDIQ_H (diabetes questionnaire)\n\n\n\n\n\nImported .XPT files and merged using SEQN (unique participant ID).\nRestricted the cohort to adults aged ≥ 20.\nStandardized variable formats before merging.\n\n\n\n\nOutcome: diabetes_dx (binary), derived from DIQ010.\nPredictors: age, BMI, sex, race/ethnicity.\nSurvey design: NHANES uses a complex multistage design\n(weights: WTMEC2YR, clusters: SDMVPSU, strata: SDMVSTRA).\n\n\n\n\nMissingness mainly in BMI and a small portion of diabetes responses.\nMissing predictors handled via MICE;\noutcome (diabetes_dx) not imputed."
  },
  {
    "objectID": "slides.html#missing-data-complete-case-analysis",
    "href": "slides.html#missing-data-complete-case-analysis",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Missing Data & Complete Case Analysis",
    "text": "Missing Data & Complete Case Analysis\n\nMissing Data OverviewComplete Case AnalysisSurvey-Weighted Logistic Regression (Complete Case)Model SummarySurvey-Weighted Plot\n\n\n\nNHANES includes complex multistage sampling, and some variables contain missing values.\nPatterns of missingness were evaluated to determine whether Missing At Random (MAR) was a reasonable assumption.\n\n\n\n\nA complete-case subset was created to ensure data quality and comparability.\nThis reduced the analytic sample but ensured that all required predictors were available for modeling.\nThis is consistent with the approach seen in Namita’s slides.\n\n\n\n\nA survey-weighted logistic regression was fitted using the complete-case dataset.\nNHANES sampling design elements (weights, strata, and clusters) were applied.\nThis model provides population-representative estimates of diabetes risk.\n\n\n\n\nPredictors included: age (standardized), BMI (standardized), sex, and race.\nSurvey weights ensure results reflect the U.S. adult population.\nThe complete-case model establishes a baseline before Bayesian modeling."
  },
  {
    "objectID": "slides.html#survey-weighted-regression-plot",
    "href": "slides.html#survey-weighted-regression-plot",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Survey-Weighted Regression Plot",
    "text": "Survey-Weighted Regression Plot\n\nOverviewInterpretationVisual\n\n\n\nThis visualization illustrates the population-adjusted relationship between BMI and predicted diabetes probability under the survey-weighted logistic regression model.\nSurvey weighting ensures the curve reflects U.S. adult population patterns, not just sample characteristics.\n\n\n\n\nHigher BMI levels correspond to substantially higher predicted diabetes probability.\nDifferences observed across race/ethnicity highlight subgroup variation in diabetes risk.\nThese weighted predictions provide a valuable reference point for comparing how the Bayesian model handles uncertainty and subgroup effects."
  },
  {
    "objectID": "slides.html#survey-weighted-model-summary",
    "href": "slides.html#survey-weighted-model-summary",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Survey-Weighted Model Summary",
    "text": "Survey-Weighted Model Summary\n\nOverviewInterpretation\n\n\n\nSummary of coefficient effects provides insight into population-adjusted predictors of diabetes.\nThis completes the classical modeling pathway before transitioning into MICE + Bayesian modeling.\n\n\n\n\nAge and BMI show strong positive associations with diabetes.\nDifferences across sex and racial/ethnic groups remain after weighting.\nThese results help establish expectations when comparing to Bayesian outcomes."
  },
  {
    "objectID": "slides.html#mice-imputation-connector",
    "href": "slides.html#mice-imputation-connector",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "MICE Imputation (Connector)",
    "text": "MICE Imputation (Connector)\n\nOverviewKey PointsVisual\n\n\n\nAfter evaluating missingness and running the survey-weighted complete-case model, the next step is to address missing data more robustly.\nMultivariate Imputation by Chained Equations (MICE) creates multiple plausible versions of the dataset.\nThis improves statistical efficiency and avoids biases introduced by complete-case restriction.\n\n\n\n\nImputations preserve relationships among predictors.\nEach completed dataset is analyzed separately.\nResults are pooled to produce final estimates."
  },
  {
    "objectID": "slides.html#transition-to-bayesian-modeling",
    "href": "slides.html#transition-to-bayesian-modeling",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Transition to Bayesian Modeling",
    "text": "Transition to Bayesian Modeling\n\nAfter complete-case frequentist analysis, MICE is used for imputing missing predictors.\nBayesian model applied to imputed dataset."
  },
  {
    "objectID": "slides.html#implementing-bayesian-logistic-regression",
    "href": "slides.html#implementing-bayesian-logistic-regression",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Implementing Bayesian Logistic Regression",
    "text": "Implementing Bayesian Logistic Regression\n\nData (Adults)MICEBayesian Logistic Regression ModelPriorCode\n\n\n\nRaw imported: 10,175 observations\n\nSurvey-weighted adult sample: 5,769 (~7%)\n\nComplete-case sample: 5,348 (no missing values)\nBayesian logistic regression is fit on a completed dataset.\n\nIn this analysis, missing values were addressed using MICE before model fitting.\n\nImputed dataset (adult_imp1): 5,592\n\n(Raw data → MICE → imputed dataset)\n\n\n\n\nMultivariate Imputation by Chained Equations (MICE)\n\nIteratively imputes incomplete variables using regression models.\n\nPredictive Mean Matching (PMM) used for continuous variables;\nlogistic models used for binary variables (Buuren and Groothuis-Oudshoorn 2011).\n\nRubin’s rule pools estimates across imputations:\nPosterior = parameter uncertainty + missing data uncertainty\n\n\n\nModel fitted in R using brms (Stan backend), with 2000 iterations.\nBayesian Regression Formula\ndiabetes_dx | weights(wt_norm) ~ age_c + bmi_c + sex + race\nModel settings:\n* Dataset: pooled imputed dataset 1\n* Family: Bernoulli\n* Chains: 4\n* Iterations: 2000\n* Seed: 123\n* Priors:\n* normal(0, 2.5) for coefficients\n* student_t(3, 0, 10) for the intercept\n\n\n\nIntercept prior: student_t(3, 0, 10) (Schoot et al. 2013)\n\ndf = 3: heavy-tailed → allows occasional extreme values\n\nlocation = 0: centered prior expectation\n\nscale = 10: wider distribution (more prior uncertainty)\n\nCoefficient prior: Normal(0, 2.5) (Gelman et al. 2008)\n\nWeakly informative\n\nMean = 0, SD = 2.5\n\nRegularizes extreme estimates and stabilizes coefficients under sparse data or quasi-separation\n\n\n\n\nlibrary(gt)\npriors &lt;- c(\n  set_prior(\"normal(0, 2.5)\", class = \"b\"),\n  set_prior(\"student_t(3, 0, 10)\", class = \"Intercept\")\n)\n\nbayes_fit &lt;- brm(\n  formula = diabetes_dx | weights(wt_norm) ~ age_c + bmi_c + sex + race,\n  data    = adult_imp1,\n  family  = bernoulli(link = \"logit\"),\n  prior   = priors,\n  chains  = 4, iter = 2000, seed = 123,\n  control = list(adapt_delta = 0.95),\n  refresh = 0\n)\n\nbrm() from the brms package fits the Bayesian model using Stan."
  },
  {
    "objectID": "slides.html#bayesian-odds-ratios",
    "href": "slides.html#bayesian-odds-ratios",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Bayesian Odds Ratios",
    "text": "Bayesian Odds Ratios\n\nPosterior Odds RatiosVisual\n\n\n\nThis plot summarizes posterior OR estimates from the Bayesian logistic regression.\nDisplays uncertainty via credible intervals."
  },
  {
    "objectID": "slides.html#mcmc-markov-chain-monte-carlo",
    "href": "slides.html#mcmc-markov-chain-monte-carlo",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "MCMC (Markov Chain Monte Carlo)",
    "text": "MCMC (Markov Chain Monte Carlo)\n\nMarkov ProcessCodePosterior Estimates\n\n\nMCMC convergence occurs when information from the prior and likelihood has been fully combined, and the sampler is drawing valid samples from the posterior distribution.\n\nGibbs sampler: an MCMC algorithm that generates a sequence of random parameter vectors.\nBurn-in period: the first 500–1,000 iterations, which are not representative of the posterior and are discarded.\nAfter burn-in, the chain reaches stationarity.\n\nImportant: Each sample depends on the previous one → samples are not independent and are sequentially correlated.\n\n\npost &lt;- posterior_summary(bayes_fit, robust = FALSE) %&gt;%\n  as_tibble(rownames = \"term\")\n\n# Extract convergence diagnostics\ndiag &lt;- rstan::monitor(as.array(bayes_fit$fit), print = FALSE) %&gt;%\n  as_tibble(rownames = \"term\") %&gt;%\n  select(term, Rhat, n_eff = Bulk_ESS)\n\nbayes_results &lt;- post %&gt;%\n  left_join(diag, by = \"term\") %&gt;%\n  mutate(\n    OR     = exp(Estimate),\n    OR_LCL = exp(Q2.5),\n    OR_UCL = exp(Q97.5)\n  ) %&gt;%\n  select(term, Estimate, Est.Error, Q2.5, Q97.5, Rhat, n_eff, OR, OR_LCL, OR_UCL)\n\nrstan::monitor (RStan) computes summary statistics and evaluates MCMC convergence.\n\nIt provides:\n\nPosterior mean and standard deviation (SD)\nMonte Carlo standard error (MCSE)\nQuantiles: 2.5%, 25%, 50%, 75%, 97.5%\nEffective sample sizes (ESS): Bulk_ESS and Tail_ESS\nR-hat: convergence diagnostic"
  },
  {
    "objectID": "slides.html#model-diagnostics",
    "href": "slides.html#model-diagnostics",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Model Diagnostics",
    "text": "Model Diagnostics\n\nMCMC ConvergenceTrace Plots (Stable Posteriors)Density PlotAutocorrelation Plot\n\n\nGood convergence is indicated by:\n\nNo drift across iterations\n\nWell-mixed chains with stable posterior estimates\n\nR-hat ≈ 1\n\nEffective sample size (ESS) ≈ 3500, meaning the chain contains information equivalent to ~3,500 independent samples\n\nReflects good mixing, low autocorrelation, and highly reliable posterior estimates\n\n\n\n\n No upward or downward trend (no drift). Chains overlap and stay within the same band.\n\n\n Stable posteriors with precise estimates and high uncertainty, narrow distributions, smooth and unimodal.\n\n\n - Rapid decay to zero shows. - Well mixed, independent draws, no relationship between parameters."
  },
  {
    "objectID": "slides.html#regression-results-posterior-estimates",
    "href": "slides.html#regression-results-posterior-estimates",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Regression Results (Posterior Estimates)",
    "text": "Regression Results (Posterior Estimates)\n\nOverviewKey FindingsInterpretation\n\n\n\nPosterior estimates summarize the average effect of each predictor and the credible intervals that quantify uncertainty.\nContinuous variables (age_c and bmi_c) represent 1 SD increases.\nCategorical predictors are interpreted relative to their reference groups\n(Male, Non-Hispanic White).\n\n\n\n\nAge: Strong positive association with diabetes risk.\nBMI: Positive, consistent with expected metabolic profiles.\nFemale: Slightly lower risk relative to males.\nNH Black and several Hispanic subgroups: Higher odds of diabetes compared to White adults.\n\n\n\n\nPosterior estimates are stable and align with known epidemiologic patterns.\nCredible intervals reflect model uncertainty and population variability."
  },
  {
    "objectID": "slides.html#prior-vs.-posterior-age-bmi",
    "href": "slides.html#prior-vs.-posterior-age-bmi",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Prior vs. Posterior (Age & BMI)",
    "text": "Prior vs. Posterior (Age & BMI)\n\nOverviewKey FindingsInterpretationVisual\n\n\n\nThis comparison illustrates Bayesian learning by showing how weakly informative priors update after observing NHANES data.\nPriors were centered near zero with moderate variance, allowing the data to drive inference.\n\n\n\n\nThe posterior distributions for age and BMI shift away from zero and become notably narrower.\nThis reflects strong evidence in the data that both predictors have meaningful associations with diabetes risk.\n\n\n\n\nPriors provided regularization, but the NHANES sample contributed most of the information.\nThis slide highlights how Bayesian updating blends prior expectations with empirical evidence."
  },
  {
    "objectID": "slides.html#posterior-draws",
    "href": "slides.html#posterior-draws",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Posterior Draws",
    "text": "Posterior Draws\n\nInterpretation: Posterior distributions are well-defined, unimodal, and narrower than priors, indicating strong information from the data."
  },
  {
    "objectID": "slides.html#code",
    "href": "slides.html#code",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Code",
    "text": "Code\n\nEssential Bayesian Model CodeMCMC SamplingKey Point\n\n\n# Standardize continuous predictors\nnhanes$age_c &lt;- scale(nhanes$age)\nnhanes$bmi_c &lt;- scale(nhanes$bmi)\n\n# Bayesian logistic regression model\nmodel_bayes &lt;- brm(\n  diabetes_dx ~ age_c + bmi_c + sex + race,\n  data = nhanes,\n  family = bernoulli(),\n  chains = 4, \n  iter = 2000,\n  warmup = 1000,\n  cores = 4,\n  seed = 123\n)\n\n\nmonitor(model_bayes$fit)\n\n\n\nThis code reflects the core model and sampling structure used for the analysis."
  },
  {
    "objectID": "slides.html#plot",
    "href": "slides.html#plot",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Plot",
    "text": "Plot\n\nOverviewInterpretationVisual\n\n\n\nThis plot visualizes posterior distribution summaries for the key model parameters.\nEach distribution shows the range of plausible coefficient values supported by the data.\n\n\n\n\nThe strongest effects appear for age and BMI, reflecting well-established diabetes risk factors.\nCategorical effects show consistent patterns across sex and race/ethnicity."
  },
  {
    "objectID": "slides.html#model-diagnostics-1",
    "href": "slides.html#model-diagnostics-1",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Model Diagnostics",
    "text": "Model Diagnostics\n\nOverviewInterpretationVisual\n\n\n\nThese diagnostics evaluate chain mixing, parameter stability, and autocorrelation.\nTogether, they help confirm that posterior estimates are reliable.\n\n\n\n\nTrace plots: chains mix well with no evidence of divergence.\nDensity plots: posteriors are smooth and unimodal for all parameters.\nAutocorrelation: rapid decay indicates efficient sampling."
  },
  {
    "objectID": "slides.html#r-square",
    "href": "slides.html#r-square",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "R Square",
    "text": "R Square\n\nTableInterpretation\n\n\n\n\n\n\n\n\n\n\nApproximately 13% of the variability in diabetes status is explained, with credible uncertainty bounds indicating modest explanatory power.\n\nPredictors (Age, BMI, Sex, Race, etc.) capture some—but not all—factors influencing diabetes risk.\n\nAdditional influences such as genetics, lifestyle, and environmental factors likely play substantial roles."
  },
  {
    "objectID": "slides.html#translational-research",
    "href": "slides.html#translational-research",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Translational Research",
    "text": "Translational Research\n\nInternal Validation — Individual ParticipantParticipant DataExternal Validation — Representative\n\n\nThreshold: &gt;30%\nBMI is modifiable — interventions lower risk (sex & race held constant).\nUsed posterior_linpred(transform = TRUE) to generate the posterior predictive distribution.\n\n\n\n\n\n\n\n\n\n\n\nMedian predicted probability: ~0.25\n\n95% Credible Interval: ~0.20–0.31\n\nThis corresponds to approximately a 1 in 4 chance of diabetes.\n\n\nparticipant1_data &lt;- adult[1, ]\nphat1 &lt;- posterior_linpred(bayes_fit, newdata = participant1_data, transform = TRUE)\n\npost_pred_df &lt;- data.frame(pred = phat1)\nci_95_participant1 &lt;- quantile(phat1, c(0.025, 0.975))\n\nggplot(post_pred_df, aes(x = pred)) +\n  geom_density(color='darkblue', fill='lightblue') +\n  geom_vline(xintercept = ci_95_participant1[1], color='red', linetype='dashed') +\n  geom_vline(xintercept = ci_95_participant1[2], color='red', linetype='dashed') +\n  xlab('Probability of being diabetic (Outcome = 1)') +\n  ggtitle('Posterior Predictive Distribution 95% Credible Interval') +\n  theme_bw()\nnew_participant &lt;- data.frame(\n  age_c = 40,\n  bmi_c = 25,\n  sex = \"Female\",\n  race = \"Mexican American\"\n)\n\n\nIn contrast to Participant 1, this example evaluates a\nhypothetical individual:\n\nage_c = 40\n\nbmi_c = 25\n\nsex = “Female”\n\nrace = “Mexican American”\n\nThe posterior predictive distribution for this profile shows an\nextremely high predicted probability of diabetes, with most\nposterior draws clustering near 1.0.\nThis illustrates how the same Bayesian model can produce very different risk estimates for individuals with different demographic and clinical characteristics, highlighting the importance of personalized risk assessment.\n\n\n\n\n\nX-axis: predicted probability of diabetes (Outcome = 1)\nY-axis: density of predicted probabilities\nBlue curve: posterior predictive distribution\nShaded area: high probability values near 1 indicate increased diabetes risk\nRed dashed line: upper bound of the 95% credible interval\nPeak predicted probability: approximately 1.0"
  },
  {
    "objectID": "slides.html#internal-validation",
    "href": "slides.html#internal-validation",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Internal Validation",
    "text": "Internal Validation\n\nInterpretation: Posterior predictive distribution for an individual reveals uncertainty in personalized diabetes risk, emphasizing probabilistic interpretation."
  },
  {
    "objectID": "slides.html#participant-data",
    "href": "slides.html#participant-data",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Participant Data",
    "text": "Participant Data\n\nOverviewInterpretationVisual\n\n\n\nIndividual-level posterior predictions show the range of possible outcomes for a specific participant.\nThis helps illustrate how uncertainty differs from person to person.\n\n\n\n\nThe distribution of predicted probabilities reflects the model’s confidence for that participant.\nWider distributions indicate greater uncertainty; narrower ones indicate stronger evidence."
  },
  {
    "objectID": "slides.html#external-validation",
    "href": "slides.html#external-validation",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "External Validation",
    "text": "External Validation\nPosterior Predicted Prevalence vs NHANES\n\nInterpretation\n\nExternal validation compares the posterior-predicted diabetes prevalence with the known NHANES survey prevalence.\nThe Bayesian model’s predicted prevalence closely aligns with the population benchmark, with overlapping intervals indicating strong generalizability.\nThe agreement between model-based and survey-based prevalence measures confirms that the Bayesian model performs reliably beyond the training dataset."
  },
  {
    "objectID": "slides.html#reverse-prediction",
    "href": "slides.html#reverse-prediction",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Reverse Prediction",
    "text": "Reverse Prediction\n\nPredicting BMIPractical Implications\n\n\nPredicting the BMI value for a representative individual\n(diabetic, age = 40, female, Mexican American).\nlogit(Diabetesᵢ) = β₀ + β_Ageᵢ + β_BMIᵢ + β_Sexᵢ + β_Raceᵢ\n\n\n\n\n\nX-axis: BMI values (centered or raw, depending on the model)\nY-axis: predicted probability of diabetes for a representative individual\nBlue curve: predicted probability of diabetes across the BMI range\n— consistently high (≈ 1)\nRed horizontal line: target probability of diabetes = 0.3\nRed vertical line: BMI where the predicted probability is closest to this target (≈ 18)\n\n\n“At what BMI does this individual have a 30% chance of being diabetic?”\nProfile evaluated:\nage = 40, female, Mexican American\nThis is an inverse prediction—identifying the BMI associated with a target diabetes risk.\nKey takeaway: * Even a relatively low BMI (~18) corresponds to a 30% predicted diabetes probability. * This suggests that non-BMI factors (age, sex, race) strongly influence risk for this profile."
  },
  {
    "objectID": "slides.html#predicting-bmi",
    "href": "slides.html#predicting-bmi",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Predicting BMI",
    "text": "Predicting BMI\n\nOverviewInterpretationVisual\n\n\n\nThe model can estimate plausible BMI values given a specific diabetes risk scenario.\nThese posterior predictions reflect uncertainty around BMI for individuals with similar characteristics.\n\n\n\n\nThe distribution of predicted BMI values highlights where most plausible outcomes lie.\nThis approach helps translate model output into clinically meaningful ranges."
  },
  {
    "objectID": "slides.html#practical-implications",
    "href": "slides.html#practical-implications",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Practical Implications",
    "text": "Practical Implications\n\nOverviewInterpretationVisual\n\n\n\nBayesian logistic regression provides flexible, uncertainty-aware estimates that can support decision-making in clinical and public health settings.\nProbability-based predictions allow for individualized risk assessment.\n\n\n\n\nPosterior distributions help identify individuals likely to benefit from early screening or intervention.\nThe model can be extended or updated as more data becomes available, improving long-term accuracy and relevance."
  },
  {
    "objectID": "slides.html#frequentist",
    "href": "slides.html#frequentist",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Frequentist",
    "text": "Frequentist\n\nUnstable with missing data or quasi-separation.\nRelies on large-sample asymptotics.\nConfidence intervals rely on repeated sampling logic.\nParameters treated as fixed unknown constants."
  },
  {
    "objectID": "slides.html#bayesian",
    "href": "slides.html#bayesian",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Bayesian",
    "text": "Bayesian\n\nProbability = degree of belief.\nIncorporates priors → stabilizes estimates.\nNaturally handles missingness and small samples.\nParameters treated as random variables.\nProduces full posterior distributions."
  },
  {
    "objectID": "slides.html#frequentist-vs-bayesian-methods",
    "href": "slides.html#frequentist-vs-bayesian-methods",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Frequentist vs Bayesian Methods",
    "text": "Frequentist vs Bayesian Methods\n\nFrequentistBayesian\n\n\n\nUnstable with missing data or quasi-separation.\nRelies on large-sample asymptotics.\nConfidence intervals rely on repeated sampling logic.\nParameters treated as fixed unknown constants.\n\n\n\n\nProbability = degree of belief.\nIncorporates priors → stabilizes estimates.\nNaturally handles missingness and small samples.\nParameters treated as random variables.\nProduces full posterior distributions."
  },
  {
    "objectID": "slides.html#bayes-theorem",
    "href": "slides.html#bayes-theorem",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Bayes’ Theorem",
    "text": "Bayes’ Theorem\n[P( heta | D) = \frac{P(D | heta) P( heta)}{P(D)}]\n\nθ: regression parameters\nD: NHANES data (age, BMI, sex, race → diabetes)\nPrior: beliefs before data\nLikelihood: probability of data given parameters\nPosterior: updated beliefs after observing data"
  },
  {
    "objectID": "slides.html#logistic-link-function",
    "href": "slides.html#logistic-link-function",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Logistic Link Function",
    "text": "Logistic Link Function\n\nPredicts probability of diabetes (0–1).\nConverts predictor effects to log-odds.\nExample model: (P(Y=1) = \beta_0 + \beta_{age} + \beta_{bmi} + \beta_{sex} + \beta_{race})"
  },
  {
    "objectID": "slides.html#priors",
    "href": "slides.html#priors",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Priors",
    "text": "Priors\n\nIntercept: Student-t(3, 0, 10)\nCoefficients: Normal(0, 2.5)\n\nInterpretation: Weakly informative priors regularize coefficients, preventing extreme estimates in low-information subgroups while still letting the data dominate."
  },
  {
    "objectID": "slides.html#posterior-distributions",
    "href": "slides.html#posterior-distributions",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Posterior Distributions",
    "text": "Posterior Distributions\nMCMC Area Plots\n\nInterpretation\n\nPosterior draw distributions show the full range of plausible coefficient values supported by both the prior and the observed data.\nThese distributions reveal the shape, spread, and central tendency of each parameter’s posterior.\nNarrower distributions indicate greater certainty, while wider distributions reflect higher uncertainty.\nAll parameters exhibit smooth, unimodal distributions, confirming stable convergence and reliable posterior estimation.\nThis visualization complements the posterior odds ratio summaries by showing how much uncertainty exists around each parameter."
  },
  {
    "objectID": "slides.html#predictor-outcome-variables",
    "href": "slides.html#predictor-outcome-variables",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Predictor & Outcome Variables",
    "text": "Predictor & Outcome Variables\n\nOutcome: DIQ010 (doctor-diagnosed diabetes)\nPredictors: RIDAGEYR, BMXBMI, RIAGENDR, RIDRETH1\nStandardization applied to age & BMI"
  },
  {
    "objectID": "slides.html#missing-data-complete-case",
    "href": "slides.html#missing-data-complete-case",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Missing Data & Complete Case",
    "text": "Missing Data & Complete Case\n\nInterpretation: Missingness primarily affects BMI; pattern supports MAR. Complete-case analysis reduces sample size but provides a baseline model."
  },
  {
    "objectID": "slides.html#survey-regression-visualization",
    "href": "slides.html#survey-regression-visualization",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Survey Regression Visualization",
    "text": "Survey Regression Visualization\n\nInterpretation: Non-Hispanic Black and Hispanic groups show elevated predicted diabetes probability compared to White adults after adjustment."
  },
  {
    "objectID": "slides.html#mice-imputation",
    "href": "slides.html#mice-imputation",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "MICE Imputation",
    "text": "MICE Imputation\nImputed Dataset Summary\n\nAdults after survey filtering: 5,769\nComplete-case adults: 5,348\nMICE-imputed dataset: 5,592\nMICE generated multiple plausible values for missing predictors.\n\nInterpretation\n\nMICE (Multivariate Imputation by Chained Equations) iteratively fills in missing values using regression models tailored to each variable type.\nThis approach creates datasets that preserve the relationships among predictors, reducing bias from complete-case analysis.\nImputation incorporates uncertainty and provides datasets suitable for Bayesian modeling.\nBayesian models require complete data, so imputation is a necessary preprocessing step.\n\nImputation Structure"
  },
  {
    "objectID": "slides.html#bayesian-model-specification",
    "href": "slides.html#bayesian-model-specification",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Bayesian Model Specification",
    "text": "Bayesian Model Specification\n\nFormula: diabetes_dx | weights(wt_norm) ~ age_c + bmi_c + sex + race\nPriors: weakly informative\nMCMC: 4 chains, 2000 iterations"
  },
  {
    "objectID": "slides.html#prior-vs-posterior-age-bmi",
    "href": "slides.html#prior-vs-posterior-age-bmi",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Prior vs Posterior (Age & BMI)",
    "text": "Prior vs Posterior (Age & BMI)\nPrior vs Posterior Distributions\n\nInterpretation\n\nThe prior distributions for age and BMI are intentionally wide, reflecting high initial uncertainty about the effect sizes before observing any data.\nAfter fitting the model with NHANES data, the posterior distributions become narrower and more concentrated, demonstrating that the data provided substantial information.\nThe posterior distributions shift away from zero, indicating strong evidence that both age and BMI are positively associated with diabetes risk.\nThis comparison illustrates the core principle of Bayesian updating: the data refine and update the prior beliefs, resulting in posteriors that more accurately reflect true population patterns."
  },
  {
    "objectID": "slides.html#model-fit-bayesian-r²",
    "href": "slides.html#model-fit-bayesian-r²",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Model Fit — Bayesian R²",
    "text": "Model Fit — Bayesian R²\n\nInterpretation: Bayesian R² centers around ~0.13. This is expected: diabetes is multifactorial, and a small number of predictors explains modest variance."
  },
  {
    "objectID": "slides.html#reverse-prediction-bmi-threshold",
    "href": "slides.html#reverse-prediction-bmi-threshold",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Reverse Prediction — BMI Threshold",
    "text": "Reverse Prediction — BMI Threshold\n\nInterpretation: Reverse prediction indicates BMI levels associated with a target diabetes probability (e.g., 30%). Results reveal strong influence of age, sex, and race in risk thresholds."
  },
  {
    "objectID": "slides.html#completed-deck-reconstructed-in-namitas-structure-cleaned-and-updated-with-autumns-results-and-figures",
    "href": "slides.html#completed-deck-reconstructed-in-namitas-structure-cleaned-and-updated-with-autumns-results-and-figures",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "(Completed deck reconstructed in Namita’s structure, cleaned, and updated with Autumn’s results and figures)",
    "text": "(Completed deck reconstructed in Namita’s structure, cleaned, and updated with Autumn’s results and figures)\nNOTE: # Bayesian Logistic Regression for Predicting Diabetes Risk"
  },
  {
    "objectID": "slides.html#completed-slide-deck-namita-structure-cleaned-using-autumns-results-figures",
    "href": "slides.html#completed-slide-deck-namita-structure-cleaned-using-autumns-results-figures",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Completed Slide Deck (Namita Structure, Cleaned, Using Autumn’s Results & Figures)",
    "text": "Completed Slide Deck (Namita Structure, Cleaned, Using Autumn’s Results & Figures)"
  },
  {
    "objectID": "slides.html#title-slide",
    "href": "slides.html#title-slide",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Title Slide",
    "text": "Title Slide\nBayesian Logistic Regression for Predicting Diabetes Risk NHANES 2013–2014 Namita Mishra & Autumn Wilcox Advisor: Dr. Ashraf Cohen"
  },
  {
    "objectID": "slides.html#frequentist-limitations",
    "href": "slides.html#frequentist-limitations",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Frequentist Limitations",
    "text": "Frequentist Limitations\n\nInstability with missing data or quasi-separation.\nLarge-sample assumptions not always appropriate.\nConfidence intervals describe repeated sampling, not parameter probability.\nParameters treated as fixed unknown constants."
  },
  {
    "objectID": "slides.html#prior-distributions",
    "href": "slides.html#prior-distributions",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Prior Distributions",
    "text": "Prior Distributions\n\nCoefficients: Normal(0, 2.5)\nIntercept: Student t(3, 0, 10)\nWeakly informative to stabilize estimates."
  },
  {
    "objectID": "slides.html#missing-data",
    "href": "slides.html#missing-data",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Missing Data",
    "text": "Missing Data\nMissingness Pattern\n\nInterpretation\n\nMAR (Missing At Random): Missingness is related to other variables in the dataset but not to the value of the missing variable itself.\nThis assumption is appropriate for NHANES and supports the use of MICE."
  },
  {
    "objectID": "slides.html#complete-case-analysis-survey-weighted",
    "href": "slides.html#complete-case-analysis-survey-weighted",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Complete Case Analysis (Survey-Weighted)",
    "text": "Complete Case Analysis (Survey-Weighted)\nSurvey-Weighted Logistic Regression Results\n\nModel estimated using svyglm() with a quasibinomial family.\nDesign-adjusted coefficients, standard errors, and odds ratios reflect population-representative associations for U.S. adults.\n\nSurvey-Weighted ORs (Autumn’s Results)\n\nAge (1 SD ↑): OR = 3.03 → Older adults have ~3× higher odds of diabetes.\nBMI (1 SD ↑): OR = 1.88 → Higher BMI increases risk of diabetes.\nFemale vs Male: OR = 0.53 → Females have ~47% lower odds of diabetes.\nRace vs NH White:\n\nMexican American: OR = 2.04\nOther Hispanic: OR = 1.59\nNH Black: OR = 1.67\nOther/Multi-racial: OR = 2.33\n\n\nInterpretation\n\nAge and BMI are the strongest predictors in the survey-weighted model.\nFemales have lower diabetes risk compared to males.\nCertain racial/ethnic groups have higher odds of diabetes relative to Non-Hispanic Whites.\nThis complete-case analysis provides a baseline reference before Bayesian modeling and highlights expected epidemiological relationships.\n\nRegression Plot"
  },
  {
    "objectID": "slides.html#posterior-coefficient-estimates",
    "href": "slides.html#posterior-coefficient-estimates",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Posterior Coefficient Estimates",
    "text": "Posterior Coefficient Estimates\nPosterior Odds Ratios\n\nInterpretation\n\nPosterior estimates summarize the average effect of each predictor, incorporating both prior information and observed NHANES data.\nContinuous predictors (age_c and bmi_c) represent 1 SD increases, making their effects directly comparable.\nAge shows a strong positive association with diabetes risk.\nBMI also demonstrates a positive relationship, consistent with metabolic expectations.\nFemale sex shows a slightly protective effect relative to males.\nSeveral race/ethnicity groups (e.g., Non-Hispanic Black, Hispanic subgroups) exhibit increased odds of diabetes compared to Non-Hispanic Whites.\nPosterior credible intervals provide direct probability statements about parameter uncertainty, a key advantage over classical methods."
  },
  {
    "objectID": "slides.html#bayesian-vs-survey-weighted-prevalence",
    "href": "slides.html#bayesian-vs-survey-weighted-prevalence",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Bayesian vs Survey-Weighted Prevalence",
    "text": "Bayesian vs Survey-Weighted Prevalence\nComparison of Prevalence Estimates\n\nInterpretation\n\nThe Bayesian model estimates a mean diabetes prevalence of 10.95%, while the survey-weighted NHANES prevalence is 8.9%.\nThe Bayesian credible interval overlaps with the NHANES estimate, indicating good calibration.\nNHANES lies near the lower range of the posterior distribution but remains fully plausible, demonstrating the model’s population-level validity."
  },
  {
    "objectID": "slides.html#observed-vs-predicted-individual-level-validation",
    "href": "slides.html#observed-vs-predicted-individual-level-validation",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Observed vs Predicted (Individual-Level Validation)",
    "text": "Observed vs Predicted (Individual-Level Validation)\nAverage Diabetes Prediction\n\nInterpretation\n\nEach point represents one participant, showing the relationship between observed diabetes status and the average posterior predicted probability.\nPoints near (0, 0) and (1, 1) reflect strong predictive accuracy for both non-diabetic and diabetic individuals.\nThe clustering along these ideal regions indicates the Bayesian model reliably predicts diabetes risk at the individual level."
  },
  {
    "objectID": "slides.html#model-r²",
    "href": "slides.html#model-r²",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Model R²",
    "text": "Model R²\nBayesian R²\n\nInterpretation\n\nThe Bayesian R² represents the proportion of variation in diabetes status explained by the model while accounting for posterior uncertainty.\nCentered around ~0.13, this value is typical for logistic regression models using population health data.\nThe distribution of R² values reflects modest explanatory power, indicating that while age, BMI, sex, and race are meaningful predictors, diabetes risk is also influenced by unmeasured factors such as genetics, behavior, and environment.\n\n—² \n\nBayesian R² ≈ 0.13"
  },
  {
    "objectID": "slides.html#data-source-1",
    "href": "slides.html#data-source-1",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Data Source",
    "text": "Data Source\n\nNHANES 2013–2014\nComplex multistage probability sampling\nSurvey weights required for national inference"
  },
  {
    "objectID": "slides.html#mice-imputation-1",
    "href": "slides.html#mice-imputation-1",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "MICE Imputation",
    "text": "MICE Imputation\n\nInterpretation: MICE preserves relationships among predictors, reducing complete-case bias and improving model precision."
  },
  {
    "objectID": "slides.html#posterior-odds-ratios-1",
    "href": "slides.html#posterior-odds-ratios-1",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Posterior Odds Ratios",
    "text": "Posterior Odds Ratios\n\nInterpretation: Age and BMI remain strong predictors. Race disparities persist. Female sex slightly protective. Bayesian ORs closely match frequentist results but include fuller uncertainty ranges."
  },
  {
    "objectID": "slides.html#prior-vs-posterior-age-bmi-1",
    "href": "slides.html#prior-vs-posterior-age-bmi-1",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Prior vs Posterior (Age & BMI)",
    "text": "Prior vs Posterior (Age & BMI)\n\nInterpretation: Posterior distributions shrink considerably, demonstrating how the data updates vague priors."
  },
  {
    "objectID": "slides.html#posterior-predictive-checks-1",
    "href": "slides.html#posterior-predictive-checks-1",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Posterior Predictive Checks",
    "text": "Posterior Predictive Checks\n\nInterpretation: Observed diabetes frequencies fall within the replicated model distributions — strong evidence of good model fit."
  },
  {
    "objectID": "slides.html#external-validation-1",
    "href": "slides.html#external-validation-1",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "External Validation",
    "text": "External Validation\n\nInterpretation: Posterior mean prevalence ≈ 10.95% closely aligns with NHANES’ 8.9%, with overlapping intervals → good calibration."
  },
  {
    "objectID": "slides.html#limitations-1",
    "href": "slides.html#limitations-1",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Limitations",
    "text": "Limitations\n\nCross-sectional design\nImputation assumes MAR\nPriors may influence results\nUnmeasured confounding likely"
  },
  {
    "objectID": "slides.html#thank-you-1",
    "href": "slides.html#thank-you-1",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Thank You",
    "text": "Thank You\nAdvisor: Dr. Achraf Cohen\nPresented by: Namita Mishra & Autumn S. Wilcox\nUniversity of West Florida"
  },
  {
    "objectID": "slides.html#introduction-aim",
    "href": "slides.html#introduction-aim",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Introduction & Aim",
    "text": "Introduction & Aim\n\nIntroductionAim\n\n\n\nDiabetes is a major U.S. public health concern.\nKey risk factors: age, BMI, sex, race/ethnicity.\nClassical logistic regression struggles with:\n\nmissing data\n\nquasi-separation\n\nsmall effective sample sizes (ESS)\n\nBayesian logistic regression helps by:\n\nstabilizing estimates\n\nincorporating prior information\n\nquantifying full uncertainty\n\nCommonly applied in diagnostic modeling and NHANES research.\n\n\n\n\nPredict doctor-diagnosed diabetes in U.S. adults (NHANES 2013–2014).\nEvaluate age, BMI, sex, and race/ethnicity as predictors.\nIncorporate NHANES complex survey design.\nHandle missing data using MICE.\nCompare Bayesian vs survey-weighted logistic regression.\nAssess model stability, interpretability, and predictive performance."
  },
  {
    "objectID": "slides.html#frequentist-limitations-motivation",
    "href": "slides.html#frequentist-limitations-motivation",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Frequentist Limitations (Motivation)",
    "text": "Frequentist Limitations (Motivation)\n\nWhy MLE StrugglesMissing DataQuasi-SeparationConfoundingWhy Bayesian Helps\n\n\n\nSensitive to missing data\n\nUnstable when predictors or outcomes are sparse\n\nVulnerable to quasi-separation (infinite or extreme coefficients)\n\nDepends on large-sample assumptions that may not hold in subgroups\n\n\n\n\nMissing BMI/diabetes values reduce effective sample size (ESS)\nComplete-case analysis can introduce bias and increase variance\n\n\n\n\nOccurs when a predictor perfectly (or nearly perfectly) predicts the outcome\n\nCauses non-finite or extremely large MLE estimates\n\n\n\n\nLimited covariates → residual confounding remains\n\nMLE provides only point estimates, not full uncertainty\n\n\n\n\nPriors stabilize unstable coefficients\n\nPosterior distributions quantify full uncertainty\n\nPerforms better with missingness, small samples, and separation"
  },
  {
    "objectID": "slides.html#study-data",
    "href": "slides.html#study-data",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Study Data",
    "text": "Study Data\n\nNHANES FilesMerge RulesAdult Cohort DefinitionWeighted Sample CharacteristicsVariables & CodingSurvey Design VariablesFinal Analytic Sample\n\n\n\nDEMO_H — demographics\n\nBMX_H — BMI & anthropometrics\n\nDIQ_H — diabetes questionnaire\nData from NHANES 2013–2014 (nhanes2013?).\n\n\n\n\nKey ID: SEQN\n\nFiles merged on SEQN with harmonized variable types\n\n\n\n\nAdults ≥ 20 years\n\nFinal analytic sample: 5,769\n\nOutcome: doctor-diagnosed diabetes (DIQ010)\n\n\n\n\nWeighted mean age: ~47.5 yrs\n\nWeighted diabetes prevalence: ~8.9%\n\nDesign effect & effective sample size (ESS) computed\n\n\n\n\nage_c — standardized age\n\nbmi_c — standardized BMI\n\nsex — Male / Female\n\nrace — 5 categories (NH White reference)\nDIQ010 — binary diabetes indicator (0 = No, 1 = Yes)\n\n\n\n\nSDMVPSU — primary sampling units\n\nSDMVSTRA — strata\n\nWTMEC2YR — exam weights\n\n\n\n\nCleaned dataset with design variables retained\n\nReady for:\n\ncomplete-case analysis\n\nMICE imputation\n\nBayesian modeling"
  },
  {
    "objectID": "slides.html#missingness",
    "href": "slides.html#missingness",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Missingness",
    "text": "Missingness\n\nMissingness OverviewKey Missing VariablesMAR DefinitionMissingness Visualization\n\n\n\n~7% overall missingness\n\n92.7% of rows complete\n\nMinimal structural missingness\n\n\n\n\nBMI: ~4.3% missing\n\ndiabetes_dx: ~3.1% missing\n\n\n\n\nMissing At Random (MAR):\nMissingness depends on observed variables\n(e.g., exam attendance), not on the missing value itself"
  },
  {
    "objectID": "slides.html#regression-results-posterior-estimates-and-distribution",
    "href": "slides.html#regression-results-posterior-estimates-and-distribution",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Regression Results (Posterior Estimates and Distribution)",
    "text": "Regression Results (Posterior Estimates and Distribution)\n\nPosterior EstimatesPosterior DistributionInterpretation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPosterior Coefficients\n\nAge (per 1 SD): strong positive effect (≈ +1.1, 80% CrI ≈ 0.9–1.3)\nBMI (per 1 SD): positive association (≈ +0.6, 80% CrI ≈ 0.45–0.75)\nFemale: slightly protective (≈ –0.2, CrI spans slightly below zero)\nMexican American: small positive effect (≈ +0.25, wide CrI)\nOther Hispanic: moderate positive effect (≈ +0.35, wide CrI)\nNon-Hispanic Black: clear positive effect (≈ +0.75, CrI ≈ 0.6–0.9)\nOther/Multi: small positive effect (≈ +0.3, wide CrI)"
  },
  {
    "objectID": "slides.html#compare-prior-and-posterior-predictive-distribution-age-and-bmi",
    "href": "slides.html#compare-prior-and-posterior-predictive-distribution-age-and-bmi",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Compare Prior and Posterior Predictive Distribution (Age and BMI)",
    "text": "Compare Prior and Posterior Predictive Distribution (Age and BMI)\n\nPlotInterpretation\n\n\n\n\n\n\n\n\n\nPrior and posterior distributions for the coefficient estimates of\nAge (per 1 SD) and BMI (per 1 SD):\n\nPrior distribution: wide spread (high variance), indicating high initial uncertainty\n\nPosterior distribution: narrower and taller\n\nData substantially updated the initial beliefs, pulling the estimates into a more precise range"
  },
  {
    "objectID": "slides.html#bayesian-vs-survey-weighted-nhanes-diabetes-prevalence",
    "href": "slides.html#bayesian-vs-survey-weighted-nhanes-diabetes-prevalence",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Bayesian vs Survey-Weighted NHANES (Diabetes Prevalence)",
    "text": "Bayesian vs Survey-Weighted NHANES (Diabetes Prevalence)\n\nPlotInterpretation\n\n\n\n\n\n\n\n\n\n\nModel mean diabetes prevalence: 10.95%\n95% CI: 8.5%–12.8%\n\nNHANES diabetes prevalence: 8.9% (SE = 0.0048)\n\nThe model mean is slightly higher, but its credible interval overlaps with the NHANES estimate, indicating good calibration.\nNHANES lies near the lower end of the posterior distribution, but still well within a plausible range—showing that the Bayesian model aligns closely with the population estimate."
  },
  {
    "objectID": "slides.html#observed-vs-predicted-in-individuals",
    "href": "slides.html#observed-vs-predicted-in-individuals",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Observed vs Predicted in Individuals",
    "text": "Observed vs Predicted in Individuals\nAverage Diabetes\n\n\nEach point = one individual\n\nx-axis: observed value (0 or 1)\n\ny-axis: average predicted posterior probability across simulations\n\nPoints clustering near (0, 0) or (1, 1) indicate good prediction accuracy"
  },
  {
    "objectID": "slides.html#compare-models",
    "href": "slides.html#compare-models",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Compare Models",
    "text": "Compare Models\n\nOverviewInterpretation\n\n\n\nSurvey-Weighted Maximum Likelihood Estimation (MLE)\nBayesian Regression\n\nsvy_tbl   &lt;- svy_or   %&gt;% mutate(Model = \"Survey-weighted MLE\")\nbayes_tbl &lt;- bayes_or %&gt;% mutate(Model = \"Bayesian\")\n\n\n\n\n\n\nBoth models consistently identified Age and BMI as strong predictors of diabetes.\n\nBayesian model: provides stable, interpretable estimates, quantifies uncertainty, and produces population-level prevalence.\n\nSurvey-weighted model: uses normalized weights but does not fully incorporate stratification, clustering, or design-based variance adjustments.\n\n\n\n\n\nMinimal shrinkage and similar uncertainty → both models are reliable and robust\n\nPosterior estimates are slightly pulled toward the prior mean (more moderate values)\n\nThe prior was weakly informative, exerting only a small influence\n\nObserved data dominated the prior, making Bayesian results very similar to survey-weighted estimates"
  },
  {
    "objectID": "slides.html#pairwise-correlation",
    "href": "slides.html#pairwise-correlation",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Pairwise Correlation",
    "text": "Pairwise Correlation\n\nPlotInterpretation\n\n\n\n\n\n\n\n\n\n\nNo strong linear relationships between Age, BMI, and Sex → each contributes independently to predicting diabetes.\n\nDiagonal histograms are smooth and unimodal, indicating stable convergence and well-behaved posterior samples.\n\nMild negative correlation between Age and Sex (Female): as the effect of Age increases, the effect of being female slightly decreases.\n\nOverall, weak correlations show that each predictor provides distinct information for modeling diabetes risk."
  },
  {
    "objectID": "slides.html#implementing-design-based-logistic-regression",
    "href": "slides.html#implementing-design-based-logistic-regression",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Implementing Design-Based Logistic Regression",
    "text": "Implementing Design-Based Logistic Regression\n\nComplete-Case SummaryKey FindingsSurvey-Weighted Coefficient Plot\n\n\n\nComplete-case sample: n = 5,348\n\nSurvey-weighted logistic regression (quasibinomial family)\n\n\n\n\nAge (per 1 SD ↑): ~3× higher odds of diabetes\n\nBMI (per 1 SD ↑): ~1.9× higher odds\n\nFemale: lower odds vs Male\n\nRace: several groups show higher odds vs NH White"
  },
  {
    "objectID": "slides.html#key-findings-1",
    "href": "slides.html#key-findings-1",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Key Findings",
    "text": "Key Findings\n\n\nAge and BMI are the strongest predictors of doctor-diagnosed diabetes across both survey-weighted and Bayesian models.\nBayesian modeling is stable and well-calibrated, avoiding quasi-separation and producing uncertainty estimates similar to the classical model.\nPosterior predictive checks support good fit, accurately reproducing diabetes prevalence and individual-level outcomes.\nConsistent population-level and individual predictions show that the model supports both broad public health insights and personalized risk assessment."
  },
  {
    "objectID": "slides.html#posterior-distribution",
    "href": "slides.html#posterior-distribution",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Posterior Distribution",
    "text": "Posterior Distribution\n\nPosterior DrawsPosterior DistributionInterpretation\n\n\nPosterior draws = 4000 samples (4 chains × 1000 iterations) generated via MCMC, producing the full posterior distribution of the predictors.\n\nRepresents the conditional distribution of parameters given the data\n\nEncodes the entire probability distribution, not just point estimates\n\nMeans and credible intervals summarize this distribution but do not replace it\n\nPosterior shape reflects the influence of the prior (prior subjectivity)\n\nSample size effect: as sample size increases, the prior has less influence on the posterior\n\n\n\n\n\n\n\n\n\n\nPosterior Coefficients\n\nAge (per 1 SD): strong positive effect (≈ +1.1, 80% CrI ≈ 0.9–1.3)\nBMI (per 1 SD): positive association (≈ +0.6, 80% CrI ≈ 0.45–0.75)\nFemale: slightly protective (≈ –0.2, CrI spans slightly below zero)\nMexican American: small positive effect (≈ +0.25, wide CrI)\nOther Hispanic: moderate positive effect (≈ +0.35, wide CrI)\nNon-Hispanic Black: clear positive effect (≈ +0.75, CrI ≈ 0.6–0.9)\nOther/Multi: small positive effect (≈ +0.3, wide CrI)"
  },
  {
    "objectID": "slides.html#compare-prior-and-posterior-distribution-age-and-bmi",
    "href": "slides.html#compare-prior-and-posterior-distribution-age-and-bmi",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk",
    "section": "Compare Prior and Posterior Distribution (Age and BMI)",
    "text": "Compare Prior and Posterior Distribution (Age and BMI)\n\nPlotInterpretation\n\n\n\n\n\n\n\n\n\nPrior and posterior distributions for the coefficient estimates of\nAge (per 1 SD) and BMI (per 1 SD):\n\nPrior distribution: wide spread (high variance), indicating high initial uncertainty\n\nPosterior distribution: narrower and taller\n\nData substantially updated the initial beliefs, pulling the estimates into a more precise range"
  }
]