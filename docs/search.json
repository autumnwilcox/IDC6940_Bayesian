[
  {
    "objectID": "contributions.html",
    "href": "contributions.html",
    "title": "Contributions",
    "section": "",
    "text": "This project was developed collaboratively as part of the Capstone Projects in Data Science course under the guidance of Dr. Ashraf Cohen.\n\nAutumn Wilcox – Contributed to analytic coding, content draft, structured the project workflow, and collaborated actively via GitHub.\nNamita Mishra – Developed the project plan, content draft, analytic coding, and coordinated commits and collaboration with the group on GitHub."
  },
  {
    "objectID": "Summaries/et/paper2.html",
    "href": "Summaries/et/paper2.html",
    "title": "Bayesian parametric models for survival prediction in medical applications",
    "section": "",
    "text": "Bayesian parametric models for survival prediction in medical applications\nIwan Paolucci, Yuan-Mao Lin, Jessica Albuquerque Marques Silva, Kristy K. Brock & Bruno C. Odisio\nBMC Medical Research Methodology volume 23, Article number: 250 (2023)\nThis research article, published in BMC Medical Research Methodology, introduces and evaluates Bayesian parametric survival models for predicting patient outcomes in medical applications. The authors, Paolucci et al., highlight the advantages of Bayesian models, such as their ability to provide uncertainty measures, require less hyperparameter tuning, and offer a natural mechanism for model updating using Bayes’ rule without needing original training data due to privacy concerns. The study compares these Bayesian models against conventional survival prediction methods like Cox Proportional Hazards and Random Survival Forests, demonstrating comparable performance while exhibiting less overfitting. The article details the mathematical background of these models, their implementation, and presents results from experiments on various public medical datasets to support their utility in personalized medicine.\n\nProblem\nSurvival analysis is crucial in medical research for predicting patient outcomes, such as time until death or disease recurrence. Traditional models like Cox Proportional Hazards (CoxPH) and Random Survival Forests (RSF) have limitations, including assumptions about hazard ratios and challenges with interpretability. Bayesian parametric models offer a promising alternative by providing uncertainty measures, requiring less hyperparameter tuning, and allowing for model updates without needing original training data, which is beneficial for privacy concerns.\nA major gap in current predictive models for medical applications is the lack of a measure of uncertainty associated with predictions, which is crucial for physicians making high-stakes clinical decisions, especially when treatments have differing side effect profiles or costs.\nThe study tackles practical and technical challenges inherent in medical machine learning. Medical datasets are often limited in size due to the subclassification of diseases, making models highly prone to overfitting. Many existing machine learning algorithms also require extensive hyperparameter tuning to implement regularization and prevent this overfitting.\n\n\nMethodology\nThe authors introduce Bayesian parametric survival models, which are based on the assumption that survival times follow a specific statistical distribution (e.g., Exponential, Weibull, Lognormal). These models use Bayesian inference to estimate the parameters of the chosen distribution, allowing for the incorporation of prior knowledge and the quantification of uncertainty in predictions.\n\nBayesian Parametric Survival Models (BPS)\n\nImplemented using the PyMC library in Python.\nModels include Exponential and Weibull distributions.\nParameters are estimated using Linear combinations and neural networks to predict the parameters of the distributions.\n\nTraining\n\nPyMC framework\nBayes Rule for updating (posterior as prior)\n\nEvaluation\n\nPublic Datasets: AIDS Clinical Trials Group (ACTG), German Breast Cancer Study (GBCS), Veteran lung cancer (Veteran), Worcester Heart Attack Study (WHAS), and primary biliary cirrhosis (PBC)\nExperiment 1: Comparison of Bayesian models with CoxPH, RSF, and DeepSurv\nExperiment 2: Model updating vs retraining from scratch\nExperiment 3: Impact of dataset size on model performance\nMetrics: Concordance Index (C-index), Integrated Brier Score (IBS)\nComparison with Cox Proportional Hazards, Random Survival Forests, and DeepSurv\n\n\n\n\nResults\nThe results demonstrate that Bayesian parametric survival models perform comparably to traditional methods like CoxPH and RSF, with some advantages in terms of reduced overfitting and the ability to provide uncertainty estimates. The models also showed robustness when updating with new data, maintaining performance without needing to retrain from scratch.\n\nPerformance comparison\n\nBPS models performed well, no consistent best model\nBPS Wb NN superior for PBC dataset\n\nOverfitting\n\nBPS and CoxPH & Weibull based models overfit least\nDeepSurv & RSF showed more overfitting\n\nUncertainty estimation beneficial for clinical decision-making\n\n\n\nConclusion\n\nBPS models are competitive, robust, and well-suited for medical applications.\nKey advantages: less overfitting, uncertainty quantification, reduced hyperparameter tuning.\nEfficient model updating without original data (preserves privacy).\nLimitations: datasets used in the study are relatively small, computation time not compared."
  },
  {
    "objectID": "Summaries/et/paper4.html",
    "href": "Summaries/et/paper4.html",
    "title": "Bayesian Logistic Regression Capstone",
    "section": "",
    "text": "What is BART-Survival?\n\nIt’s a Python software package developed by the CDC.\nThe package is for survival analysis: studying time-to-event data (how long until something happens, or doesn’t happen).\nIt uses a machine learning method called Bayesian Additive Regression Trees (BART).\n\nHow does it work (in simple terms)? - “Discrete-time” means it splits up time into chunks (for example, weeks, months, etc.). The model checks at each time‐point whether the event has happened yet or not. - BART is “non-parametric”, which means the model doesn’t assume a fixed mathematical form (like linear or exponential) for how risk changes over time. It lets the data itself shape the risk patterns more flexibly. - Because it’s Bayesian, it gives not just a prediction but also measures of uncertainty (how confident the model is) about those predictions.\nWhat makes it useful / better in some cases - Traditional survival analysis (like the Cox model) often assumes certain things (e.g. that hazards are proportional over time). If those assumptions are wrong, the models can be misleading. BART-Survival is more flexible and can perform better when assumptions of older methods fail. - It provides a friendly API, and tools to dive deeper into the model (e.g. inspecting results, uncertainty) when needed.\nWhen / where we might use it - When you have data about when events happen (or don’t happen) and want to model that. For example, time until recovery, time until equipment failure, time until some event in public health etc. - When you suspect that the standard assumptions of simpler survival models may not hold (e.g. that risk doesn’t change proportionally over time). - When you want flexible models that can give you both predictions and uncertainty.\n\nReference\n\nSparapani, R. A., Logan, B. R., McCulloch, R., & Laud, P. W. (2020). Nonparametric survival analysis using Bayesian additive regression trees (BART). Statistics in Medicine, 39(20), 2526-2546. https://doi.org/10.1002/sim.8523\nBART-Survival GitHub Repository"
  },
  {
    "objectID": "Summaries/et/paper1.html",
    "href": "Summaries/et/paper1.html",
    "title": "Bayesian Nonparametric Regression for Healthcare Claims Modeling",
    "section": "",
    "text": "Bayesian Nonparametric Regression for Healthcare Claims Modeling\nRichardson, R., & Hartman, B. (2018). Bayesian nonparametric regression models for modeling and predicting healthcare claims. Insurance, Mathematics & Economics, 79, 1-13.\n\n\nSummary\nThis paper introduces Bayesian nonparametric regression models to effectively model and predict healthcare claims data, which often exhibit complex characteristics such as skewness, heavy tails, and excess zeros. The authors propose using Dirichlet process mixtures to flexibly capture the underlying distribution of healthcare claims without assuming a specific parametric form. The models are designed to handle the unique features of healthcare claims data, including the presence of many zero claims (non-claimants) and the variability in claim amounts among claimants.\nThe authors apply their models to a real-world dataset of healthcare claims, demonstrating the models’ ability to accurately predict claim amounts and identify high-risk individuals. The Bayesian framework allows for the incorporation of prior information and provides a coherent way to quantify uncertainty in predictions. The results show that the proposed nonparametric regression models outperform traditional parametric models in terms of predictive accuracy and flexibility.\n\n\nProblem\n\nStandard regression limitations\n\nInsufficient for complex relationships in healthcare claims data\nAssumes Gaussianity, independence, linearity\nAssumptions often not met in insurance data\nDifficulty in capturing skewness, heavy tails, outliers, and bimodality present in claims data\n\n\n\n\nSolution\n\nBayesian nonparametric regression models\n\nFlexible regression model\nRelaxes Normality and linearity assumptions\nPoweful tool for non-Gaussian densities\nIncreased predictive accuracy\nHandles complex error distribution characteristics\nApplicable to all insurance regression problems\n\n\n\n\nMethodology\n\nDependent Dirichlet Process (DDP) ANOVA Model\n\nDirichlet Process (DP) (3.1)\n\nStandard building block for Bayesian nonparametric models\nConstructed via stick-breaking process\nDiscrete distribution with countably infinite atoms\nDP mixture of normals for continuous settings\nAllows for infinite mixture models\n\nDDP (3.2)\n\nBasis for fully nonparametric regression model\nPrior on space of random probability measures\nPoint masses are realizations of stochastic processes\nInfinite mixture of Gaussian processes\n\nDDP ANOVA model (3.3)\n\nExtends DDP to include covariate information\nAtoms are regression coefficients and covariance\nFlexible relationships and error structure\nUses Gibbs sampling for posterior inference\n\n\n\n\n\nApplication\n\nHealthcare claims by ETG\n\nDataset: Episode Treatment Groups (ETG) from a large health insurer\nETG\n\nClassification system for medical conditions\nPredicts healthcare costs\nUncertainty in cost predictions is important\n\nCovariates\n\nage, gender, healthcare charges\n\nFocus on prediction of new observations\n\n\n\n\nResults and performance\n\nOutperforms standard linear model\nOutperforms Generalized Beta 2 (GB2) regression (all but 11 of 347 ETGs)\nBetter predictive accuracy\nDIC (Deviance Information Criterion) consistently lower for BNP\nLower averarge CRPS for out-of-sample predictions\nAccurately captures tail behavior (useful for reserving/risk assessment)\n\n\n\nCase studies\n\nConjunctivitis\n\nLarge dataset (160,000+ observations)\nDistribution is highly non-Gaussian, bimodal\nBNP captures gender effect, standard models fail\nLowest DIC (10,600 vs 12,600 for BLM and 12,300 for GB2)\nAccurately predicts extra mode in left tail\n\nLung transplant\n\nSmaller dataset\nData skewed, Gaussian assumption inappropriate\nBNP/GB2 predict thicker tail for outliers\nGB2 slightly better DIC (1002 vs 1005 for BNP, and 1080 for BLM)\nLess advantage from flexible models in smaller datasets\n\n\n\n\nLimitations\n\nBNP more computationally intensive than standard linear regression\nBut BNP regression scales at the same reate as linear regression\n\n\n\nConclusion\n\nPowerful tool for healthcare claims modeling\nVastly outperforms linear and GB2 regression\nUseful for reserving due to improved distributional fit\nDDP ANOVA useful for continuous independent variables\nCan be extended(e.g. covariate-dependent weights)\nAvailable in R package DPpackage"
  },
  {
    "objectID": "Summaries/nm/My articles.html",
    "href": "Summaries/nm/My articles.html",
    "title": "Bayesian Logistic Regression Capstone",
    "section": "",
    "text": "Namita’s Literature"
  },
  {
    "objectID": "Summaries/nm/My articles.html#introduction",
    "href": "Summaries/nm/My articles.html#introduction",
    "title": "Bayesian Logistic Regression Capstone",
    "section": "Introduction",
    "text": "Introduction\n\nBayesian Hierarchical Model (Disease reclassification and prediction)\n\nWhat is the goal of the paper?\nThe authors develop a Bayesian hierarchical model for multivariate longitudinal data to predict health status, trajectories, and intervention effects at the individual level in the PCORI mission to address questions about health status from patients and clinicians.\nWhy is it important?\nHealthcare data (DNA sequences, functional images of the brain, patient-reported outcomes, and electronic health records with patients’ sequences of health measurements, diagnoses, and treatments) are complex, and the standard approaches are not adequate for clinical data analysis. Electronic health records (EHRs) could improve diagnostic accuracy and predict treatment effects. Visualizations of characteristics of posterior distributions can be immediately understood by clinicians and patients as relevant to their decision. Combining prior knowledge and patient data with evidence could predict the patient’s health status, trajectory, and/or likely benefits of interventions.\nHow is it solved?\nMethod: The authors applied Bayesian hierarchical regression for multivariate longitudinal patient data using open-source R-packages and developed 2 levels—time within person and persons within a population\nThe model combined exogenous (eg, age, clinical history) factors and endogenous (eg, current treatment) variables on the individual’s multivariate health measurements and the effects of health measurements at one time on subsequent interventions.\nThe model produced an estimate of the posterior distribution for each value of the predictor variable and an estimate of the marginal distribution of the regression coefficients for each coefficient that measures the outcome (health status) associated with its predictor variables. In a larger sample, the likelihood dominates the prior distribution for regression coefficients and Bayesian hierarchical model used a likelihood-based approach, used priors (prior laboratory and clinical trials data) that provided the assay sensitivities, which through the prior assumptions, made the model identifiable and the integration of Markov chain Monte Carlo (MCMC) estimates the posterior distributions, avoided missing data and complex outcome measurements.\nResults\nThree case studies: pneumonia etiology in children, prostate cancer, and mental disorders chosen for model development, identified low-risk patient population, reduced the risk of overtreatment, complications, adverse effects, and financial burden for patients (Disease Reclassification). Prostate cancer software was then implemented within the JHM HER.\nLimitation:\nModels were entirely parametric, and extensions to nonparametric or more flexible parametric models were recommended to improve approaches for neuroimage or genomic data.\nneuroimage or genomic data.\nApplications:\n\nto scale a tool that addresses a particular unmet need across a larger, more diverse population of patients and clinicians\nuse in autoimmune diseases, sudden cardiac arrest, and diabetes.\nembed a collection of tools to acquire and use the most relevant information, agnostic to its level of measurement, to improve population and individual health decisions that cause better outcomes at more affordable costs. @Zeger2020\n\n\nBayesian Inference (parametric vs non-parametric)\n\nWhat is the goal of the paper?\nThe authors calculated the posterior probability of disease diagnosis and applied Bayesian inference to develop three modules comparing parametric (with a fixed set of parameters) and nonparametric distributions (which do not make a priori assumptions) by analyzing the National Health and Nutrition Examination Survey dataset from two separate diagnostic tests on both diseased and non-diseased populations.\nWhy is it important?\nMedical diagnosis, treatment, and management decisions are crucial, and conventional methods for diagnosis using clinical criteria and fixed numerical thresholds limit the capture of other information related to the intricate relationship between diagnostic tests and the varying prevalence of diseases. The probability distributions associated with quantitative diagnostic test outcomes often demonstrate some overlap between the diseased and nondiseased groups. The dichotomous method fails to capture the complexity and heterogeneity of disease presentations across diverse populations. The applicability of the normal distribution (conventional method) is critiqued, especially in dealing with clinical measurands having skewness, bimodality, or multimodality.\nHow is it solved?\nMethods: The Authors developed models employing Bayesian inference (Bayesian diagnostic approach) to calculate the posterior probability of disease diagnosis in the Wolfram Language and integrated prior probabilities of disease with distributions of diagnostic measurands in both diseased and nondiseased populations. The approach enabled the evaluation of combined data from multiple diagnostic tests and improved the diagnostic accuracy, precision and adaptability, The model showed flexibility, adaptability, and versatility in the diagnostic.\nResults\nNonparametric Bayesian models tend to fit data distributions better, especially given limited existing literature, and are more robust in capturing complex data patterns.\nThese models produce multimodal probability patterns for disease, unlike the bimodal, double-sigmoidal curves seen with parametric models.\nLimitations\n\nReliance on parametric models: A need to extend to nonparametric or more flexible parametric models for medical data.\nLimited scholarly publications and over-dependence on prior probabilities increase uncertainties, resulting in broader confidence intervals for posterior probabilities. Systemic bias (unrepresentative datasets) compromises the accuracy of Bayesian calculations. For Incomplete datasets, Bayesian methods combined with other statistical and computational techniques could enhance diagnostic capabilities.\nThe foundational data is crucial to compare new diagnostic measurements. Absence of normative data compromises the reliability and validity of Bayesian diagnostic methods. @Chatzimichail2023\n\n\nBayesian model stages, development and advantages, and temporal models\nWhat is the goal of the paper?\n\nThe study describes the stages of Bayesian analysis, specifying the importance of the priors, data modeling, inferences, model checking and refinement, selecting a proper sampling technique from a posterior distribution, variational inferences, variable selection and its application across various research fields. The study proposes strategies for reproducibility and reporting standards, outlining an updated WAMBS (when to Worry and how to Avoid the Misuse of Bayesian Statistics) checklist and outlining the impact of Bayesian analysis on artificial intelligence in the future.\nWhy is it important?\nBayesian statistics is suitable for quantitative researchers accross different fields who have at least some knowledge of regression modelling.\nHow is it solved?\nExamples of successful applications of Bayesian analysis across various research fields (social sciences, ecology, genetics, medicine) and the advantages and disadvantages of the Bayesian model are provided here, and overview of the current and future use of Bayesian statistics.\nThe study mention priors into three categories (informative, weakly informative and diffuse) based on the degree of (un)certainty (hyperparameters) surrounding the population parameter. The prior distribution is as - N( μ0 , σ^ 20) where a larger variance represents a greater amount of uncertainty surrounding.\nPrior elicitation (experts, generic expert, data-based, sample data using maximum likelihood or sample statistics, etc) construct a prior distribution.\nPrior sensitivity analysis of the likelihood helps examine different forms of the model, assesses how the priors and the likelihood align and have an impact on posterior estimates, reflecting variations not captured by the prior or the likelihood alone.\nPrior estimation allows data-informed shrinkage, enacts regularization or influence algorithms towards a likely high-density region, and improves estimation efficiency.\nKnowing the exact probabilistic specification of the priors for a complex model with smaller sample sizes is important. A small sample conveys less information compared to the priors that quantify the strength of support the observed data lends to possible value(s) for the unknown parameter(s).\nFrequentists do not consider the probability of the unknown parameters as useful, and they are considered to be fixed; the likelihood is the conditional probability distribution p(y|θ) of the data (y), given fixed parameters (θ). In Bayesian inference, unknown parameters (random variables) have varied values, while the (observed) data have fixed values, and the likelihood is a function of θ for the fixed data y.\nTherefore, the likelihood function summarizes a statistical model that stochastically generates a range of possible values for θ and the observed data y. With priors and the likelihood of the observed data, the resulting posterior distribution provides an estimate of the unknown parameters, capturing the primary factors and improving our understanding. Monte Carlo technique provides integrals of sampled values from a given distribution through computer simulations. The packages BRMS and Blavaan in R are used for the probabilistic programming language Stan.\nVariable selection after checking correlations among the variables in the model (Eg: gene-to-gene interaction) aids in the prediction of genes in biomedical research (genome-wide association studies).\nSpatial and temporal variability are factored in Bayesian general linear models. A posterior distribution can simulate new data conditional on this distribution, assess, and provide valid predictions to be used for extrapolating to future events.\nResults\nThe Bayesian approach analyzes large-scale cancer genomic data, identifies novel molecular changes in cancer initiation and progression, the interactions between mutated genes and captured mutational signatures, highlighting key genetic interactions components, allowing genomic-based patient stratification both in clinical trials, in the personalized use of therapeutics, and in understanding cancer and its evolutionary processes.\nLimitations:\n\nIn temporal models, posterior inference challenges are inherent in the spatial and/or temporal dependencies (autocorrelation of parameters over time). @VandeSchoot2021\n\n\nBayesian Normal linear regression\n\nWhat is the goal of the paper?\nThe author provides guidance on Bayesian inference by performing Bayesian Normal linear regression in metrology to calibrate instruments and to evaluate inter-laboratory comparisons in determining fundamental constants.\nWhy is it important?\nThe measurement errors are assumed to be additive, independent, and identically distributed according to a Gaussian distribution with mean zero and variance σ2, which is usually unknown.\nRegression is used to calibrate instruments, evaluate inter-laboratory comparisons, or determine fundamental constants, but the regression model cannot be uniquely formulated as a measurement function. Guide to the Expression of Uncertainty in Measurement (GUM) and its supplements are not applicable directly.\nHow is it solved?\nMethods: Bayesian inference has the advantage of accounting for additional a priori information, which robustifies the analyses.\nThree steps (prior elicitation, posterior calculation, and robustness to prior uncertainty and model adequacy) and model assumptions are critical to Bayesian inference.\nIn Bayesian inference, all unknowns—observables (data) as well as unobservables (parameters and auxiliary variables) are considered to be random, are assigned probability distributions to summarize the available information, and to update prior knowledge about the unobservables with information about them contained in the data. The prior distribution and likelihood function provided by simple graphical displays, sensitivity analyses, or model checking enhance the elicitation and interpretation process.\nFor Normal linear regression problems\n(1) a family of prior distributions for θ and σ2 is (Normal inverse Gamma (NIG) distribution to a posterior from the same family of (NIG) distributions or\n(2) alternative families of prior distributions (hierarchical priors) assign an additional layer of distributions to uncertain prior parameters or non-para- metric priors.\nThe NIG prior with known variance σ2 of observations is a conjugate prior distribution. Vague or non-informative prior distributions can be derived from the NIG prior.\nBayesian inference is influenced by\n\nthe uncertainty in the transformation of prior knowledge to prior distributions\nthe assumptions of the statistical model\nthe mistakes in data acquisition\n\nResults\nThe knowledge from related previous experiments (Normal inverse Gamma distributions) allow for analytic posterior calculations of many quantities of interest. @Klauenberg2015\n\nBayesian linear regression and priors (exchangeable and unexchangeable)\n\nWhat is the goal of the paper?\nThe study developed a test of a formal method for augmenting data in linear regression analyses, by incorporating both exchangeable and unexchangeable information on regression coefficients (and standard errors) of previous studies.\nWhy is it important?\nThe frequent combination of multiple testing has relatively low statistical power, which is problematic in null-hypothesis significance testing. Linear regression analyses do not account for the published results and summary statistics from similar previous studies. Ignoring information on parameters from previous studies (relevant and readily available), affects the stability and precision of the parameter estimates and results in lower values than they could have been, resulting in conclusions that are less certain and are affected by sampling variation.\nMultiple linear regression with separate significance tests for all regression coefficients, and with the modest sample sizes, different studies find different sets of statistically significant predictors, and addressing the issue on larger samples is practically unrealistic.\nHow is it solved?\nMethods: Bayesian linear regression accommodates prior knowledge, overcoming the absence of formal studies, handles the issues of increasing the sample size, and augments the data of a new study with previous results (regression coefficients and standard errors) from similar studies.\nThe authors used Bayesian linear regression to solve the issue of the univariate case analysis by combining evidence of specific predictors from different linear regression analyses (as in meta-analysis) and found it a better method to resolve the issue of simultaneously combining multiple regression parameters per study, which ignored the relationship between the regression coefficients. Includes summary statistics from previous studies, Bayesian linear regression provided a more acceptable solution when data from previous studies were not (realistically) obtainable.\nModels in the study as categorized the model into - (1) Exchangable - when the current data and previous studies have the same set of predictors. (2) Unexchangable – when the predictors were different in the two.\nTo yield the posterior density that reflects the updated knowledge about the model parameters after having observed the data, the steps to Bayesian linear regression steps are mentioned -\n(1) To calculate the probability density function for the data given the unknown model parameters;\n(2) Taken as a function of those model parameters, the likelihood function is the second part of the prior density function of the model parameters. It quantifies what is assumed to be known about the model parameters before observing the data. The study applied the Standard multiple linear regression model, and with the integration of the prior, provided the joint posterior density using the Gibbs sampler. An Ordinary Least Squares linear regression was then applied to each of these samples to obtain the estimated regression coefficients B and the corresponding standard errors.\n(3) A hierarchical model version was developed in analyzing parameters where studies are not exchangeable.\nResults\nIncorporating priors from previous studies in a linear regression on new data yielded a significantly better parameter estimate with an adequate approximation.\nThe gains in comparison to using just the new data, and the large effects were obtained when the data from previous studies were available, resulting in encouraging performance.\nPerformance of the two versions (exchangeable and unexchangeable) of the replication model was consistently superior to using current data alone.\nThe model developed in the article offers the possibility of obtaining significantly better parameter estimates in a linear regression setting without needing to expend a prohibitive amount of time and energy to obtain data from the previous studies.\nHierarchical version (unexchangeable) of the model offers the advantage of being able to address questions about differences between studies and thus allows for explicit testing of the exchangeability assumption.\nLimitations:\n\nAll studies need to have the same set of predictors.\nThe issue of predictor variables that are correlated. @DeLeeuw2012a\n\n\nBayesian logistic regression (Sequential clinical reasoning approach)\n\nWhat is the goal of the paper?\nThe study aimed to develop models using a longitudinal prospective cohort to predict the risk of incident cardiovascular disease by incorporating demographic features (basic), six metabolic syndrome components (metabolic score), and conventional risk factors (enhanced model). The study participants free of CVD at baseline were followed up over five years, and a Bayesian clinical reasoning model was applied to diagnose new CVD cases.\nWhy is it important?\nEarly diagnosis, prevention, by identifying subjects under the high-risk category for cardiovascular disease (CVD), impacts health interventions.\nLimited availability of molecular information in clinical practice due to being costly and unavailability affects efficient disease diagnosis. To efficiently identify a high-risk population based on the routinely checked biological markers before doing these expensive molecular testsThe requires an alternative approach to analyze data.\nThe Tailored Framingham Risk Score method, for the purpose, is not sufficient because of the differences present in ethnic groups, location, and socio-economic status, as they require the construction of their own models. Heterogeneity (geographic, ethnic group, variations, and different characteristics of social contextual network) often is unobservable and unmeasurable.\nHow is it solved?\nMethods: The study evaluated subjects enrolled in a Keelung Community-based Integrated Screening (KCIS) Program, for mass screening (20–79 years) in the Keelung city of Taiwan, who were followed for 5 years to identify incident cancers and chronic diseases (cardiovascular disease).\nThe study classified the risk of having incident CVD cases or death from CVD by dint of available calculated standardized risk score of the MetS components (fasting glucose, blood pressure, HDL-C, triglyceride and waist circumference) together with conventional risk factors (gender, heredity, smoking, alcohol drinking, family history of parent’s CVD and betel quid and other relevant factors).\nEmulating a clinician’s evaluation process, the Bayesian clinical reasoning approach in a sequential manner was applied and three models were developed. The approach considered the normal distribution of regression coefficients of all predictors, allowing for uncertainty of clinical weights. The credible intervals of predicted risk estimates were obtained by averaging out. In the model, the individual risk is elicited by prior speculation (first impression) that is updated by objective observed data (patient’s history and laboratory findings), the regression coefficients for computing risk score were treated as random variable with a certain statistical distribution (e.g. normal distribution) rather than a fixed value (traditional risk prediction model by frequentist). The updated prior distribution with the likelihood of the current data provided a posterior distribution to predict the risk for a specific disease. The sequential approach included -\n\nThe (basic) basic model developed via logistic regression used prior information constructed on gender, age, age2, and time period.\nThe second (classical) model (metabolic score model: MS model) was based on six MetS components.\nThe third (enhanced model) incorporated information on smoking, drinking, betel-quid, and family history of CVD.\n\nResults\nCompared to the basic model and classical model, the enhanced model had better performance. The proposed models predicted CVD incidence at the individual level by incorporating routine information with a sequential Bayesian clinical reasoning approach. Patients’ background significantly contributes to baseline risk. Even with ecological heterogeneity, the regression model adopts individual characteristics and makes individual risk prediction for the CVD incidence.\nLimitations:\n\nWhether the interactions between age, gender, metabolic score, and other risk factors should be included.\nThe use of an enhanced model should be validated through external validation by applying the proposed models to new subjects not included in the training of the model parameters. @Liu2013"
  },
  {
    "objectID": "Summaries/nm/DATA.html",
    "href": "Summaries/nm/DATA.html",
    "title": "Bayesian Logistic Regression Capstone",
    "section": "",
    "text": "Data Source: NHANES merging the below files make 10175 sample size but have to clean it so maybe we will reduce the sample size 1. DEMO_H.xpt 2. DSQTOT_H.xpt 3. BMX_H.xpt 4. TCHOL_H.xpt 5. CDQ_H.xpt 6. SMQ_H.xpt 7. MCQ_H.xpt"
  },
  {
    "objectID": "Summaries/nm/My_articles.html",
    "href": "Summaries/nm/My_articles.html",
    "title": "Bayesian Logistic Regression Capstone",
    "section": "",
    "text": "Namita’s Literature"
  },
  {
    "objectID": "Summaries/nm/My_articles.html#introduction",
    "href": "Summaries/nm/My_articles.html#introduction",
    "title": "Bayesian Logistic Regression Capstone",
    "section": "Introduction",
    "text": "Introduction\n\nBayesian Hierarchical Model (Disease reclassification and prediction)\n\nWhat is the goal of the paper?\nThe authors develop a Bayesian hierarchical model for multivariate longitudinal data to predict health status, trajectories, and intervention effects at the individual level in the PCORI mission to address questions about health status from patients and clinicians.\nWhy is it important?\nHealthcare data (DNA sequences, functional images of the brain, patient-reported outcomes, and electronic health records with patients’ sequences of health measurements, diagnoses, and treatments) are complex, and the standard approaches are not adequate for clinical data analysis. Electronic health records (EHRs) could improve diagnostic accuracy and predict treatment effects. Visualizations of characteristics of posterior distributions can be immediately understood by clinicians and patients as relevant to their decision. Combining prior knowledge and patient data with evidence could predict the patient’s health status, trajectory, and/or likely benefits of interventions.\nHow is it solved?\nMethod: The authors applied Bayesian hierarchical regression for multivariate longitudinal patient data using open-source R-packages and developed 2 levels—time within person and persons within a population\nThe model combined exogenous (eg, age, clinical history) factors and endogenous (eg, current treatment) variables on the individual’s multivariate health measurements and the effects of health measurements at one time on subsequent interventions.\nThe model produced an estimate of the posterior distribution for each value of the predictor variable and an estimate of the marginal distribution of the regression coefficients for each coefficient that measures the outcome (health status) associated with its predictor variables. In a larger sample, the likelihood dominates the prior distribution for regression coefficients and Bayesian hierarchical model used a likelihood-based approach, used priors (prior laboratory and clinical trials data) that provided the assay sensitivities, which through the prior assumptions, made the model identifiable and the integration of Markov chain Monte Carlo (MCMC) estimates the posterior distributions, avoided missing data and complex outcome measurements.\nResults\nThree case studies: pneumonia etiology in children, prostate cancer, and mental disorders chosen for model development, identified low-risk patient population, reduced the risk of overtreatment, complications, adverse effects, and financial burden for patients (Disease Reclassification). Prostate cancer software was then implemented within the JHM HER.\nLimitation:\nModels were entirely parametric, and extensions to nonparametric or more flexible parametric models were recommended to improve approaches for neuroimage or genomic data.\nneuroimage or genomic data.\nApplications:\n\nto scale a tool that addresses a particular unmet need across a larger, more diverse population of patients and clinicians\nuse in autoimmune diseases, sudden cardiac arrest, and diabetes.\nembed a collection of tools to acquire and use the most relevant information, agnostic to its level of measurement, to improve population and individual health decisions that cause better outcomes at more affordable costs. @Zeger2020\n\n\nBayesian Inference (parametric vs non-parametric)\n\nWhat is the goal of the paper?\nThe authors calculated the posterior probability of disease diagnosis and applied Bayesian inference to develop three modules comparing parametric (with a fixed set of parameters) and nonparametric distributions (which do not make a priori assumptions) by analyzing the National Health and Nutrition Examination Survey dataset from two separate diagnostic tests on both diseased and non-diseased populations.\nWhy is it important?\nMedical diagnosis, treatment, and management decisions are crucial, and conventional methods for diagnosis using clinical criteria and fixed numerical thresholds limit the capture of other information related to the intricate relationship between diagnostic tests and the varying prevalence of diseases. The probability distributions associated with quantitative diagnostic test outcomes often demonstrate some overlap between the diseased and nondiseased groups. The dichotomous method fails to capture the complexity and heterogeneity of disease presentations across diverse populations. The applicability of the normal distribution (conventional method) is critiqued, especially in dealing with clinical measurands having skewness, bimodality, or multimodality.\nHow is it solved?\nMethods: The Authors developed models employing Bayesian inference (Bayesian diagnostic approach) to calculate the posterior probability of disease diagnosis in the Wolfram Language and integrated prior probabilities of disease with distributions of diagnostic measurands in both diseased and nondiseased populations. The approach enabled the evaluation of combined data from multiple diagnostic tests and improved the diagnostic accuracy, precision and adaptability, The model showed flexibility, adaptability, and versatility in the diagnostic.\nResults\nNonparametric Bayesian models tend to fit data distributions better, especially given limited existing literature, and are more robust in capturing complex data patterns.\nThese models produce multimodal probability patterns for disease, unlike the bimodal, double-sigmoidal curves seen with parametric models.\nLimitations\n\nReliance on parametric models: A need to extend to nonparametric or more flexible parametric models for medical data.\nLimited scholarly publications and over-dependence on prior probabilities increase uncertainties, resulting in broader confidence intervals for posterior probabilities. Systemic bias (unrepresentative datasets) compromises the accuracy of Bayesian calculations. For Incomplete datasets, Bayesian methods combined with other statistical and computational techniques could enhance diagnostic capabilities.\nThe foundational data is crucial to compare new diagnostic measurements. Absence of normative data compromises the reliability and validity of Bayesian diagnostic methods. @Chatzimichail2023\n\n\nBayesian model stages, development and advantages, and temporal models\nWhat is the goal of the paper?\n\nThe study describes the stages of Bayesian analysis, specifying the importance of the priors, data modeling, inferences, model checking and refinement, selecting a proper sampling technique from a posterior distribution, variational inferences, variable selection and its application across various research fields. The study proposes strategies for reproducibility and reporting standards, outlining an updated WAMBS (when to Worry and how to Avoid the Misuse of Bayesian Statistics) checklist and outlining the impact of Bayesian analysis on artificial intelligence in the future.\nWhy is it important?\nBayesian statistics is suitable for quantitative researchers accross different fields who have at least some knowledge of regression modelling.\nHow is it solved?\nExamples of successful applications of Bayesian analysis across various research fields (social sciences, ecology, genetics, medicine) and the advantages and disadvantages of the Bayesian model are provided here, and overview of the current and future use of Bayesian statistics.\nThe study mention priors into three categories (informative, weakly informative and diffuse) based on the degree of (un)certainty (hyperparameters) surrounding the population parameter. The prior distribution is as - N( μ0 , σ^ 20) where a larger variance represents a greater amount of uncertainty surrounding.\nPrior elicitation (experts, generic expert, data-based, sample data using maximum likelihood or sample statistics, etc) construct a prior distribution.\nPrior sensitivity analysis of the likelihood helps examine different forms of the model, assesses how the priors and the likelihood align and have an impact on posterior estimates, reflecting variations not captured by the prior or the likelihood alone.\nPrior estimation allows data-informed shrinkage, enacts regularization or influence algorithms towards a likely high-density region, and improves estimation efficiency.\nKnowing the exact probabilistic specification of the priors for a complex model with smaller sample sizes is important. A small sample conveys less information compared to the priors that quantify the strength of support the observed data lends to possible value(s) for the unknown parameter(s).\nFrequentists do not consider the probability of the unknown parameters as useful, and they are considered to be fixed; the likelihood is the conditional probability distribution p(y|θ) of the data (y), given fixed parameters (θ). In Bayesian inference, unknown parameters (random variables) have varied values, while the (observed) data have fixed values, and the likelihood is a function of θ for the fixed data y.\nTherefore, the likelihood function summarizes a statistical model that stochastically generates a range of possible values for θ and the observed data y. With priors and the likelihood of the observed data, the resulting posterior distribution provides an estimate of the unknown parameters, capturing the primary factors and improving our understanding. Monte Carlo technique provides integrals of sampled values from a given distribution through computer simulations. The packages BRMS and Blavaan in R are used for the probabilistic programming language Stan.\nVariable selection after checking correlations among the variables in the model (Eg: gene-to-gene interaction) aids in the prediction of genes in biomedical research (genome-wide association studies).\nSpatial and temporal variability are factored in Bayesian general linear models. A posterior distribution can simulate new data conditional on this distribution, assess, and provide valid predictions to be used for extrapolating to future events.\nResults\nThe Bayesian approach analyzes large-scale cancer genomic data, identifies novel molecular changes in cancer initiation and progression, the interactions between mutated genes and captured mutational signatures, highlighting key genetic interactions components, allowing genomic-based patient stratification both in clinical trials, in the personalized use of therapeutics, and in understanding cancer and its evolutionary processes.\nLimitations:\n\nIn temporal models, posterior inference challenges are inherent in the spatial and/or temporal dependencies (autocorrelation of parameters over time). @VandeSchoot2021\n\n\nBayesian Normal linear regression\n\nWhat is the goal of the paper?\nThe author provides guidance on Bayesian inference by performing Bayesian Normal linear regression in metrology to calibrate instruments and to evaluate inter-laboratory comparisons in determining fundamental constants.\nWhy is it important?\nThe measurement errors are assumed to be additive, independent, and identically distributed according to a Gaussian distribution with mean zero and variance σ2, which is usually unknown.\nRegression is used to calibrate instruments, evaluate inter-laboratory comparisons, or determine fundamental constants, but the regression model cannot be uniquely formulated as a measurement function. Guide to the Expression of Uncertainty in Measurement (GUM) and its supplements are not applicable directly.\nHow is it solved?\nMethods: Bayesian inference has the advantage of accounting for additional a priori information, which robustifies the analyses.\nThree steps (prior elicitation, posterior calculation, and robustness to prior uncertainty and model adequacy) and model assumptions are critical to Bayesian inference.\nIn Bayesian inference, all unknowns—observables (data) as well as unobservables (parameters and auxiliary variables) are considered to be random, are assigned probability distributions to summarize the available information, and to update prior knowledge about the unobservables with information about them contained in the data. The prior distribution and likelihood function provided by simple graphical displays, sensitivity analyses, or model checking enhance the elicitation and interpretation process.\nFor Normal linear regression problems\n(1) a family of prior distributions for θ and σ2 is (Normal inverse Gamma (NIG) distribution to a posterior from the same family of (NIG) distributions or\n(2) alternative families of prior distributions (hierarchical priors) assign an additional layer of distributions to uncertain prior parameters or non-para- metric priors.\nThe NIG prior with known variance σ2 of observations is a conjugate prior distribution. Vague or non-informative prior distributions can be derived from the NIG prior.\nBayesian inference is influenced by\n\nthe uncertainty in the transformation of prior knowledge to prior distributions\nthe assumptions of the statistical model\nthe mistakes in data acquisition\n\nResults\nThe knowledge from related previous experiments (Normal inverse Gamma distributions) allow for analytic posterior calculations of many quantities of interest. @Klauenberg2015\n\nBayesian linear regression and priors (exchangeable and unexchangeable)\n\nWhat is the goal of the paper?\nThe study developed a test of a formal method for augmenting data in linear regression analyses, by incorporating both exchangeable and unexchangeable information on regression coefficients (and standard errors) of previous studies.\nWhy is it important?\nThe frequent combination of multiple testing has relatively low statistical power, which is problematic in null-hypothesis significance testing. Linear regression analyses do not account for the published results and summary statistics from similar previous studies. Ignoring information on parameters from previous studies (relevant and readily available), affects the stability and precision of the parameter estimates and results in lower values than they could have been, resulting in conclusions that are less certain and are affected by sampling variation.\nMultiple linear regression with separate significance tests for all regression coefficients, and with the modest sample sizes, different studies find different sets of statistically significant predictors, and addressing the issue on larger samples is practically unrealistic.\nHow is it solved?\nMethods: Bayesian linear regression accommodates prior knowledge, overcoming the absence of formal studies, handles the issues of increasing the sample size, and augments the data of a new study with previous results (regression coefficients and standard errors) from similar studies.\nThe authors used Bayesian linear regression to solve the issue of the univariate case analysis by combining evidence of specific predictors from different linear regression analyses (as in meta-analysis) and found it a better method to resolve the issue of simultaneously combining multiple regression parameters per study, which ignored the relationship between the regression coefficients. Includes summary statistics from previous studies, Bayesian linear regression provided a more acceptable solution when data from previous studies were not (realistically) obtainable.\nModels in the study as categorized the model into - (1) Exchangable - when the current data and previous studies have the same set of predictors. (2) Unexchangable – when the predictors were different in the two.\nTo yield the posterior density that reflects the updated knowledge about the model parameters after having observed the data, the steps to Bayesian linear regression steps are mentioned -\n(1) To calculate the probability density function for the data given the unknown model parameters;\n(2) Taken as a function of those model parameters, the likelihood function is the second part of the prior density function of the model parameters. It quantifies what is assumed to be known about the model parameters before observing the data. The study applied the Standard multiple linear regression model, and with the integration of the prior, provided the joint posterior density using the Gibbs sampler. An Ordinary Least Squares linear regression was then applied to each of these samples to obtain the estimated regression coefficients B and the corresponding standard errors.\n(3) A hierarchical model version was developed in analyzing parameters where studies are not exchangeable.\nResults\nIncorporating priors from previous studies in a linear regression on new data yielded a significantly better parameter estimate with an adequate approximation.\nThe gains in comparison to using just the new data, and the large effects were obtained when the data from previous studies were available, resulting in encouraging performance.\nPerformance of the two versions (exchangeable and unexchangeable) of the replication model was consistently superior to using current data alone.\nThe model developed in the article offers the possibility of obtaining significantly better parameter estimates in a linear regression setting without needing to expend a prohibitive amount of time and energy to obtain data from the previous studies.\nHierarchical version (unexchangeable) of the model offers the advantage of being able to address questions about differences between studies and thus allows for explicit testing of the exchangeability assumption.\nLimitations:\n\nAll studies need to have the same set of predictors.\nThe issue of predictor variables that are correlated. @DeLeeuw2012a\n\n\nBayesian logistic regression (Sequential clinical reasoning approach)\n\nWhat is the goal of the paper?\nThe study aimed to develop models using a longitudinal prospective cohort to predict the risk of incident cardiovascular disease by incorporating demographic features (basic), six metabolic syndrome components (metabolic score), and conventional risk factors (enhanced model). The study participants free of CVD at baseline were followed up over five years, and a Bayesian clinical reasoning model was applied to diagnose new CVD cases.\nWhy is it important?\nEarly diagnosis, prevention, by identifying subjects under the high-risk category for cardiovascular disease (CVD), impacts health interventions.\nLimited availability of molecular information in clinical practice due to being costly and unavailability affects efficient disease diagnosis. To efficiently identify a high-risk population based on the routinely checked biological markers before doing these expensive molecular testsThe requires an alternative approach to analyze data.\nThe Tailored Framingham Risk Score method, for the purpose, is not sufficient because of the differences present in ethnic groups, location, and socio-economic status, as they require the construction of their own models. Heterogeneity (geographic, ethnic group, variations, and different characteristics of social contextual network) often is unobservable and unmeasurable.\nHow is it solved?\nMethods: The study evaluated subjects enrolled in a Keelung Community-based Integrated Screening (KCIS) Program, for mass screening (20–79 years) in the Keelung city of Taiwan, who were followed for 5 years to identify incident cancers and chronic diseases (cardiovascular disease).\nThe study classified the risk of having incident CVD cases or death from CVD by dint of available calculated standardized risk score of the MetS components (fasting glucose, blood pressure, HDL-C, triglyceride and waist circumference) together with conventional risk factors (gender, heredity, smoking, alcohol drinking, family history of parent’s CVD and betel quid and other relevant factors).\nEmulating a clinician’s evaluation process, the Bayesian clinical reasoning approach in a sequential manner was applied and three models were developed. The approach considered the normal distribution of regression coefficients of all predictors, allowing for uncertainty of clinical weights. The credible intervals of predicted risk estimates were obtained by averaging out. In the model, the individual risk is elicited by prior speculation (first impression) that is updated by objective observed data (patient’s history and laboratory findings), the regression coefficients for computing risk score were treated as random variable with a certain statistical distribution (e.g. normal distribution) rather than a fixed value (traditional risk prediction model by frequentist). The updated prior distribution with the likelihood of the current data provided a posterior distribution to predict the risk for a specific disease. The sequential approach included -\n\nThe (basic) basic model developed via logistic regression used prior information constructed on gender, age, age2, and time period.\nThe second (classical) model (metabolic score model: MS model) was based on six MetS components.\nThe third (enhanced model) incorporated information on smoking, drinking, betel-quid, and family history of CVD.\n\nResults\nCompared to the basic model and classical model, the enhanced model had better performance. The proposed models predicted CVD incidence at the individual level by incorporating routine information with a sequential Bayesian clinical reasoning approach. Patients’ background significantly contributes to baseline risk. Even with ecological heterogeneity, the regression model adopts individual characteristics and makes individual risk prediction for the CVD incidence.\nLimitations:\n\nWhether the interactions between age, gender, metabolic score, and other risk factors should be included.\nThe use of an enhanced model should be validated through external validation by applying the proposed models to new subjects not included in the training of the model parameters. @Liu2013"
  },
  {
    "objectID": "Summaries/aw/paper5_summary.html",
    "href": "Summaries/aw/paper5_summary.html",
    "title": "Summary: The No-U-Turn Sampler (NUTS)—Adaptively Setting Path Lengths in Hamiltonian Monte Carlo",
    "section": "",
    "text": "Matthew D. Hoffman & Andrew Gelman (2014)\n\n\nHamiltonian Monte Carlo (HMC) offers efficient sampling from complex posteriors but requires manual tuning of the trajectory length (number of leapfrog steps). Poor tuning can cause slow convergence or biased sampling, making HMC difficult for non-experts to use effectively.\n\n\n\nThe authors introduce the No-U-Turn Sampler (NUTS), an adaptive extension of HMC that automatically determines the optimal trajectory length. NUTS expands the trajectory until it detects a “U-turn” in parameter space—meaning further movement would retrace steps—and then stops. Combined with dual-averaging step-size adaptation, this removes nearly all manual tuning while preserving the sampling efficiency of HMC.\n\n\n\nAcross hierarchical models and logistic regression tasks, NUTS produces higher effective sample sizes per second and improved convergence diagnostics compared to traditional HMC or Gibbs sampling. It is robust across diverse posterior geometries and scales well with model complexity, forming the default MCMC algorithm in Stan.\n\n\n\nWhile adaptive, NUTS still relies on differentiable log-posteriors and can be computationally intensive for extremely complex or multimodal distributions. Performance depends on the choice of priors and model reparameterization, and it remains unsuitable for discrete or non-differentiable models.\n\n\n\nThe paper evaluates NUTS on simulated hierarchical models and real logistic regression datasets, reporting metrics like effective sample size (ESS) and Gelman–Rubin convergence diagnostics (()) to assess efficiency and accuracy."
  },
  {
    "objectID": "Summaries/aw/paper5_summary.html#problem-the-article-is-addressing",
    "href": "Summaries/aw/paper5_summary.html#problem-the-article-is-addressing",
    "title": "Summary: The No-U-Turn Sampler (NUTS)—Adaptively Setting Path Lengths in Hamiltonian Monte Carlo",
    "section": "",
    "text": "Hamiltonian Monte Carlo (HMC) offers efficient sampling from complex posteriors but requires manual tuning of the trajectory length (number of leapfrog steps). Poor tuning can cause slow convergence or biased sampling, making HMC difficult for non-experts to use effectively."
  },
  {
    "objectID": "Summaries/aw/paper5_summary.html#how-it-has-been-solved",
    "href": "Summaries/aw/paper5_summary.html#how-it-has-been-solved",
    "title": "Summary: The No-U-Turn Sampler (NUTS)—Adaptively Setting Path Lengths in Hamiltonian Monte Carlo",
    "section": "",
    "text": "The authors introduce the No-U-Turn Sampler (NUTS), an adaptive extension of HMC that automatically determines the optimal trajectory length. NUTS expands the trajectory until it detects a “U-turn” in parameter space—meaning further movement would retrace steps—and then stops. Combined with dual-averaging step-size adaptation, this removes nearly all manual tuning while preserving the sampling efficiency of HMC."
  },
  {
    "objectID": "Summaries/aw/paper5_summary.html#results",
    "href": "Summaries/aw/paper5_summary.html#results",
    "title": "Summary: The No-U-Turn Sampler (NUTS)—Adaptively Setting Path Lengths in Hamiltonian Monte Carlo",
    "section": "",
    "text": "Across hierarchical models and logistic regression tasks, NUTS produces higher effective sample sizes per second and improved convergence diagnostics compared to traditional HMC or Gibbs sampling. It is robust across diverse posterior geometries and scales well with model complexity, forming the default MCMC algorithm in Stan."
  },
  {
    "objectID": "Summaries/aw/paper5_summary.html#limitations",
    "href": "Summaries/aw/paper5_summary.html#limitations",
    "title": "Summary: The No-U-Turn Sampler (NUTS)—Adaptively Setting Path Lengths in Hamiltonian Monte Carlo",
    "section": "",
    "text": "While adaptive, NUTS still relies on differentiable log-posteriors and can be computationally intensive for extremely complex or multimodal distributions. Performance depends on the choice of priors and model reparameterization, and it remains unsuitable for discrete or non-differentiable models."
  },
  {
    "objectID": "Summaries/aw/paper5_summary.html#datasets",
    "href": "Summaries/aw/paper5_summary.html#datasets",
    "title": "Summary: The No-U-Turn Sampler (NUTS)—Adaptively Setting Path Lengths in Hamiltonian Monte Carlo",
    "section": "",
    "text": "The paper evaluates NUTS on simulated hierarchical models and real logistic regression datasets, reporting metrics like effective sample size (ESS) and Gelman–Rubin convergence diagnostics (()) to assess efficiency and accuracy."
  },
  {
    "objectID": "Summaries/aw/paper1_summary.html",
    "href": "Summaries/aw/paper1_summary.html",
    "title": "Summary: Bayesian Data Analysis for Newcomers",
    "section": "",
    "text": "John K. Kruschke and Torrin M. Liddell (2017)\n\n\nThe paper tackles the issue that Bayesian data analysis is often viewed as too complicated or intimidating for newcomers. Traditional explanations rely heavily on mathematical formulas, which discourages students and researchers who want to understand the ideas without advanced math. The authors aim to show that Bayesian reasoning is actually natural and aligns with how people already think in everyday life.\n\n\n\nThe authors present Bayesian concepts in plain language with minimal technical notation. They explain the main idea of Bayesian analysis as reallocating credibility among possible explanations as new data comes in. Simple examples are used to show how priors, likelihoods, and posteriors work together. They also clarify common misunderstandings, such as the role of prior distributions and how Bayesian inference differs from frequentist methods like null hypothesis significance testing.\n\n\n\nThe article provides readers with a clear conceptual framework for Bayesian analysis. Instead of being overwhelmed by formulas, the reader can see how Bayesian methods produce intuitive and interpretable results. Examples highlight how Bayesian outputs, such as posterior distributions and credible intervals, can be directly applied to research questions.\n\n\n\nBecause the article is written as an introduction, it does not go into technical detail or advanced modeling. It is not a substitute for mathematical training or for learning to implement Bayesian models in practice. The focus is on clarity rather than rigor.\n\n\n\nThe paper does not use any real-world datasets. Instead, it relies on simple examples meant to illustrate the principles of Bayesian reasoning. These examples are conceptual rather than empirical, making the ideas easier to grasp for newcomers like myself."
  },
  {
    "objectID": "Summaries/aw/paper1_summary.html#problem-the-article-is-addressing",
    "href": "Summaries/aw/paper1_summary.html#problem-the-article-is-addressing",
    "title": "Summary: Bayesian Data Analysis for Newcomers",
    "section": "",
    "text": "The paper tackles the issue that Bayesian data analysis is often viewed as too complicated or intimidating for newcomers. Traditional explanations rely heavily on mathematical formulas, which discourages students and researchers who want to understand the ideas without advanced math. The authors aim to show that Bayesian reasoning is actually natural and aligns with how people already think in everyday life."
  },
  {
    "objectID": "Summaries/aw/paper1_summary.html#how-it-has-been-solved",
    "href": "Summaries/aw/paper1_summary.html#how-it-has-been-solved",
    "title": "Summary: Bayesian Data Analysis for Newcomers",
    "section": "",
    "text": "The authors present Bayesian concepts in plain language with minimal technical notation. They explain the main idea of Bayesian analysis as reallocating credibility among possible explanations as new data comes in. Simple examples are used to show how priors, likelihoods, and posteriors work together. They also clarify common misunderstandings, such as the role of prior distributions and how Bayesian inference differs from frequentist methods like null hypothesis significance testing."
  },
  {
    "objectID": "Summaries/aw/paper1_summary.html#results",
    "href": "Summaries/aw/paper1_summary.html#results",
    "title": "Summary: Bayesian Data Analysis for Newcomers",
    "section": "",
    "text": "The article provides readers with a clear conceptual framework for Bayesian analysis. Instead of being overwhelmed by formulas, the reader can see how Bayesian methods produce intuitive and interpretable results. Examples highlight how Bayesian outputs, such as posterior distributions and credible intervals, can be directly applied to research questions."
  },
  {
    "objectID": "Summaries/aw/paper1_summary.html#limitations",
    "href": "Summaries/aw/paper1_summary.html#limitations",
    "title": "Summary: Bayesian Data Analysis for Newcomers",
    "section": "",
    "text": "Because the article is written as an introduction, it does not go into technical detail or advanced modeling. It is not a substitute for mathematical training or for learning to implement Bayesian models in practice. The focus is on clarity rather than rigor."
  },
  {
    "objectID": "Summaries/aw/paper1_summary.html#datasets",
    "href": "Summaries/aw/paper1_summary.html#datasets",
    "title": "Summary: Bayesian Data Analysis for Newcomers",
    "section": "",
    "text": "The paper does not use any real-world datasets. Instead, it relies on simple examples meant to illustrate the principles of Bayesian reasoning. These examples are conceptual rather than empirical, making the ideas easier to grasp for newcomers like myself."
  },
  {
    "objectID": "Summaries/aw/paper3_summary.html",
    "href": "Summaries/aw/paper3_summary.html",
    "title": "Bayesian Logistic Regression Capstone",
    "section": "",
    "text": "Paper 3: Abdullah, Hassan, & Mustafa (2022). “A Review on Bayesian Deep Learning in Healthcare: Applications and Challenges”\nGoal\nThe paper systematically reviews how Bayesian deep learning (BDL) is being applied in healthcare: its use cases, methodological approaches, and the challenges and future directions.\nImportance\nHealthcare data is often uncertain (noisy measurements, missing data, variability) and involves high stakes where mistakes can cost lives. While deep learning is powerful, it typically lacks mechanisms for representing uncertainty or handling limited data in a principled way. Bayesian techniques address these gaps by incorporating uncertainty, prior knowledge, and probabilistic reasoning—making models potentially safer, more trustworthy, and interpretable in clinical settings.\nMethods\n- Conduct a literature survey of recent work combining Bayesian methods with deep learning in healthcare.\n- Categorize approaches such as variational inference, Monte Carlo dropout, ensemble methods, Gaussian processes, and Bayesian neural networks.\n- Map these methods to healthcare tasks like diagnosis, prognosis, and treatment planning.\n- Evaluate strengths/weaknesses in terms of data availability, computational costs, interpretability, uncertainty calibration, and privacy.\nResults & Limitations\n- Results: BDL has shown success in disease classification, survival analysis, medical image segmentation, and predictive modeling. It often improves uncertainty quantification, may improve generalization, and provides clinicians with added information (e.g., confidence in predictions).\n- Limitations: High computational demands, scalability issues, difficulty specifying priors, and interpretability challenges remain. Many studies are proof-of-concept and lack validation in real-world clinical environments. Regulatory, privacy, and workflow integration concerns also limit deployment."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Us",
    "section": "",
    "text": "Dr. Namita Mishra is a physician, Head and Neck surgeon, and public health researcher with a strong foundation in medicine, epidemiology, and data science. She is currently a graduate student in Data Science (Health Analytics).\nHer work focuses on the early detection and prevention of non-communicable diseases (cancer, obesity) and on health disparities at the community level. She has conducted research on salivary gland tumors, cardiac implants, and community-based healthy food access. Leveraging her skills in Data Science, she integrates statistical modeling and Bayesian methods into her analyses. Her bioinformatics expertise includes using geodata visualization tools (3D maps and GIS) for presentations. Passionate about bridging clinical insight with data-driven approaches, she is dedicated to advancing sustainable, evidence-based solutions in epidemiology and community health.\nOutside of work, she enjoys gardening, cooking, singing, and sewing.\n📧 Contact: nmishra@uwf.edu\n\nAutumn S. Wilcox is a U.S. Navy veteran and Data Science graduate student at the University of West Florida, specializing in Analytics and Modeling. She has over nine years of experience in Network Operations and Technical Writing, including her current role at Navy Federal Credit Union, where she supports enterprise technology and process documentation initiatives. Autumn also holds certification in Clinical Research Quality Management (CRQM) and has contributed to quality oversight and compliance efforts in clinical research settings.\nHer background bridges technology, analytics, and healthcare, with a focus on applying data-driven approaches to improve communication and systems reliability. Outside of work, Autumn enjoys traveling, photography, and finding creative inspiration through music.\n📧 Contact: awr12@students.uwf.edu"
  },
  {
    "objectID": "slides.html#aim",
    "href": "slides.html#aim",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013–2014)",
    "section": "Aim",
    "text": "Aim\n\nCan Bayesian logistic regression provide more stable and transparent inference than classical MLE for doctor-diagnosed diabetes in NHANES 2013–2014?"
  },
  {
    "objectID": "slides.html#why-bayesian-principles-motivation",
    "href": "slides.html#why-bayesian-principles-motivation",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013–2014)",
    "section": "Why Bayesian? (Principles & Motivation)",
    "text": "Why Bayesian? (Principles & Motivation)\n\nPosterior ∝ Likelihood × Prior: update beliefs with data.\nWe need stability under missingness and potential separation.\nAdvantages here:\n\nRegularization from weakly-informative priors\nFull uncertainty via posterior distributions (credible intervals)\nIncorporates prior clinical/epi knowledge when available\n\nBottom line: improves stability and interpretability vs. classical MLE."
  },
  {
    "objectID": "slides.html#study-overview-data",
    "href": "slides.html#study-overview-data",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013–2014)",
    "section": "Study Overview & Data",
    "text": "Study Overview & Data\n\nData Source: NHANES 2013–2014 (CDC/NCHS) (National Center for Health Statistics (NCHS) 2014)\nPopulation: Adults aged ≥20 years\n\nOutcome Variable: DIQ010 — “Has a doctor told you that you have diabetes?”\n\nRecorded as 1 = Yes, 0 = No (excluding 7 = Refused, 9 = Don’t know)\n\nPredictors:\n\nBMXBMI – Body Mass Index (kg/m²)\n\nRIDAGEYR – Age (years)\n\nRIAGENDR – Sex (Male/Female)\n\nRIDRETH1 – Race/Ethnicity (5 categories)\n\nSurvey Design Variables:\n\nWeight = WTMEC2YR, Strata = SDMVSTRA, PSU = SDMVPSU\n\nSample Size: 5,769 adults; 5,592 with non-missing diabetes status\n\nWeighted diabetes prevalence: 8.9%\n\n\n\n\n   1    2    3    7    9 &lt;NA&gt; \n 737 8841  185    1    5  406"
  },
  {
    "objectID": "slides.html#methods",
    "href": "slides.html#methods",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013–2014)",
    "section": "Methods",
    "text": "Methods\nWe compared four approaches to model doctor-diagnosed diabetes:\n\nSurvey-weighted Logistic Regression (MLE)\n\nBaseline frequentist model using NHANES sampling weights.\n\nProvided odds ratios (OR) and 95% confidence intervals.\n\nFirth Penalized Logistic Regression\n\nAdded a Jeffreys-prior penalty to reduce small-sample bias.\n\nProduced finite estimates under quasi- or complete separation.\n\nMultiple Imputation + Logistic Regression (MICE)\n\nImputed missing predictor values using chained equations.\n\nCombined five imputed datasets via Rubin’s rules.\n\nBayesian Logistic Regression (brms)\n\nWeakly informative priors (coefficients Normal(0, 2.5); intercept Student-t(3, 0, 10)) (Gelman et al. 2008)\n\nFour chains × 2000 iterations, adapt_delta = 0.95 using NUTS (Hoffman and Gelman 2014)\n\nUsed normalized survey weights (mean = 1) as importance weights.\n\nDiagnostics: R̂ &lt; 1.01, high effective sample sizes, trace plot inspection."
  },
  {
    "objectID": "slides.html#modeling-framework-bayes-our-model",
    "href": "slides.html#modeling-framework-bayes-our-model",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013–2014)",
    "section": "Modeling Framework (Bayes & Our Model)",
    "text": "Modeling Framework (Bayes & Our Model)\nBayes’ Rule [ P() P(),P() ]\nProject Model (logistic regression) [ P(=1)= _0+_1(_c)+_2(_c)+_3() +_k _k(_k) ]\n\nPriors: Normal(0, 2.5) for coefficients; Student-t(3, 0, 10) for intercept\n\nWeights: NHANES exam weights normalized to mean 1\nSampling: 4 chains, 2000 iter, adapt_delta = 0.95"
  },
  {
    "objectID": "slides.html#key-results-correlations",
    "href": "slides.html#key-results-correlations",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013–2014)",
    "section": "Key Results & Correlations",
    "text": "Key Results & Correlations\n\n\n\nModel\nBMI (per 1 SD) OR (95% CI)\nAge (per 1 SD) OR (95% CI)\n\n\n\n\nSurvey-weighted MLE\n1.89 (1.65 – 2.15)\n3.03 (2.70 – 3.40)\n\n\nMICE pooled\n1.73 (1.58 – 1.89)\n2.90 (2.60 – 3.24)\n\n\nBayesian\n1.87 (1.71 – 2.05)\n2.99 (2.64 – 3.37)\n\n\n\n\nHigher BMI and age → increased odds of diabetes across all models.\n\nFemales had lower odds than males.\n\nMexican American, Non-Hispanic Black, and Multiracial groups showed elevated odds relative to Non-Hispanic White.\n\n\n\n\n\n\n\n\n\n\nPredictor\nRelationship with Diabetes\nDirection\nSignificance\n\n\n\n\nAge\nOlder adults more likely diagnosed\nPositive\nStrong\n\n\nBMI\nHigher BMI → greater risk\nPositive\nStrong\n\n\nSex (Female)\nLower odds vs. male\nNegative\nSignificant\n\n\nRace (Mexican Am., NH Black, Multiracial)\nHigher risk vs. NH White\nPositive\nSignificant"
  },
  {
    "objectID": "slides.html#posterior-distributions",
    "href": "slides.html#posterior-distributions",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013–2014)",
    "section": "Posterior Distributions",
    "text": "Posterior Distributions"
  },
  {
    "objectID": "slides.html#comparative-interpretation",
    "href": "slides.html#comparative-interpretation",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013–2014)",
    "section": "Comparative Interpretation",
    "text": "Comparative Interpretation\n\nEffect stability:\nBayesian estimates closely aligned with frequentist ORs, showing robust inference even under moderate missingness.\nUncertainty quantification:\nBayesian credible intervals were comparable in width to 95% CIs but offered a clearer probabilistic interpretation of uncertainty.\nSurvey weighting:\nThe Bayesian model’s normalized weights approximated the NHANES design effectively, while the MLE incorporated full survey structure directly."
  },
  {
    "objectID": "slides.html#discussion-and-conclusion",
    "href": "slides.html#discussion-and-conclusion",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013–2014)",
    "section": "Discussion and Conclusion",
    "text": "Discussion and Conclusion\n\nAll modeling frameworks identified age and BMI as strong independent predictors of doctor-diagnosed diabetes.\n\nBayesian modeling produced stable, interpretable, and well-regularized estimates while fully quantifying uncertainty.\n\nResults were consistent with classical logistic regression but more resilient to potential separation or small-sample issues.\n\nConclusion:\nBayesian logistic regression achieved the study aim — providing more stable and transparent inference than traditional MLE for population-based diabetes risk modeling.\n\nFuture directions:\nExtend Bayesian analysis across multiple NHANES cycles and include biomarker covariates to evaluate nonlinear effects."
  },
  {
    "objectID": "slides.html#key-takeaways",
    "href": "slides.html#key-takeaways",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013–2014)",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nConsistent Findings:\nHigher BMI and age were significant predictors of diabetes across all models.\nBayesian Advantage:\nProvided stable, interpretable estimates with full uncertainty quantification — outperforming classical MLE under missingness or quasi-separation.\nPractical Impact:\nDemonstrates that Bayesian regression offers a more transparent and resilient framework for population-based health modeling."
  },
  {
    "objectID": "slides.html#references",
    "href": "slides.html#references",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013–2014)",
    "section": "References",
    "text": "References\n\n\n\n\n\n\n\nGelman, A., A. Jakulin, M. G. Pittau, and Y. S. Su. 2008. “A Weakly Informative Default Prior Distribution for Logistic and Other Regression Models.” Annals of Applied Statistics 2 (4): 1360–83. https://doi.org/10.1214/08-AOAS191.\n\n\nHoffman, M. D., and A. Gelman. 2014. “The No-u-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo.” Journal of Machine Learning Research 15 (1): 1593–623. https://www.jmlr.org/papers/volume15/hoffman14a/hoffman14a.pdf.\n\n\nNational Center for Health Statistics (NCHS). 2014. “National Health and Nutrition Examination Survey (NHANES) 2013–2014 Data Documentation, Codebook, and Frequencies.” U.S. Department of Health; Human Services, Centers for Disease Control; Prevention. https://wwwn.cdc.gov/nchs/nhanes/continuousnhanes/overview.aspx?BeginYear=2013."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk Using NHANES 2013–2014 Data",
    "section": "",
    "text": "Slides: slides.html (Edit slides.qmd.)"
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk Using NHANES 2013–2014 Data",
    "section": "Overview",
    "text": "Overview\nWe analyzed NHANES 2013–2014 data from the CDC’s National Center for Health Statistics (National Center for Health Statistics (NCHS) 2014). Three public-use datasets were merged: demographics (DEMO_H), body measures (BMX_H), and the diabetes questionnaire (DIQ_H).\nOur workflow followed these key steps: 1. Import, merge, and clean NHANES files. 2. Define outcome and predictors, exclude gestational diabetes. 3. Standardize continuous predictors for numerical stability. 4. Specify the NHANES survey design with proper weights, strata, and PSUs. 5. Perform exploratory visualization and missing data checks. 6. Save cleaned datasets for reproducible downstream modeling."
  },
  {
    "objectID": "index.html#variables",
    "href": "index.html#variables",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk Using NHANES 2013–2014 Data",
    "section": "Variables",
    "text": "Variables\n\nOutcome: DIQ010 – doctor-diagnosed diabetes (1 = Yes, 2 = No; 7/9 = missing).\nPredictors: BMXBMI (Body Mass Index), RIDAGEYR (Age), RIAGENDR (Sex), and RIDRETH1 (Race/Ethnicity, 5 categories).\nExclusion: Females with gestational diabetes (DIQ050 == 1).\nCohort: Adults aged ≥20 years.\nSurvey Variables: WTMEC2YR (weight), SDMVPSU, and SDMVSTRA."
  },
  {
    "objectID": "index.html#data-preparation",
    "href": "index.html#data-preparation",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk Using NHANES 2013–2014 Data",
    "section": "Data Preparation",
    "text": "Data Preparation\n\n\nCode\n# ---- Import and merge NHANES 2013–2014 ----\n\ndemo_h &lt;- nhanesA::nhanes(\"DEMO_H\")\nbmx_h  &lt;- nhanesA::nhanes(\"BMX_H\")\ndiq_h  &lt;- nhanesA::nhanes(\"DIQ_H\")\n\nmerged_data &lt;- demo_h %&gt;%\n  select(SEQN, RIDAGEYR, RIAGENDR, RIDRETH1, SDMVPSU, SDMVSTRA, WTMEC2YR) %&gt;%\n  left_join(bmx_h %&gt;% select(SEQN, BMXBMI), by = \"SEQN\") %&gt;%\n  left_join(diq_h %&gt;% select(SEQN, DIQ010, DIQ050), by = \"SEQN\")\n\ndir.create(\"data\", showWarnings = FALSE, recursive = TRUE)\n\n\n\n\nCode\n# Coerce key fields to numeric codes if they arrived as labels/characters\n\ncoerce_num &lt;- function(x) {\nif (is.numeric(x)) return(x)\nxc &lt;- as.character(x)\nnx &lt;- suppressWarnings(readr::parse_number(xc))\nif (mean(is.na(nx)) &gt; 0.80) {\nxl &lt;- tolower(trimws(xc))\nnx &lt;- dplyr::case_when(\nxl %in% c(\"1\",\"yes\",\"yes, told\") ~ 1,\nxl %in% c(\"2\",\"no\",\"no, not told\") ~ 2,\nxl %in% c(\"3\",\"borderline\") ~ 3,\nxl %in% c(\"7\",\"refused\") ~ 7,\nxl %in% c(\"9\",\"don't know\",\"dont know\",\"unknown\") ~ 9,\nTRUE ~ NA_real_\n)\n}\nas.numeric(nx)\n}\n\nmerged_data &lt;- merged_data %&gt;%\nmutate(\nDIQ010   = coerce_num(DIQ010),\nDIQ050   = if (!\"DIQ050\" %in% names(.)) NA_real_ else coerce_num(DIQ050),\nRIDAGEYR = suppressWarnings(as.numeric(RIDAGEYR)),\nRIAGENDR = coerce_num(RIAGENDR),\nRIDRETH1 = coerce_num(RIDRETH1),\nBMXBMI   = suppressWarnings(as.numeric(BMXBMI))\n)\n\nlist(\nDIQ010_head = head(merged_data$DIQ010),\nDIQ010_tab  = table(merged_data$DIQ010, useNA = \"ifany\")\n)\n\n\n$DIQ010_head\n[1] 1 1 1 2 2 2\n\n$DIQ010_tab\n\n   1    2    3    7    9 &lt;NA&gt; \n 737 8841  185    1    5  406 \n\n\nCode\nsaveRDS(merged_data, \"data/merged_2013_2014.rds\")"
  },
  {
    "objectID": "index.html#data-structure-missingness",
    "href": "index.html#data-structure-missingness",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk Using NHANES 2013–2014 Data",
    "section": "Data Structure & Missingness",
    "text": "Data Structure & Missingness\n\n\nCode\n# ---- Explore structure and missingness ----\n\nstr(merged_data)\n\n\n'data.frame':   10175 obs. of  10 variables:\n $ SEQN    : num  73557 73558 73559 73560 73561 ...\n $ RIDAGEYR: num  69 54 72 9 73 56 0 61 42 56 ...\n $ RIAGENDR: num  NA NA NA NA NA NA NA NA NA NA ...\n $ RIDRETH1: num  NA NA NA NA NA NA NA NA NA NA ...\n $ SDMVPSU : num  1 1 1 2 2 1 1 1 2 1 ...\n $ SDMVSTRA: num  112 108 109 109 116 111 105 114 106 112 ...\n $ WTMEC2YR: num  13481 24472 57193 55767 65542 ...\n $ BMXBMI  : num  26.7 28.6 28.9 17.1 19.7 41.7 NA 35.7 NA 26.5 ...\n $ DIQ010  : num  1 1 1 2 2 2 NA 2 2 2 ...\n $ DIQ050  : num  1 1 1 2 2 2 NA 2 2 2 ...\n\n\nCode\nsummary(merged_data)\n\n\n      SEQN          RIDAGEYR        RIAGENDR        RIDRETH1    \n Min.   :73557   Min.   : 0.00   Min.   : NA     Min.   : NA    \n 1st Qu.:76100   1st Qu.:10.00   1st Qu.: NA     1st Qu.: NA    \n Median :78644   Median :26.00   Median : NA     Median : NA    \n Mean   :78644   Mean   :31.48   Mean   :NaN     Mean   :NaN    \n 3rd Qu.:81188   3rd Qu.:52.00   3rd Qu.: NA     3rd Qu.: NA    \n Max.   :83731   Max.   :80.00   Max.   : NA     Max.   : NA    \n                                 NA's   :10175   NA's   :10175  \n    SDMVPSU         SDMVSTRA        WTMEC2YR          BMXBMI     \n Min.   :1.000   Min.   :104.0   Min.   :     0   Min.   :12.10  \n 1st Qu.:1.000   1st Qu.:107.0   1st Qu.: 12562   1st Qu.:19.70  \n Median :1.000   Median :111.0   Median : 20175   Median :24.70  \n Mean   :1.484   Mean   :110.9   Mean   : 30585   Mean   :25.68  \n 3rd Qu.:2.000   3rd Qu.:115.0   3rd Qu.: 36748   3rd Qu.:30.20  \n Max.   :2.000   Max.   :118.0   Max.   :171395   Max.   :82.90  \n                                                  NA's   :1120   \n     DIQ010          DIQ050     \n Min.   :1.000   Min.   :1.000  \n 1st Qu.:2.000   1st Qu.:2.000  \n Median :2.000   Median :2.000  \n Mean   :1.948   Mean   :1.979  \n 3rd Qu.:2.000   3rd Qu.:2.000  \n Max.   :9.000   Max.   :9.000  \n NA's   :406     NA's   :407    \n\n\nCode\nplot_intro(merged_data, title = \"Overview of NHANES Variables and Types\")\nplot_missing(merged_data, title = \"Missing Data Patterns in NHANES 2013–2014\")\n\n\n\n\n\n\n\n\nFigure 1: Overview of variables and missing-data patterns for NHANES 2013–2014.\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Overview of variables and missing-data patterns for NHANES 2013–2014."
  },
  {
    "objectID": "index.html#adult-analysis-dataset",
    "href": "index.html#adult-analysis-dataset",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk Using NHANES 2013–2014 Data",
    "section": "Adult Analysis Dataset",
    "text": "Adult Analysis Dataset\n\n\nCode\n# ---- Clean and filter for adults (≥ 20 years) ----\n\nadult &lt;- merged_data %&gt;%\nfilter(RIDAGEYR &gt;= 20) %&gt;%\ntransmute(\nSDMVPSU, SDMVSTRA, WTMEC2YR,\n# Outcome\ndiabetes_dx = case_when(\nDIQ010 == 1 ~ 1,\nDIQ010 == 2 ~ 0,\nDIQ010 %in% c(3,7,9) ~ NA_real_,\nTRUE ~ NA_real_\n),\n# Predictors\nbmi  = as.numeric(BMXBMI),\nage  = as.numeric(RIDAGEYR),\nsex  = fct_recode(factor(RIAGENDR), Male = \"1\", Female = \"2\"),\nrace = fct_recode(\nfactor(RIDRETH1),\n\"Mexican American\" = \"1\",\n\"Other Hispanic\"   = \"2\",\n\"NH White\"         = \"3\",\n\"NH Black\"         = \"4\",\n\"Other/Multi\"      = \"5\"\n),\nDIQ050 = DIQ050\n) %&gt;%\n\n# Exclude gestational diabetes\n\nmutate(diabetes_dx = ifelse(sex == \"Female\" & !is.na(DIQ050) & DIQ050 == 1, 0, diabetes_dx)) %&gt;%\n\n# Standardize continuous predictors\n\nmutate(\nage_c = as.numeric(scale(age)),\nbmi_c = as.numeric(scale(bmi)),\ndiabetes_f = factor(diabetes_dx, levels = c(0,1), labels = c(\"No\",\"Yes\"))\n) %&gt;%\n\n# Set NH White as reference \n\nmutate(race = fct_relevel(race, \"NH White\"))\n\nstr(adult)\n\n\n'data.frame':   5769 obs. of  12 variables:\n $ SDMVPSU    : num  1 1 1 2 1 1 2 1 2 2 ...\n $ SDMVSTRA   : num  112 108 109 116 111 114 106 112 112 113 ...\n $ WTMEC2YR   : num  13481 24472 57193 65542 25345 ...\n $ diabetes_dx: num  NA NA NA 0 0 0 0 0 0 0 ...\n $ bmi        : num  26.7 28.6 28.9 19.7 41.7 35.7 NA 26.5 22 20.3 ...\n $ age        : num  69 54 72 73 56 61 42 56 65 26 ...\n $ sex        : Factor w/ 0 levels: NA NA NA NA NA NA NA NA NA NA ...\n $ race       : Factor w/ 0 levels: NA NA NA NA NA NA NA NA NA NA ...\n $ DIQ050     : num  1 1 1 2 2 2 2 2 2 2 ...\n $ age_c      : num  1.132 0.278 1.303 1.36 0.392 ...\n $ bmi_c      : num  -0.3359 -0.0703 -0.0283 -1.3144 1.761 ...\n $ diabetes_f : Factor w/ 2 levels \"No\",\"Yes\": NA NA NA 1 1 1 1 1 1 1 ...\n\n\n\n\nCode\n# Create a coarser race factor to avoid sparse categories in modeling\n\nadult &lt;- adult %&gt;%\n  mutate(\n    race3 = forcats::fct_collapse(\n      race,\n      White    = \"NH White\",\n      Black    = \"NH Black\",\n      Hispanic = c(\"Mexican American\", \"Other Hispanic\"),\n      Other    = \"Other/Multi\"\n    ),\n    race3 = forcats::fct_lump_min(race3, min = 30, other_level = \"Other\"),\n    race3 = forcats::fct_relevel(race3, \"White\")\n  )\n\n\nThe variable race3 represents a coarsened race/ethnicity factor derived from the original five NHANES categories (RIDRETH1).\nTo mitigate sparse cells, we collapsed categories into three primary groups: White, Black, and Hispanic, with remaining low-frequency levels combined as Other when necessary.\nAll model-based analyses used this coarsened factor."
  },
  {
    "objectID": "index.html#survey-design",
    "href": "index.html#survey-design",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk Using NHANES 2013–2014 Data",
    "section": "Survey Design",
    "text": "Survey Design\n\n\nCode\n# ---- Survey design object ----\n\nnhanes_design_adult &lt;- survey::svydesign(\nid = ~SDMVPSU,\nstrata = ~SDMVSTRA,\nweights = ~WTMEC2YR,\nnest = TRUE,\ndata = adult\n)\n\n# Quick weighted checks (silently return; useful for QC)\n\nsurvey::svymean(~age, nhanes_design_adult, na.rm = TRUE)\n\n\n      mean     SE\nage 47.496 0.3805\n\n\nCode\nsurvey::svymean(~diabetes_dx, nhanes_design_adult, na.rm = TRUE)\n\n\n               mean     SE\ndiabetes_dx 0.07675 0.0043"
  },
  {
    "objectID": "index.html#exploratory-visualization",
    "href": "index.html#exploratory-visualization",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk Using NHANES 2013–2014 Data",
    "section": "Exploratory Visualization",
    "text": "Exploratory Visualization\n\n\nCode\n# ---- BMI, Age, and Diabetes Distributions ----\n\nlibrary(ggplot2)\n\np_bmi &lt;- ggplot(adult, aes(x = bmi, fill = diabetes_f)) +\ngeom_histogram(position = \"identity\", alpha = 0.6, bins = 30) +\nlabs(title = \"BMI Distribution by Diabetes Status\", x = \"BMI (kg/m²)\", y = \"Count\", fill = \"Diabetes\") +\ntheme_minimal()\n\np_age &lt;- ggplot(adult, aes(x = age, fill = diabetes_f)) +\ngeom_histogram(position = \"identity\", alpha = 0.6, bins = 30) +\nlabs(title = \"Age Distribution by Diabetes Status\", x = \"Age (years)\", y = \"Count\", fill = \"Diabetes\") +\ntheme_minimal()\n\n# Print sequentially\n\np_bmi\np_age\n\n\n\n\n\n\n\n\nFigure 3: Distributions of BMI and Age by diabetes status (adult cohort, ≥20y).\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Distributions of BMI and Age by diabetes status (adult cohort, ≥20y)."
  },
  {
    "objectID": "index.html#logistic-model",
    "href": "index.html#logistic-model",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk Using NHANES 2013–2014 Data",
    "section": "Logistic Model",
    "text": "Logistic Model\nWe model the probability of doctor-diagnosed diabetes as:\n\\[\n\\operatorname{logit}\\{\\Pr(Y_i=1)\\}\n= \\beta_0 + \\beta_1\\,\\mathrm{BMI}_i + \\beta_2\\,\\mathrm{Age}_i\n+ \\beta_3\\,\\mathrm{Sex}_i + \\sum_{g}\\beta_{3+g}\\,\\mathrm{Race}_{ig}\n\\]\nBMI and Age are standardized (mean 0, SD 1). Categorical predictors use Male and White as reference levels, with race modeled using the coarsened three-level factor (race3) (White, Black, Hispanic; smaller groups lumped as ‘Other’ if present). Because some race/ethnicity categories are sparse, race was coarsened to a three-level factor (race3: White, Black, Hispanic; rare/other levels lumped as needed) to reduce separation and improve stability. All analyses respect NHANES design elements (PSU, strata, exam weights)."
  },
  {
    "objectID": "index.html#frequentist-logistic-regression-survey-weighted",
    "href": "index.html#frequentist-logistic-regression-survey-weighted",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk Using NHANES 2013–2014 Data",
    "section": "Frequentist Logistic Regression (Survey-Weighted)",
    "text": "Frequentist Logistic Regression (Survey-Weighted)\nWe fit a survey-weighted logistic regression (svyglm) for:\n\\[\n\\text{diabetes\\_dx} \\sim \\text{age\\_c} + \\text{bmi\\_c} + \\text{sex} + \\text{race3}\n\\]\nTo avoid separation from sparse cells, we used the coarsened race3 factor, and we only ran the model when both sex and race3 had ≥2 observed levels among complete cases. We report odds ratios (ORs) with 95% CIs."
  },
  {
    "objectID": "index.html#multiple-imputation-mice",
    "href": "index.html#multiple-imputation-mice",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk Using NHANES 2013–2014 Data",
    "section": "Multiple Imputation (MICE)",
    "text": "Multiple Imputation (MICE)\nWe performed MICE (m = 5) using predictive mean matching for continuous variables and polytomous/logistic regression for categoricals (sex, race3). The outcome was not imputed; NHANES design variables were included as auxiliaries only. Because certain imputed datasets occasionally collapsed a factor to a single level, we fit the logistic model only on imputations where each included factor retained ≥2 levels, selected a consistent right-hand side, and pooled those estimates with Rubin’s rules."
  },
  {
    "objectID": "index.html#bayesian-logistic-regression",
    "href": "index.html#bayesian-logistic-regression",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk Using NHANES 2013–2014 Data",
    "section": "Bayesian Logistic Regression",
    "text": "Bayesian Logistic Regression\nWe fit the same formula in brms with weakly informative priors (Normal(0, 2.5) for coefficients; Student-t(3, 0, 10) for the intercept, parameterized as Student-t(ν = 3, μ = 0, σ = 10) (Schoot et al. 2013)), sampling with NUTS (4 chains × 2000 iterations; adapt_delta = 0.95). NHANES exam weights were normalized and used as importance weights, approximating—but not fully reproducing—design-based variance. To mitigate sparsity, we used race3 and adopted an adaptive formula: if a factor collapsed to one level in the selected imputed dataset, that factor was omitted from the Bayesian model. We report posterior ORs with 95% credible intervals for terms retained in the final formula.\nSensitivity. Running the Bayesian model on alternative valid imputations yielded similar age and BMI effects.\nDiagnostics. Convergence and fit were acceptable (\\(\\hat{R} &lt; 1.01\\) and large effective sample sizes). Posterior predictive checks (pp_check) were satisfactory; we also report Bayesian \\(R^2\\)."
  },
  {
    "objectID": "index.html#modeling-code",
    "href": "index.html#modeling-code",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk Using NHANES 2013–2014 Data",
    "section": "Modeling Code",
    "text": "Modeling Code\n\n\nCode\n# Define complete-case rows once\n\nkeep_cc &lt;- with(adult, !is.na(diabetes_dx) & !is.na(age_c) & !is.na(bmi_c) &\n                        !is.na(sex) & !is.na(race3))\n\ncat(\"Complete-case N:\", sum(keep_cc), \"\\n\")\n\n\nComplete-case N: 0 \n\n\nCode\ncat(\"Sex (complete cases):\\n\");  print(table(droplevels(adult$sex[keep_cc]),  useNA=\"ifany\"))\n\n\nSex (complete cases):\n\n\n&lt; table of extent 0 &gt;\n\n\nCode\ncat(\"race3 (complete cases):\\n\"); print(table(droplevels(adult$race3[keep_cc]), useNA=\"ifany\"))\n\n\nrace3 (complete cases):\n\n\n&lt; table of extent 0 &gt;\n\n\n\n\nCode\n# --- Guardrails & complete-case checks ---\n\nstopifnot(sum(!is.na(adult$diabetes_dx)) &gt; 0)\n\nadult &lt;- adult %&gt;%\n  dplyr::mutate(\n    sex  = if (!is.factor(sex))  factor(sex)  else sex,\n    race = if (!is.factor(race)) factor(race) else race\n  )\n\n# Define complete-case indicator for the survey-weighted model\nkeep_cc &lt;- with(adult, !is.na(diabetes_dx) & !is.na(age_c) & !is.na(bmi_c) &\n                        !is.na(sex) & !is.na(race3))\n\nlev_sex   &lt;- nlevels(droplevels(adult$sex[keep_cc]))\nlev_race3 &lt;- nlevels(droplevels(adult$race3[keep_cc]))\ndo_svy    &lt;- (lev_sex &gt;= 2 && lev_race3 &gt;= 2)\n\nif (!do_svy) {\n  warning(sprintf(\n    \"Survey-weighted model skipped: insufficient observed levels among complete cases (sex=%d, race3=%d).\",\n    lev_sex, lev_race3\n  ))\n  print(table(droplevels(adult$sex[keep_cc]),   useNA = \"ifany\"))\n  print(table(droplevels(adult$race3[keep_cc]), useNA = \"ifany\"))\n}\n\n\n&lt; table of extent 0 &gt;\n&lt; table of extent 0 &gt;\n\n\nCode\n# 1) Survey-weighted complete case  (conditionally run)\nif (do_svy) {\n  des_cc  &lt;- subset(nhanes_design_adult, keep_cc)\n  svy_fit &lt;- survey::svyglm(\n    diabetes_dx ~ age_c + bmi_c + sex + race3,\n    design = des_cc, family = quasibinomial()\n  )\n\n  svy_or &lt;- broom::tidy(svy_fit, conf.int = TRUE) %&gt;%\n    dplyr::mutate(\n      OR  = exp(estimate),\n      LCL = exp(conf.low),\n      UCL = exp(conf.high)\n    ) %&gt;%\n    dplyr::select(term, OR, LCL, UCL, p.value) %&gt;%\n    dplyr::filter(term != \"(Intercept)\")\n\n  knitr::kable(\n    svy_or,\n    caption = \"Survey-weighted odds ratios (per 1 SD) — references: Male; White (race3).\"\n  )\n} else {\n  svy_or &lt;- tibble::tibble(\n    term = character(), OR = numeric(), LCL = numeric(), UCL = numeric(), p.value = numeric()\n  )\n}\n\n # ---- 2) Multiple Imputation (predictors only) ----\n\n# Build the data used for imputation (already created above, but safe here)\nmi_dat &lt;- adult %&gt;%\n  dplyr::select(diabetes_dx, age, bmi, sex, race3, WTMEC2YR, SDMVPSU, SDMVSTRA)\n\n# Methods & predictor matrix\nmeth &lt;- mice::make.method(mi_dat)\npred &lt;- mice::make.predictorMatrix(mi_dat)\n\n# Outcome not imputed; use it to help impute predictors\nmeth[\"diabetes_dx\"] &lt;- \"\"\npred[\"diabetes_dx\", ] &lt;- 0\npred[, \"diabetes_dx\"] &lt;- 1\n\n# Imputation models\nmeth[c(\"age\",\"bmi\")]   &lt;- c(\"norm\",\"pmm\")\nmeth[c(\"sex\",\"race3\")] &lt;- c(\"logreg\",\"polyreg\")\n\n# Design vars as auxiliaries only\nmeth[c(\"WTMEC2YR\",\"SDMVPSU\",\"SDMVSTRA\")] &lt;- \"\"\npred[, c(\"WTMEC2YR\",\"SDMVPSU\",\"SDMVSTRA\")] &lt;- 1\n\nimp &lt;- mice::mice(mi_dat, m = 5, method = meth, predictorMatrix = pred, seed = 123)\n\n\n\n iter imp variable\n  1   1  bmi\n  1   2  bmi\n  1   3  bmi\n  1   4  bmi\n  1   5  bmi\n  2   1  bmi\n  2   2  bmi\n  2   3  bmi\n  2   4  bmi\n  2   5  bmi\n  3   1  bmi\n  3   2  bmi\n  3   3  bmi\n  3   4  bmi\n  3   5  bmi\n  4   1  bmi\n  4   2  bmi\n  4   3  bmi\n  4   4  bmi\n  4   5  bmi\n  5   1  bmi\n  5   2  bmi\n  5   3  bmi\n  5   4  bmi\n  5   5  bmi\n\n\nCode\n# Lock global levels for consistency\nlvl_sex   &lt;- levels(adult$sex)\nlvl_race3 &lt;- levels(adult$race3)\n\nfit_one_imp &lt;- function(i) {\n  dat &lt;- mice::complete(imp, i)\n\n  # Enforce levels, then standardize\n  dat$sex   &lt;- factor(dat$sex,   levels = lvl_sex)\n  dat$race3 &lt;- factor(dat$race3, levels = lvl_race3)\n\n  dat$age_c &lt;- as.numeric(scale(dat$age))\n  dat$bmi_c &lt;- as.numeric(scale(dat$bmi))\n\n  dat &lt;- dat %&gt;%\n    dplyr::filter(!is.na(diabetes_dx), !is.na(age_c), !is.na(bmi_c)) %&gt;%\n    dplyr::mutate(\n      sex   = droplevels(sex),\n      race3 = droplevels(race3)\n    )\n\n  # Build formula adaptively per imputation\n  rhs_terms &lt;- c(\"age_c\", \"bmi_c\")\n  if (nlevels(dat$sex)   &gt;= 2) rhs_terms &lt;- c(rhs_terms, \"sex\")\n  if (nlevels(dat$race3) &gt;= 2) rhs_terms &lt;- c(rhs_terms, \"race3\")\n\n  # If even sex & race3 both collapse, we still fit age+bmi only\n  fml &lt;- reformulate(rhs_terms, response = \"diabetes_dx\")\n\n  # Keep only rows where included factors are not NA\n  keep &lt;- !is.na(dat$age_c) & !is.na(dat$bmi_c) & !is.na(dat$diabetes_dx)\n  if (\"sex\"   %in% rhs_terms)  keep &lt;- keep & !is.na(dat$sex)\n  if (\"race3\" %in% rhs_terms)  keep &lt;- keep & !is.na(dat$race3)\n  dat &lt;- dat[keep, , drop = FALSE]\n\n  if (nrow(dat) &lt; 10) return(NULL)  # too small to be useful\n  fit &lt;- stats::glm(fml, data = dat, family = binomial())\n  # stash the RHS we used so we can pool consistently later\n  attr(fit, \"rhs_terms\") &lt;- rhs_terms\n  fit\n}\n\nfits &lt;- lapply(seq_len(imp$m), fit_one_imp)\ngood &lt;- vapply(fits, inherits, logical(1), \"glm\")\nfits_ok &lt;- fits[good]\n\nif (!length(fits_ok)) stop(\"All MI fits failed even after adaptive formulas.\")\n\n# Choose the most common RHS across successful fits \nrhs_key &lt;- vapply(fits_ok, function(f) paste(attr(f, \"rhs_terms\"), collapse = \"+\"), \"\")\ntop_rhs &lt;- names(sort(table(rhs_key), decreasing = TRUE))[1]\nfits_consistent &lt;- Filter(function(f) paste(attr(f, \"rhs_terms\"), collapse = \"+\") == top_rhs, fits_ok)\n\n# Pool\npool_mi &lt;- mice::pool(fits_consistent)\n\nmi_or &lt;- summary(pool_mi, conf.int = TRUE, exponentiate = TRUE) %&gt;%\n  dplyr::rename(OR = estimate, LCL = `2.5 %`, UCL = `97.5 %`) %&gt;%\n  dplyr::filter(term != \"(Intercept)\")\n\nknitr::kable(\n  mi_or,\n  caption = sprintf(\n    \"MICE pooled odds ratios (per 1 SD) — references: Male; White (race3). Pooled across %d/%d imputations; formula: diabetes_dx ~ %s\",\n    length(fits_consistent), imp$m, top_rhs\n  )\n)\n\n\n\nMICE pooled odds ratios (per 1 SD) — references: Male; White (race3). Pooled across 5/5 imputations; formula: diabetes_dx ~ age_c+bmi_c\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nOR\nstd.error\nstatistic\ndf\np.value\nLCL\nUCL\nconf.low\nconf.high\n\n\n\n\n2\nage_c\n2.549055\n0.0556941\n16.80111\n5319.822\n0\n2.285397\n2.843132\n2.285397\n2.843132\n\n\n3\nbmi_c\n1.599624\n0.0453757\n10.35287\n1719.076\n0\n1.463413\n1.748513\n1.463413\n1.748513\n\n\n\n\n\nCode\n# ---- 3) Bayesian (importance weights) ----\n\nif (!exists(\"imp\")) stop(\"Missing MI object 'imp' — run the MI block first.\")\n\nlvl_sex   &lt;- levels(adult$sex)\nlvl_race3 &lt;- levels(adult$race3)\n\nprep_imputed &lt;- function(i, coarsen = FALSE) {\n  dat &lt;- mice::complete(imp, i) %&gt;%\n    dplyr::mutate(\n      sex   = factor(sex,   levels = lvl_sex),\n      race3 = factor(race3, levels = lvl_race3),\n      age_c = as.numeric(scale(age)),\n      bmi_c = as.numeric(scale(bmi)),\n      wt_norm = WTMEC2YR / mean(WTMEC2YR, na.rm = TRUE)\n    ) %&gt;%\n    dplyr::filter(!is.na(diabetes_dx), !is.na(age_c), !is.na(bmi_c))\n\n  if (coarsen) {\n    # Collapse race to 2 groups to avoid sparsity\n    dat$race3 &lt;- forcats::fct_collapse(dat$race3,\n      White = \"White\",\n      `Non-White` = setdiff(levels(dat$race3), \"White\")\n    )\n    dat$race3 &lt;- forcats::fct_relevel(dat$race3, \"White\")\n  }\n\n  dat &lt;- dat %&gt;%\n    dplyr::mutate(\n      sex   = droplevels(sex),\n      race3 = droplevels(race3)\n    )\n\n  list(\n    dat = dat,\n    sex_ok  = nlevels(dat$sex)   &gt;= 2,\n    race_ok = nlevels(dat$race3) &gt;= 2\n  )\n}\n\n# Try to find a good imputation as-is; otherwise try with coarsened race\ncandidates &lt;- lapply(seq_len(imp$m), prep_imputed, coarsen = FALSE)\ngood_idx &lt;- which(vapply(candidates, function(x) x$sex_ok && x$race_ok, logical(1)))\nuse &lt;- NULL; used_coarsen &lt;- FALSE\nif (length(good_idx)) {\n  use &lt;- candidates[[good_idx[1]]]  \n} else {\n  candidates2 &lt;- lapply(seq_len(imp$m), prep_imputed, coarsen = TRUE)\n  good2_idx &lt;- which(vapply(candidates2, function(x) x$sex_ok && x$race_ok, logical(1)))\n  if (length(good2_idx)) {\n    use &lt;- candidates2[[good2_idx[1]]]; used_coarsen &lt;- TRUE\n  } else {\n    # take the first and drop collapsed factors later\n    use &lt;- candidates2[[1]]\n    used_coarsen &lt;- TRUE\n  }\n}\n\nadult_imp1 &lt;- use$dat\n\n# Build RHS adaptively\nrhs &lt;- c(\"age_c\", \"bmi_c\")\nif (use$sex_ok)  rhs &lt;- c(rhs, \"sex\")\nif (use$race_ok) rhs &lt;- c(rhs, \"race3\")\n\nif (!use$sex_ok)  message(\"Bayes: 'sex' collapsed to 1 level in selected imputation — dropping from model.\")\nif (!use$race_ok) message(\"Bayes: 'race3' collapsed to 1 level in selected imputation — dropping from model.\")\nif (used_coarsen) message(\"Bayes: using coarsened race (White vs Non-White).\")\n\n# Final formula\nfml_bayes &lt;- as.formula(paste(\"diabetes_dx | weights(wt_norm) ~\", paste(rhs, collapse = \" + \")))\n\n# Priors\npriors &lt;- c(\n  brms::set_prior(\"normal(0, 2.5)\", class = \"b\"),\n  brms::set_prior(\"student_t(3, 0, 10)\", class = \"Intercept\")\n)\n\n# Fit\nbayes_fit &lt;- brms::brm(\n  fml_bayes,\n  data    = adult_imp1,\n  family  = bernoulli(link = \"logit\"),\n  prior   = priors,\n  chains  = 4, iter = 2000, seed = 123,\n  control = list(adapt_delta = 0.95),\n  refresh = 0\n)\n\n\nRunning /Library/Frameworks/R.framework/Resources/bin/R CMD SHLIB foo.c\nusing C compiler: ‘Apple clang version 17.0.0 (clang-1700.0.13.5)’\nusing SDK: ‘MacOSX15.5.sdk’\nclang -arch arm64 -std=gnu2x -I\"/Library/Frameworks/R.framework/Resources/include\" -DNDEBUG   -I\"/Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/Rcpp/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/RcppEigen/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/RcppEigen/include/unsupported\"  -I\"/Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/BH/include\" -I\"/Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/StanHeaders/include/src/\"  -I\"/Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/StanHeaders/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/RcppParallel/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/rstan/include\" -DEIGEN_NO_DEBUG  -DBOOST_DISABLE_ASSERTS  -DBOOST_PENDING_INTEGER_LOG2_HPP  -DSTAN_THREADS  -DUSE_STANC3 -DSTRICT_R_HEADERS  -DBOOST_PHOENIX_NO_VARIADIC_EXPRESSION  -D_HAS_AUTO_PTR_ETC=0  -include '/Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp'  -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1   -I/opt/R/arm64/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c foo.c -o foo.o\nIn file included from &lt;built-in&gt;:1:\nIn file included from /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp:22:\nIn file included from /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/RcppEigen/include/Eigen/Dense:1:\nIn file included from /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/RcppEigen/include/Eigen/Core:19:\n/Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:679:10: fatal error: 'cmath' file not found\n  679 | #include &lt;cmath&gt;\n      |          ^~~~~~~\n1 error generated.\nmake: *** [foo.o] Error 1\n\n\nCode\n# Posterior ORs table \nbayes_or &lt;- brms::posterior_summary(bayes_fit, pars = \"^b_\") %&gt;%\n  as.data.frame() %&gt;% tibble::rownames_to_column(\"raw\") %&gt;%\n  dplyr::mutate(\n    term = gsub(\"^b_\", \"\", raw),\n    term = gsub(\"sex\",   \"sex:\",   term),\n    term = gsub(\"race3\", \"race3:\", term),\n    OR   = exp(Estimate), LCL = exp(Q2.5), UCL = exp(Q97.5)\n  ) %&gt;%\n  dplyr::select(term, OR, LCL, UCL) %&gt;%\n  dplyr::filter(term != \"Intercept\")\n\nknitr::kable(\n  dplyr::mutate(bayes_or, dplyr::across(c(OR, LCL, UCL), ~ round(.x, 2))),\n  digits = 2,\n  caption = paste0(\n    \"Bayesian posterior odds ratios (95% CrI; per 1 SD for age_c/bmi_c) — \",\n    \"references: Male; White (race3). Factors shown only when retained in the final model \",\n    \"(adaptive formula). Weights are normalized NHANES exam weights (importance weights).\"\n  )\n)\n\n\n\nBayesian posterior odds ratios (95% CrI; per 1 SD for age_c/bmi_c) — references: Male; White (race3). Factors shown only when retained in the final model (adaptive formula). Weights are normalized NHANES exam weights (importance weights).\n\n\nterm\nOR\nLCL\nUCL\n\n\n\n\nage_c\n2.63\n2.34\n2.97\n\n\nbmi_c\n1.78\n1.62\n1.95"
  },
  {
    "objectID": "index.html#data-quality-missingness",
    "href": "index.html#data-quality-missingness",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk Using NHANES 2013–2014 Data",
    "section": "1. Data Quality & Missingness",
    "text": "1. Data Quality & Missingness\n\n\nCode\nplot_intro(merged_data)\nplot_missing(merged_data)\n\n\n\n\n\n\n\n\nFigure 5: Variable overview and missing-data patterns (NHANES 2013–2014).\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: Variable overview and missing-data patterns (NHANES 2013–2014)."
  },
  {
    "objectID": "index.html#exploratory-distributions",
    "href": "index.html#exploratory-distributions",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk Using NHANES 2013–2014 Data",
    "section": "2. Exploratory Distributions",
    "text": "2. Exploratory Distributions\n\n\nCode\nadult %&gt;%\nfilter(!is.na(diabetes_dx), !is.na(bmi)) %&gt;%\nmutate(diabetes_f = factor(diabetes_dx, levels = c(0,1), labels = c(\"No\",\"Yes\"))) %&gt;%\nggplot(aes(x = bmi, fill = diabetes_f)) +\ngeom_density(alpha = 0.6) +\nlabs(x = \"BMI (kg/m²)\", y = \"Density\", fill = \"Diabetes\", title = \"BMI by Diabetes Status\")\n\n\n\n\n\n\n\n\nFigure 7: BMI distribution by diabetes status (adult cohort, ≥20y).\n\n\n\n\n\n\n\nCode\nadult %&gt;%\nfilter(!is.na(diabetes_dx), !is.na(age)) %&gt;%\nmutate(diabetes_f = factor(diabetes_dx, levels = c(0,1), labels = c(\"No\",\"Yes\"))) %&gt;%\nggplot(aes(x = age, fill = diabetes_f)) +\ngeom_histogram(position = \"dodge\", alpha = 0.7, bins = 40, color = \"white\") +\nlabs(x = \"Age (years)\", y = \"Count\", fill = \"Diabetes\", title = \"Age by Diabetes Status\")\n\n\n\n\n\n\n\n\nFigure 8: Age distribution by diabetes status (adult cohort, ≥20y).\n\n\n\n\n\n\n\nCode\n# Correlation heatmap on the imputed+clean analysis data used by the Bayesian model\n\nstopifnot(exists(\"adult_imp1\"))\nlibrary(ggplot2)\n\ncorr_vars &lt;- subset(adult_imp1, select = c(diabetes_dx, age, bmi))\ncorr_mat  &lt;- cor(corr_vars, use = \"pairwise.complete.obs\", method = \"pearson\")\n\ncorr_long &lt;- reshape2::melt(corr_mat, varnames = c(\"Var1\",\"Var2\"), value.name = \"r\")\nggplot(corr_long, aes(Var1, Var2, fill = r)) +\ngeom_tile(color = \"white\") +\ngeom_text(aes(label = sprintf(\"%.2f\", r)), size = 4) +\nscale_fill_gradient2(low = \"steelblue\", mid = \"white\", high = \"firebrick\",\nmidpoint = 0, limits = c(-1, 1), name = \"r\") +\nlabs(title = \"Correlation Heatmap (adult_imp1)\", x = NULL, y = NULL) +\ntheme_minimal(base_size = 12) +\ntheme(axis.text.x = element_text(angle = 45, hjust = 1),\nlegend.position = \"right\")\n\n\n\n\n\n\n\n\nFigure 9: Pearson correlations among diabetes_dx (binary), age, and BMI in adult_imp1.\n\n\n\n\n\nCorrelation Note: The weighted Pearson correlation between age and BMI was 0.042."
  },
  {
    "objectID": "index.html#posterior-mean-probability-vs-bmi-bayesian",
    "href": "index.html#posterior-mean-probability-vs-bmi-bayesian",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk Using NHANES 2013–2014 Data",
    "section": "3. Posterior Mean Probability vs BMI (Bayesian)",
    "text": "3. Posterior Mean Probability vs BMI (Bayesian)\n\n\nCode\npp_df &lt;- tibble(\n  bmi_c = seq(min(adult$bmi_c, na.rm = TRUE), max(adult$bmi_c, na.rm = TRUE), length.out = 120),\n  age_c = mean(adult$age_c, na.rm = TRUE),\n  sex   = factor(\"Male\",  levels = levels(adult$sex)),\n  race3 = factor(\"White\", levels = levels(adult$race3))\n)\npp_pred &lt;- posterior_epred(bayes_fit, newdata = pp_df)  # draws x N\npp_df &lt;- pp_df %&gt;%\nmutate(\npred  = colMeans(pp_pred),\nlower = apply(pp_pred, 2, quantile, 0.025),\nupper = apply(pp_pred, 2, quantile, 0.975)\n)\nggplot(pp_df, aes(x = bmi_c, y = pred)) +\ngeom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.25) +\ngeom_line(linewidth = 1) +\nlabs(\n  title = \"Posterior Mean Diabetes Probability vs Standardized BMI (Bayesian Model)\",\n  subtitle = \"Predicted probability holding age = mean, sex = Male, race3 = White\",\n  x = \"Standardized BMI (bmi_c)\",\n  y = \"Predicted Probability (Posterior Mean)\"\n)\n\n\n\n\n\n\n\n\nFigure 10: Posterior mean predicted probability of diabetes vs standardized BMI (bmi_c), holding other covariates at reference/mean."
  },
  {
    "objectID": "index.html#priors-vs-posteriors-key-coefficients",
    "href": "index.html#priors-vs-posteriors-key-coefficients",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk Using NHANES 2013–2014 Data",
    "section": "4. Priors vs Posteriors (Key Coefficients)",
    "text": "4. Priors vs Posteriors (Key Coefficients)\n\n\nCode\nset.seed(1)\nprior_draws &lt;- tibble(\nterm  = rep(c(\"Age (per 1 SD)\", \"BMI (per 1 SD)\"), each = 4000),\nvalue = c(rnorm(4000, 0, 2.5), rnorm(4000, 0, 2.5)),\nsrc   = \"Prior\"\n)\npost_draws &lt;- posterior::as_draws_df(bayes_fit) %&gt;%\nselect(b_age_c, b_bmi_c) %&gt;%\npivot_longer(everything(), names_to = \"raw\", values_to = \"value\") %&gt;%\nmutate(term = recode(raw, b_age_c = \"Age (per 1 SD)\", b_bmi_c = \"BMI (per 1 SD)\"),\nsrc = \"Posterior\") %&gt;%\nselect(term, value, src)\nbind_rows(prior_draws, post_draws) %&gt;%\nggplot(aes(x = value, fill = src)) +\ngeom_density(alpha = 0.4) +\nfacet_wrap(~ term, scales = \"free\") +\nlabs(\n  title = \"Prior vs Posterior Distributions for Age and BMI Coefficients\",\n  subtitle = \"Bayesian logistic model using weakly informative priors (Normal(0, 2.5))\",\n  x = \"Coefficient Value\",\n  y = \"Density\"\n)\n\n\n\n\n\n\n\n\nFigure 11: Illustrative prior (Normal(0,2.5)) vs posterior for age and BMI coefficients."
  },
  {
    "objectID": "index.html#posterior-predictive-checks",
    "href": "index.html#posterior-predictive-checks",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk Using NHANES 2013–2014 Data",
    "section": "5. Posterior Predictive Checks",
    "text": "5. Posterior Predictive Checks\n\n\nCode\npp_check(bayes_fit, type = \"bars\") + ggtitle(\"Posterior Predictive Check: Outcome Distribution\")\npp_check(bayes_fit, type = \"stat\", stat = \"mean\") + ggtitle(\"Posterior Predictive Check: Mean Outcome\")\n\n\n\n\n\n\n\n\nFigure 12: Posterior predictive checks for outcome distribution and mean.\n\n\n\n\n\n\n\n\n\n\n\nFigure 13: Posterior predictive checks for outcome distribution and mean."
  },
  {
    "objectID": "index.html#observed-vs-predicted-calibration-style",
    "href": "index.html#observed-vs-predicted-calibration-style",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk Using NHANES 2013–2014 Data",
    "section": "6. Observed vs Predicted (Calibration-style)",
    "text": "6. Observed vs Predicted (Calibration-style)\n\n\nCode\nbayes_pred &lt;- posterior_epred(bayes_fit, newdata = adult_imp1)  # draws x N (fits' data)\npred_prob  &lt;- colMeans(bayes_pred)\nadult_plot &lt;- dplyr::mutate(adult_imp1, pred_prob = pred_prob)\n\nggplot2::ggplot(adult_plot, aes(x = pred_prob, y = as.numeric(diabetes_dx))) +\ngeom_jitter(height = 0.05, alpha = 0.25) +\ngeom_smooth(method = \"loess\", se = TRUE) +\nlabs(\ntitle = \"Observed vs Predicted Diabetes Probability\",\nsubtitle = \"Posterior mean predictions from Bayesian model — using fitted data\",\nx = \"Predicted Probability (Posterior Mean)\",\ny = \"Observed Outcome (0 = No, 1 = Yes)\"\n)\n\n\n\n\n\n\n\n\nFigure 14: Observed outcome vs posterior mean predicted probability (calibration-style view)."
  },
  {
    "objectID": "index.html#posterior-vs-survey-weighted-prevalence",
    "href": "index.html#posterior-vs-survey-weighted-prevalence",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk Using NHANES 2013–2014 Data",
    "section": "7. Posterior vs Survey-Weighted Prevalence",
    "text": "7. Posterior vs Survey-Weighted Prevalence\n\n\nCode\n# --- Design-based (survey) prevalence + 95% CI\n\nsvy_mean &lt;- svymean(~diabetes_dx, nhanes_design_adult, na.rm = TRUE)\npop_prev &lt;- as.numeric(svy_mean)\npop_se   &lt;- as.numeric(SE(svy_mean))\npop_ci   &lt;- c(pop_prev - 1.96 * pop_se, pop_prev + 1.96 * pop_se)\n\n# --- Model-based (Bayesian) posterior prevalence + 95% CrI\n\n# Predict on the SAME data used to fit (adult_imp1) to avoid NAs and factor-level issues\n\nbayes_pred &lt;- posterior_epred(bayes_fit, newdata = adult_imp1)  # draws x N\n\n# one prevalence per posterior draw:\n\npost_prev_draw &lt;- rowMeans(bayes_pred)\npost_prev      &lt;- mean(post_prev_draw)\npost_ci        &lt;- quantile(post_prev_draw, c(0.025, 0.975))\n\nstopifnot(is.finite(post_prev))  # guardrail\n\nbar_df &lt;- tibble::tibble(\nSource     = c(\"Bayesian Posterior\", \"Survey-weighted (Population)\"),\nPrevalence = c(post_prev, pop_prev),\nL          = c(post_ci[1], pop_ci[1]),\nU          = c(post_ci[2], pop_ci[2])\n)\n\nggplot2::ggplot(bar_df, aes(x = Source, y = Prevalence, fill = Source)) +\ngeom_col(alpha = 0.85, width = 0.65) +\ngeom_errorbar(aes(ymin = L, ymax = U), width = 0.15, linewidth = 0.7) +\nguides(fill = \"none\") +\nlabs(\ntitle = \"Population vs Posterior Diabetes Prevalence\",\nsubtitle = \"Survey-weighted estimate (design-based) vs Bayesian (model-based)\",\ny = \"Prevalence (Proportion with Diabetes)\",\nx = NULL\n) +\ntheme_minimal(base_size = 13)\n\n\n\n\n\n\n\n\nFigure 15: Comparison of survey-weighted population diabetes prevalence vs Bayesian posterior prevalence."
  },
  {
    "objectID": "Summaries/aw/paper6_summary.html",
    "href": "Summaries/aw/paper6_summary.html",
    "title": "Summary: Bayesian Model Averaging: A Practical Review",
    "section": "",
    "text": "Jennifer A. Hoeting, David Madigan, Adrian E. Raftery, & Chris T. Volinsky (1999)\n\n\nIn applied statistics and data science, analysts often face uncertainty about which model best represents the data. Standard practice is to select a single “best” model using criteria like AIC or BIC, but this ignores model uncertainty and leads to overconfident inferences and biased predictions. The paper addresses how to properly account for model uncertainty in statistical inference.\n\n\n\nThe authors propose Bayesian Model Averaging (BMA), a framework that combines predictions from multiple models weighted by their posterior probabilities. Instead of committing to a single model, BMA integrates over all plausible models using Bayes’ theorem. This approach accounts for uncertainty in both model selection and parameter estimation, yielding more robust predictions.\n\n\n\nBMA is shown to improve predictive performance and reduce overfitting across a variety of domains, including regression, classification, and time-series modeling. The method provides posterior model probabilities and model-averaged parameter estimates, offering a principled way to handle model uncertainty. The paper also presents examples and computational techniques for implementing BMA, such as Markov Chain Monte Carlo Model Composition (MC³).\n\n\n\nBMA can be computationally demanding when the number of candidate models is large, as it requires calculating and storing posterior probabilities for each. Its success also depends on reasonable prior specifications over both models and parameters. Simplified approximations (like Occam’s window) may introduce bias but are often necessary for feasibility.\n\n\n\nThe authors apply BMA to several real-world and simulated datasets, including linear regression examples and ecological modeling case studies. The paper focuses on demonstrating methodology rather than a specific dataset, emphasizing reproducible model-averaging workflows and sensitivity analysis."
  },
  {
    "objectID": "Summaries/aw/paper6_summary.html#problem-the-article-is-addressing",
    "href": "Summaries/aw/paper6_summary.html#problem-the-article-is-addressing",
    "title": "Summary: Bayesian Model Averaging: A Practical Review",
    "section": "",
    "text": "In applied statistics and data science, analysts often face uncertainty about which model best represents the data. Standard practice is to select a single “best” model using criteria like AIC or BIC, but this ignores model uncertainty and leads to overconfident inferences and biased predictions. The paper addresses how to properly account for model uncertainty in statistical inference."
  },
  {
    "objectID": "Summaries/aw/paper6_summary.html#how-it-has-been-solved",
    "href": "Summaries/aw/paper6_summary.html#how-it-has-been-solved",
    "title": "Summary: Bayesian Model Averaging: A Practical Review",
    "section": "",
    "text": "The authors propose Bayesian Model Averaging (BMA), a framework that combines predictions from multiple models weighted by their posterior probabilities. Instead of committing to a single model, BMA integrates over all plausible models using Bayes’ theorem. This approach accounts for uncertainty in both model selection and parameter estimation, yielding more robust predictions."
  },
  {
    "objectID": "Summaries/aw/paper6_summary.html#results",
    "href": "Summaries/aw/paper6_summary.html#results",
    "title": "Summary: Bayesian Model Averaging: A Practical Review",
    "section": "",
    "text": "BMA is shown to improve predictive performance and reduce overfitting across a variety of domains, including regression, classification, and time-series modeling. The method provides posterior model probabilities and model-averaged parameter estimates, offering a principled way to handle model uncertainty. The paper also presents examples and computational techniques for implementing BMA, such as Markov Chain Monte Carlo Model Composition (MC³)."
  },
  {
    "objectID": "Summaries/aw/paper6_summary.html#limitations",
    "href": "Summaries/aw/paper6_summary.html#limitations",
    "title": "Summary: Bayesian Model Averaging: A Practical Review",
    "section": "",
    "text": "BMA can be computationally demanding when the number of candidate models is large, as it requires calculating and storing posterior probabilities for each. Its success also depends on reasonable prior specifications over both models and parameters. Simplified approximations (like Occam’s window) may introduce bias but are often necessary for feasibility."
  },
  {
    "objectID": "Summaries/aw/paper6_summary.html#datasets",
    "href": "Summaries/aw/paper6_summary.html#datasets",
    "title": "Summary: Bayesian Model Averaging: A Practical Review",
    "section": "",
    "text": "The authors apply BMA to several real-world and simulated datasets, including linear regression examples and ecological modeling case studies. The paper focuses on demonstrating methodology rather than a specific dataset, emphasizing reproducible model-averaging workflows and sensitivity analysis."
  },
  {
    "objectID": "Summaries/aw/paper4_summary.html",
    "href": "Summaries/aw/paper4_summary.html",
    "title": "Summary: Dropout as a Bayesian Approximation—Representing Model Uncertainty in Deep Learning",
    "section": "",
    "text": "Yarin Gal & Zoubin Ghahramani (2016)\n\n\nDeep neural networks achieve strong predictive accuracy but typically do not quantify uncertainty. This limitation is critical in applications like healthcare or autonomous systems, where overconfidence can lead to serious consequences. Traditional Bayesian neural networks can model uncertainty but are often computationally expensive and hard to train.\n\n\n\nThe authors demonstrate that standard dropout, a common regularization technique, can serve as an approximate Bayesian inference method. By keeping dropout active during both training and testing and performing multiple stochastic forward passes, the resulting distribution of predictions can be interpreted as samples from a posterior. This provides an efficient, scalable way to estimate model uncertainty without altering model architecture.\n\n\n\nThe paper shows that “Monte Carlo Dropout” (MC-Dropout) yields well-calibrated uncertainty estimates and competitive accuracy on tasks such as image classification (MNIST, CIFAR-10) and regression. The method improves out-of-distribution detection, active learning, and decision-making under uncertainty.\n\n\n\nThe uncertainty estimates depend on the dropout rate and can underestimate epistemic uncertainty for small or biased datasets. The approach is an approximation, not a full Bayesian posterior, so results may deviate from exact inference. Additional calibration steps may still be required.\n\n\n\nExperiments include standard image datasets (MNIST, CIFAR-10) and regression benchmarks from the UCI repository. Predictions are made using multiple forward passes with dropout enabled to approximate posterior mean and variance."
  },
  {
    "objectID": "Summaries/aw/paper4_summary.html#problem-the-article-is-addressing",
    "href": "Summaries/aw/paper4_summary.html#problem-the-article-is-addressing",
    "title": "Summary: Dropout as a Bayesian Approximation—Representing Model Uncertainty in Deep Learning",
    "section": "",
    "text": "Deep neural networks achieve strong predictive accuracy but typically do not quantify uncertainty. This limitation is critical in applications like healthcare or autonomous systems, where overconfidence can lead to serious consequences. Traditional Bayesian neural networks can model uncertainty but are often computationally expensive and hard to train."
  },
  {
    "objectID": "Summaries/aw/paper4_summary.html#how-it-has-been-solved",
    "href": "Summaries/aw/paper4_summary.html#how-it-has-been-solved",
    "title": "Summary: Dropout as a Bayesian Approximation—Representing Model Uncertainty in Deep Learning",
    "section": "",
    "text": "The authors demonstrate that standard dropout, a common regularization technique, can serve as an approximate Bayesian inference method. By keeping dropout active during both training and testing and performing multiple stochastic forward passes, the resulting distribution of predictions can be interpreted as samples from a posterior. This provides an efficient, scalable way to estimate model uncertainty without altering model architecture."
  },
  {
    "objectID": "Summaries/aw/paper4_summary.html#results",
    "href": "Summaries/aw/paper4_summary.html#results",
    "title": "Summary: Dropout as a Bayesian Approximation—Representing Model Uncertainty in Deep Learning",
    "section": "",
    "text": "The paper shows that “Monte Carlo Dropout” (MC-Dropout) yields well-calibrated uncertainty estimates and competitive accuracy on tasks such as image classification (MNIST, CIFAR-10) and regression. The method improves out-of-distribution detection, active learning, and decision-making under uncertainty."
  },
  {
    "objectID": "Summaries/aw/paper4_summary.html#limitations",
    "href": "Summaries/aw/paper4_summary.html#limitations",
    "title": "Summary: Dropout as a Bayesian Approximation—Representing Model Uncertainty in Deep Learning",
    "section": "",
    "text": "The uncertainty estimates depend on the dropout rate and can underestimate epistemic uncertainty for small or biased datasets. The approach is an approximation, not a full Bayesian posterior, so results may deviate from exact inference. Additional calibration steps may still be required."
  },
  {
    "objectID": "Summaries/aw/paper4_summary.html#datasets",
    "href": "Summaries/aw/paper4_summary.html#datasets",
    "title": "Summary: Dropout as a Bayesian Approximation—Representing Model Uncertainty in Deep Learning",
    "section": "",
    "text": "Experiments include standard image datasets (MNIST, CIFAR-10) and regression benchmarks from the UCI repository. Predictions are made using multiple forward passes with dropout enabled to approximate posterior mean and variance."
  },
  {
    "objectID": "Summaries/aw/paper2_summary.html",
    "href": "Summaries/aw/paper2_summary.html",
    "title": "Summary: An Introduction to Using Bayesian Linear Regression with Clinical Data",
    "section": "",
    "text": "Scott A. Baldwin and Michael J. Larson (2017)\n\n\nTraditional statistics training in psychology emphasizes frequentist methods, and researchers often feel unsure about Bayesian alternatives. This paper aims to introduce Bayesian linear regression in a clear and practical way, using real clinical data to help researchers understand and apply Bayesian tools.\n\n\n\nThe authors walk readers through setting up practical Bayesian regression models. They explain how to choose and justify priors, run models using software (like R and Stan), check convergence, and interpret results—including posterior distributions, credible intervals, and predictive outputs. They also compare Bayesian and frequentist approaches and offer useful model-comparison tools. The article includes EEG-anxiety data, and it comes with the data and R code for easy replication.\n\n\n\nThe paper shows how Bayesian regression can be applied step-by-step and how outputs like posterior distributions and interval estimates offer more intuitive, flexible insights than traditional methods. Including data and code makes the method transparent and replicable, while model diagnostics (like convergence checks) ensure reliability.\n\n\n\nThe paper is introductory; it doesn’t dive into advanced modeling extensions or deeply mathematical derivations. It focuses on a single case study (EEG data and anxiety) so broader generalization requires additional application. Expertise is still needed to adapt methods to other complex datasets.\n\n\n\nThe authors illustrate methods using clinical EEG data related to error-related negativity (ERN) and trait anxiety. They provide both the actual dataset and R/Stan code for readers to replicate and learn directly from the example."
  },
  {
    "objectID": "Summaries/aw/paper2_summary.html#problem-the-article-is-addressing",
    "href": "Summaries/aw/paper2_summary.html#problem-the-article-is-addressing",
    "title": "Summary: An Introduction to Using Bayesian Linear Regression with Clinical Data",
    "section": "",
    "text": "Traditional statistics training in psychology emphasizes frequentist methods, and researchers often feel unsure about Bayesian alternatives. This paper aims to introduce Bayesian linear regression in a clear and practical way, using real clinical data to help researchers understand and apply Bayesian tools."
  },
  {
    "objectID": "Summaries/aw/paper2_summary.html#how-it-has-been-solved",
    "href": "Summaries/aw/paper2_summary.html#how-it-has-been-solved",
    "title": "Summary: An Introduction to Using Bayesian Linear Regression with Clinical Data",
    "section": "",
    "text": "The authors walk readers through setting up practical Bayesian regression models. They explain how to choose and justify priors, run models using software (like R and Stan), check convergence, and interpret results—including posterior distributions, credible intervals, and predictive outputs. They also compare Bayesian and frequentist approaches and offer useful model-comparison tools. The article includes EEG-anxiety data, and it comes with the data and R code for easy replication."
  },
  {
    "objectID": "Summaries/aw/paper2_summary.html#results",
    "href": "Summaries/aw/paper2_summary.html#results",
    "title": "Summary: An Introduction to Using Bayesian Linear Regression with Clinical Data",
    "section": "",
    "text": "The paper shows how Bayesian regression can be applied step-by-step and how outputs like posterior distributions and interval estimates offer more intuitive, flexible insights than traditional methods. Including data and code makes the method transparent and replicable, while model diagnostics (like convergence checks) ensure reliability."
  },
  {
    "objectID": "Summaries/aw/paper2_summary.html#limitations",
    "href": "Summaries/aw/paper2_summary.html#limitations",
    "title": "Summary: An Introduction to Using Bayesian Linear Regression with Clinical Data",
    "section": "",
    "text": "The paper is introductory; it doesn’t dive into advanced modeling extensions or deeply mathematical derivations. It focuses on a single case study (EEG data and anxiety) so broader generalization requires additional application. Expertise is still needed to adapt methods to other complex datasets."
  },
  {
    "objectID": "Summaries/aw/paper2_summary.html#datasets",
    "href": "Summaries/aw/paper2_summary.html#datasets",
    "title": "Summary: An Introduction to Using Bayesian Linear Regression with Clinical Data",
    "section": "",
    "text": "The authors illustrate methods using clinical EEG data related to error-related negativity (ERN) and trait anxiety. They provide both the actual dataset and R/Stan code for readers to replicate and learn directly from the example."
  },
  {
    "objectID": "Summaries/nm/Comparing conventional and Bayesian.html",
    "href": "Summaries/nm/Comparing conventional and Bayesian.html",
    "title": "Bayesian Logistic Regression Capstone",
    "section": "",
    "text": "Comparing conventional and Bayesian workflows for clinical outcome prediction modelling with an exemplar cohort study of severe COVID-19 infection incorporating clinical biomarker test results\nRef: Sullivan, B., Barker, E., MacGregor, L. et al. Comparing conventional and Bayesian workflows for clinical outcome prediction modelling with an exemplar cohort study of severe COVID-19 infection incorporating clinical biomarker test results. BMC Med Inform Decis Mak 25, 123 (2025). https://doi.org/10.1186/s12911-025-02955-3\nProblem: Data curation is challenging as clinical data are heterogeneous in multiple ways. Biomarkers are recorded for different reasons. If missing data imputation is performed, it raises another decision point on whether to impute the continuous or the transformed categorical data.\nAim: Estimating predictive risk factors for disease outcomes with explainable statistical models is desirable for clinical use and decision making. The author provide a guide for modern Bayesian approaches for joint risk factor analysis and variable selection.\nStudy design: Retrospective observational cohort design, lab data linked to the patient data for laboratory markers and clinical outcomes from three hospitals in the Southwest region of England, UK on adults admitted between March to October 2020 and tested positive for SARS CoV-2 by PCR.\nMethod: Analyze range of laboratory blood marker values, Develop cross-validated logistic regression prediction models using the candidate biomarkers, highlighting biomarkers worthy of future research. Employed selection techniques, comparing LASSO frequentist method and Projective Prediction approach on Bayesian logistic regression models with horseshoe priors to illustrate the process of creating a reduced model. They considered models deliver good prediction performance with a small amount of biomarker data.\nVairables (1) Predictors: Includes a variety of clinical severity indices, lab biomarkers (microbiological, immunological, haematological and biochemistry) as parameters used as predictive variables in the regression models.\n\nOutcome: The primary prediction outcome was death or transfer to the ICU.\n\nData management: Continuous biomarkers were trannsfromed into categorical variables (reference ranges for clinical use).\nStaistical calcukation: Analytics carried out using the R statistical language using- Standard logistic regression analyses used the R Stats GLM package (v4.4.1); LASSO analyses, GLMnet (v4.1.8); and for Bayesian analyses, BRMS (v2.22.0) and ProjPred (v2.8.0).\nBefore running full regression models, the independent contribution of individual biomarkers were examined in the training dataset predicting ICU entry or death via standard logistic regressions and Bayesian logistic regressions with either a flat (aka uniform) or horseshoe prior and calculated p-values and odds ratios for each biomarker.\nPer biomarker, patients with and without the outcome were separated and then these groups were shuffled and split into 5 equal subgroups. These groups were then paired at random, to ensure training and test datasets have the same proportion of patients with a severe outcome as in full the sample for that biomarker. This was to improve the chance of convergence for biomarkers with high data missingness and only complete cases of training data available for each biomarker were considered for the study.\nEach individual biomarker model including age and gender (except univariate age and gender models) were compared against a standard model including only age and gender. Regressions were fit using all associated dummy variables for a given biomarker (e.g. ‘Mild’, ‘Moderate’, ‘Severe’) using ‘Normal’ as the reference.\nAnalysis using all valid biomarker data: After individual biomarker evaluation, logistic regression models considering all valid biomarkers (Prediction using individual variables section) and demographic variables were fit to the data and the predictions were tested via internal and external validation using the stratified cross-validation procedures.\nAnalysis using reduced variable models: Considering that eben though a model using all biomarker data may have strong predictive power, but clinically a strong prediction with the least amount of biomarkers might save on time, money and other resources, they used LASSO and Bayesian Projective Prediction methodologies to choose reduced variable models to predict COVID-19 severe outcomes.\nTo estimate variability in model performance and allow comparison between models, we compute inter-quantile AUC difference ranges using 5-fold 20-repeat cross-validation of models\nReduced variable models: LASSO and projective prediction performed for creation of reduced models with fewer biomarkers.\nModel performance evaluation and dissemination They chose to peform cross-validated estimates of AUC, sensitivity, and specificity and the Inter-quartile intervals over these measures.\nRecommendations: Categorization is worth critical consideration in model planning Reduced number of variables Imputation Bayesian approaches should include that coefficients estimated via Bayes should on average deliver better predictive performance than standard GLM"
  },
  {
    "objectID": "Summaries/nm/Summaries.html",
    "href": "Summaries/nm/Summaries.html",
    "title": "Bayesian Logistic Regression Capstone",
    "section": "",
    "text": "All 6 summaries and references & citation"
  },
  {
    "objectID": "Summaries/nm/Summaries.html#namitas-literature",
    "href": "Summaries/nm/Summaries.html#namitas-literature",
    "title": "Bayesian Logistic Regression Capstone",
    "section": "Namita’s Literature",
    "text": "Namita’s Literature"
  },
  {
    "objectID": "Summaries/nm/Summaries.html#introduction",
    "href": "Summaries/nm/Summaries.html#introduction",
    "title": "Bayesian Logistic Regression Capstone",
    "section": "Introduction",
    "text": "Introduction\n\nBayesian Hierarchical Model (Disease reclassification and prediction)\n\nWhat is the goal of the paper?\nThe authors develop a Bayesian hierarchical model for multivariate longitudinal data to predict health status, trajectories, and intervention effects at the individual level in the PCORI mission to address questions about health status from patients and clinicians.\nWhy is it important?\nHealthcare data (DNA sequences, functional images of the brain, patient-reported outcomes, and electronic health records with patients’ sequences of health measurements, diagnoses, and treatments) are complex, and the standard approaches are not adequate for clinical data analysis. Electronic health records (EHRs) could improve diagnostic accuracy and predict treatment effects. Visualizations of characteristics of posterior distributions can be immediately understood by clinicians and patients as relevant to their decision. Combining prior knowledge and patient data with evidence could predict the patient’s health status, trajectory, and/or likely benefits of interventions.\nHow is it solved?\nMethod: The authors applied Bayesian hierarchical regression for multivariate longitudinal patient data using open-source R-packages and developed 2 levels—time within person and persons within a population\nThe model combined exogenous (eg, age, clinical history) factors and endogenous (eg, current treatment) variables on the individual’s multivariate health measurements and the effects of health measurements at one time on subsequent interventions.\nThe model produced an estimate of the posterior distribution for each value of the predictor variable and an estimate of the marginal distribution of the regression coefficients for each coefficient that measures the outcome (health status) associated with its predictor variables. In a larger sample, the likelihood dominates the prior distribution for regression coefficients and Bayesian hierarchical model used a likelihood-based approach, used priors (prior laboratory and clinical trials data) that provided the assay sensitivities, which through the prior assumptions, made the model identifiable and the integration of Markov chain Monte Carlo (MCMC) estimates the posterior distributions, avoided missing data and complex outcome measurements.\nResults/limitations\nThree case studies: pneumonia etiology in children, prostate cancer, and mental disorders chosen for model development, identified low-risk patient population, reduced the risk of overtreatment, complications, adverse effects, and financial burden for patients (Disease Reclassification). Prostate cancer software was then implemented within the JHM HER.\nLimitation:\nModels were entirely parametric, and extensions to nonparametric or more flexible parametric models were recommended to improve approaches for neuroimage or genomic data.\nApplications:\n\nto scale a tool that addresses a particular unmet need across a larger, more diverse population of patients and clinicians\nuse in autoimmune diseases, sudden cardiac arrest, and diabetes.\nembed a collection of tools to acquire and use the most relevant information, agnostic to its level of measurement, to improve population and individual health decisions that cause better outcomes at more affordable costs. @Zeger2020\n\n\nBayesian Inference (parametric vs non-parametric)\n\nWhat is the goal of the paper?\nThe authors calculated the posterior probability of disease diagnosis and applied Bayesian inference to develop three modules comparing parametric (with a fixed set of parameters) and nonparametric distributions (which do not make a priori assumptions) by analyzing the National Health and Nutrition Examination Survey dataset from two separate diagnostic tests on both diseased and non-diseased populations.\nWhy is it important?\nMedical diagnosis, treatment, and management decisions are crucial, and conventional methods for diagnosis using clinical criteria and fixed numerical thresholds limit the capture of other information related to the intricate relationship between diagnostic tests and the varying prevalence of diseases. The probability distributions associated with quantitative diagnostic test outcomes often demonstrate some overlap between the diseased and nondiseased groups. The dichotomous method fails to capture the complexity and heterogeneity of disease presentations across diverse populations. The applicability of the normal distribution (conventional method) is critiqued, especially in dealing with clinical measurands having skewness, bimodality, or multimodality.\nHow is it solved?\nMethods: The Authors developed models employing Bayesian inference (Bayesian diagnostic approach) to calculate the posterior probability of @Liu2013"
  },
  {
    "objectID": "Summaries/nm/r code.html",
    "href": "Summaries/nm/r code.html",
    "title": "making subsets for each dataset",
    "section": "",
    "text": "NHANES DATASET -https://wwwn.cdc.gov/nchs/nhanes/continuousnhanes/default.aspx?BeginYear=2013 # loading packages\nlibrary(tidyverse) library(knitr) library(ggthemes) library(ggrepel) library(dslabs) library(Hmisc) library(dplyr) library(tidyr) library(forcats) library(ggplot2) library (“nhanesA”)\noptions(repos = c(CRAN = “https://cloud.r-project.org”))\ninstall.packages(“nhanesA”)\n\nmaking subsets for each dataset\n                   nhanesTables('EXAM', 2013)\n                   nhanesTables('QUESTIONNAIRE', 2013)\n                   nhanesTables('DEMOGRAPHICS', 2013)\n                   \n                   \nnhanesCodebook(“BMX_H”) nhanesCodebook(“SMQ_H”)\n# .xpt files read ( 2013–2014) bmx_h &lt;- nhanes(“BMX_H”) #Exam smq_h &lt;- nhanes(“SMQ_H”) #Quest demo_h &lt;- nhanes(“DEMO_H”) #Demo diq_h &lt;- nhanes(“DIQ_H”) #diabetes\n\n\nvariables of interest\nexam_sub &lt;- bmx_h %&gt;% select(SEQN, BMDBMIC) demo_sub &lt;- demo_h %&gt;% select(SEQN, RIDAGEYR, RIAGENDR, RIDRETH1, SDMVPSU, SDMVSTRA, WTMEC2YR) diq_sub &lt;- diq_h %&gt;% select (SEQN, DIQ240)\n\n\nNames of all variables selected for analysis\nnames(exam_sub) names(demo_sub) names(diq_sub)\n\n\nmerged dataframe\nmerged_data &lt;- exam_sub %&gt;% left_join(demo_sub, by = “SEQN”) %&gt;% left_join(diq_sub, by = “SEQN”) head(merged_data)\nnhanesCodebook(“DEMO_H”,‘RIDRETH1’) nhanesCodebook(“DEMO_H”,‘RIAGENDR’) nhanesCodebook(“DEMO_H”,‘RIDAGEYR’) nhanesCodebook(“DIQ_H”,“DIQ240”) nhanesCodebook(“BMX_H”,‘BMDBMIC’)"
  },
  {
    "objectID": "Summaries/nm/My_references.html",
    "href": "Summaries/nm/My_references.html",
    "title": "Bayesian Logistic Regression Capstone",
    "section": "",
    "text": "@article{Zeger2020, abstract = {The PCORI mission is to address questions about health care from the patients’ perspective, such as “What is my health status and its trajectory?” and “What are my treatment options and the expected benefits and harms of each?” The purpose of this PCORI-funded project is to make it easier for clinicians and patients to find valid answers to these and other clinical questions by using modern digital tools that support (1) learning from the experience of prior patients, and (2) translating what is learned to inform the decision at hand, taking into account each patient’s unique circumstances. For this project, we developed and implemented statistical methods called bayesian hierarchical models that combine existing data on past clinical experience from a reference population with new measurements for the individual. Clinicians currently use such methods when screening patients for disease. Modern technologies make it possible for this proven approach to extend far beyond its current use. The recent revolution in information technology has unleashed new types of health data, from DNA sequences to functional images of the brain to patient-reported outcomes. Furthermore, the electronic health record captures every patient’s sequence of health measurements, diagnoses, and treatments. The bayesian methods developed and reported on here combine even complex data to produce predictions about an individual patient’s health status, trajectory, and likely benefits and harms of interventions. In addition to developing novel methods, we facilitated their use by creating and locally disseminating a software package, OSLER inHealth, that will allow other researchers to apply this methodology. The software repository is open-source and includes the methodology developed as part of this research as well as other existing methods that facilitate individualized health prediction. We have tested the proposed methods and software on 3 case studies to (1) estimate the frequency with which various pathogens cause children’s pneumonia and predict which pathogen is likely to be causing a particular child’s pneumonia given her or his clinical data, potentially reducing unnecessary use of antibiotics; (2) infer whether a prostate cancer is indolent or aggressive for a patient under active surveillance; and (3) characterize the variation in multiple, time-varying symptoms of major mental disorders, including schizophrenia and depression, and then use this knowledge to provide patient-specific estimates of past and, likely, future trajectories. With this project, we have developed and demonstrated the value of combining even complex measurements on a population of patients, then translating this experience into more valid assessments of a new patient’s health status and trajectory. The model also supports inferences about the likely benefits and harms associated with available interventions. Copyright {} 2020. Johns Hopkins Bloomberg School of Public Health. All Rights Reserved.}, author = {Zeger, Scott L and Wu, Zhenke and Coley, Yates and Fojo, Anthony Todd and Carter, Bal and O’Brien, Katherine and Zandi, Peter and Cooke, Mary and Carey, Vince and Crainiceanu, Ciprian and Muscelli, John and Gherman, Adrian and Mekosh, Jason}, mendeley-groups = {CapStone_2025/CapStone_DS_2025}, number = {2020}, title = {{Using a Bayesian Approach to Predict Patients’ Health and Response to Treatment}}, url = {http://ovidsp.ovid.com/ovidweb.cgi?T=JS&PAGE=reference&D=medp&NEWS=N&AN=37708307}, year = {2020} }\n@article{Chatzimichail2023, abstract = {Medical diagnosis is the basis for treatment and management decisions in healthcare. Conventional methods for medical diagnosis commonly use established clinical criteria and fixed numerical thresholds. The limitations of such an approach may result in a failure to capture the intricate relations between diagnostic tests and the varying prevalence of diseases. To explore this further, we have developed a freely available specialized computational tool that employs Bayesian inference to calculate the posterior probability of disease diagnosis. This novel software comprises of three distinct modules, each designed to allow users to define and compare parametric and nonparametric distributions effectively. The tool is equipped to analyze datasets generated from two separate diagnostic tests, each performed on both diseased and nondiseased populations. We demonstrate the utility of this software by analyzing fasting plasma glucose, and glycated hemoglobin A1c data from the National Health and Nutrition Examination Survey. Our results are validated using the oral glucose tolerance test as a reference standard, and we explore both parametric and nonparametric distribution models for the Bayesian diagnosis of diabetes mellitus.}, author = {Chatzimichail, Theodora and Hatjimihail, Aristides T.}, doi = {10.3390/DIAGNOSTICS13193135,}, file = {:C:/Users/Namita/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chatzimichail, Hatjimihail - 2023 - A Bayesian Inference Based Computational Tool for Parametric and Nonparametric Medical Diagnosis.pdf:pdf}, issn = {20754418}, journal = {Diagnostics}, keywords = {Bayesian diagnosis,Bayesian inference,copula distribution,diabetes mellitus,kernel density estimator,likelihood,nonparametric distribution,parametric distribution,posterior probability,prior probability,probability density function}, mendeley-groups = {CapStone_2025}, month = {oct}, number = {19}, publisher = {Multidisciplinary Digital Publishing Institute (MDPI)}, title = {{A Bayesian Inference Based Computational Tool for Parametric and Nonparametric Medical Diagnosis}}, url = {https://pubmed.ncbi.nlm.nih.gov/37835877/}, volume = {13}, year = {2023} }\n@article{VandeSchoot2021, abstract = {Bayesian statistics is an approach to data analysis based on Bayes’ theorem, where available knowledge about parameters in a statistical model is updated with the information in observed data. The background knowledge is expressed as a prior distribution and combined with observational data in the form of a likelihood function to determine the posterior distribution. The posterior can also be used for making predictions about future events. This Primer describes the stages involved in Bayesian analysis, from specifying the prior and data models to deriving inference, model checking and refinement. We discuss the importance of prior and posterior predictive checking, selecting a proper technique for sampling from a posterior distribution, variational inference and variable selection. Examples of successful applications of Bayesian analysis across various research fields are provided, including in social sciences, ecology, genetics, medicine and more. We propose strategies for reproducibility and reporting standards, outlining an updated WAMBS (when to Worry and how to Avoid the Misuse of Bayesian Statistics) checklist. Finally, we outline the impact of Bayesian analysis on artificial intelligence, a major goal in the next decade.}, author = {van de Schoot, Rens and Depaoli, Sarah and King, Ruth and Kramer, Bianca and M{\"{a}}rtens, Kaspar and Tadesse, Mahlet G. and Vannucci, Marina and Gelman, Andrew and Veen, Duco and Willemsen, Joukje and Yau, Christopher}, doi = {10.1038/s43586-020-00001-2}, file = {:C:/Users/Namita/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/van de Schoot et al. - 2021 - Bayesian statistics and modelling.pdf:pdf}, issn = {2662-8449}, journal = {Nature Reviews Methods Primers}, keywords = {Scientific community,Statistics}, mendeley-groups = {CapStone_2025}, month = {jan}, number = {1}, pages = {1}, publisher = {Springer Nature}, title = {{Bayesian statistics and modelling}}, url = {https://www.nature.com/articles/s43586-020-00001-2}, volume = {1}, year = {2021} }\n@article{Klauenberg2015, abstract = {Regression is a common task in metrology and often applied to calibrate instruments, evaluate inter-laboratory comparisons or determine fundamental constants, for example. Yet, a regression model cannot be uniquely formulated as a measurement function, and consequently the Guide to the Expression of Uncertainty in Measurement (GUM) and its supplements are not applicable directly. Bayesian inference, however, is well suited to regression tasks, and has the advantage of accounting for additional a priori information, which typically robustifies analyses. Furthermore, it is anticipated that future revisions of the GUM shall also embrace the Bayesian view. Guidance on Bayesian inference for regression tasks is largely lacking in metrology. For linear regression models with Gaussian measurement errors this tutorial gives explicit guidance. Divided into three steps, the tutorial first illustrates how a priori knowledge, which is available from previous experiments, can be translated into prior distributions from a specific class. These prior distributions have the advantage of yielding analytical, closed form results, thus avoiding the need to apply numerical methods such as Markov Chain Monte Carlo. Secondly, formulas for the posterior results are given, explained and illustrated, and software implementations are provided. In the third step, Bayesian tools are used to assess the assumptions behind the suggested approach. These three steps (prior elicitation, posterior calculation, and robustness to prior uncertainty and model adequacy) are critical to Bayesian inference. The general guidance given here for Normal linear regression tasks is accompanied by a simple, but real-world, metrological example. The calibration of a flow device serves as a running example and illustrates the three steps. It is shown that prior knowledge from previous calibrations of the same sonic nozzle enables robust predictions even for extrapolations.}, author = {Klauenberg, Katy and W{\"{u}}bbeler, Gerd and Mickan, Bodo and Harris, Peter and Elster, Clemens}, doi = {10.1088/0026-1394/52/6/878}, file = {:C:/Users/Namita/Downloads/Klauenberg_2015_Metrologia_52_878.pdf:pdf;:C:/Users/Namita/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Klauenberg et al. - 2015 - A tutorial on Bayesian Normal linear regression.pdf:pdf}, issn = {16817575}, journal = {Metrologia}, keywords = {Bayesian inference,Gaussian measurement error,Normal inverse Gamma distribution,conjugate prior distribution,linear regression,prior knowledge,sonic nozzle calibration}, mendeley-groups = {CapStone_2025}, number = {6}, pages = {878–892}, publisher = {IOP Publishing}, title = {{A tutorial on Bayesian Normal linear regression}}, volume = {52}, year = {2015} }\n@article{DeLeeuw2012a, abstract = {In most research, linear regression analyses are performed without taking into account published results (i.e., reported summary statistics) of similar previous studies. Although the prior density in Bayesian linear regression could accommodate such prior knowledge, formal models for doing so are absent from the literature. The goal of this article is therefore to develop a Bayesian model in which a linear regression analysis on current data is augmented with the reported regression coefficients (and standard errors) of previous studies. Two versions of this model are presented. The first version incorporates previous studies through the prior density and is applicable when the current and all previous studies are exchangeable. The second version models all studies in a hierarchical structure and is applicable when studies are not exchangeable. Both versions of the model are assessed using simulation studies. Performance for each in estimating the regression coefficients is consistently superior to using current data alone and is close to that of an equivalent model that uses the data from previous studies rather than reported regression coefficients. Overall the results show that augmenting data with results from previous studies is viable and yields significant improvements in the parameter estimation. {} 2012 Copyright Taylor and Francis Group, LLC.}, author = {de Leeuw, Christiaan and Klugkist, Irene}, doi = {10.1080/00273171.2012.673957}, file = {:C:/Users/Namita/Downloads/Augmenting Data With Published Results in Bayesian Linear Regression.pdf:pdf}, issn = {00273171}, journal = {Multivariate Behavioral Research}, mendeley-groups = {CapStone_2025}, number = {3}, pages = {369–391}, title = {{Augmenting Data With Published Results in Bayesian Linear Regression}}, volume = {47}, year = {2012} }\n@article{Liu2013, abstract = {Background: A Bayesian clinical reasoning model was developed to predict an individual risk for cardiovascular disease (CVD) for desk-top reference. Methods: Three Bayesian models were constructed to estimate the CVD risk by sequentially incorporating demographic features (basic), six metabolic syndrome components (metabolic score) and conventional risk factors (enhanced model). By considering clinical weights (regression coefficients) of each model as normal distribution, individual risk can be predicted making allowance for uncertainty of clinical weights. A community-based cohort that enrolled 64,489 participants free of CVD at baseline and followed up over five years to ascertain newly diagnosed CVD cases during the period through 2000 to 2004 was used for the illustration of the three proposed models (full empirical data are available from website http://homepage.ntu.edu.tw/\\(\\sim\\)chenlin/CVD-prediction-data.rar). Results: The proposed models can be applied to predicting the CVD risk with any combination of risk factors. For a 47-year-old man, the five-year risk for CVD with the basic model was 11.2% (95% CI: 7.8%-15.6%). His metabolic syndrome score, leading to 1.488 of likelihood ratio, enhanced the risk for CVD up to 15.8% (95% CI: 11.0%-21.5%) and put him in highest deciles. As with the habit of smoking over 2 packs per-day and family history of CVD, yielding the likelihood ratios of 1.62 and 1.47, respectively, the risk was further raised to 30.9% (95% CI: 20.7%-39.8%). Conclusions: We demonstrate how to make individual risk prediction for CVD by incorporating routine information with a sequential Bayesian clinical reasoning approach. {} 2012 Elsevier Ireland Ltd.}, author = {Liu, Yi Ming and Chen, Sam Li Sheng and Yen, Amy Ming Fang and Chen, Hsiu Hsi}, doi = {10.1016/J.IJCARD.2012.05.016}, file = {:C:/Users/Namita/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu et al. - 2013 - Individual risk prediction model for incident cardiovascular disease A Bayesian clinical reasoning approach(9).pdf:pdf}, issn = {0167-5273}, journal = {International Journal of Cardiology}, keywords = {Bayes’ theorem,Bayesian,Cardiovascular disease,Likelihood ratio,Metabolic syndrome,Prediction model}, mendeley-groups = {CapStone_2025}, month = {sep}, number = {5}, pages = {2008–2012}, pmid = {22658349}, publisher = {Elsevier}, title = {{Individual risk prediction model for incident cardiovascular disease: A Bayesian clinical reasoning approach}}, url = {https://www.sciencedirect.com/science/article/pii/S0167527312006274}, volume = {167}, year = {2013} }"
  },
  {
    "objectID": "Summaries/et/paper5.html",
    "href": "Summaries/et/paper5.html",
    "title": "A Dynamic Bayesian Model for Identifying High-Mortality Risk in Hospitalized COVID-19 Patients",
    "section": "",
    "text": "Amir Momeni-Boroujeni, Rachelle Mendoza, Isaac J. Stopard, Ben Lambert, and Alejandro Zuretti\nInfect. Dis. Rep. 2021, 13, 239–250. https://doi.org/10.3390/idr13010027\n\n\nTHis paper describes a study that used a Bayesian Markov model to better predict mortality risk for hospitalized COVID-19 patients by incorporating dynamic changes in laboratory values over time, rather than relying solely on admission data. The researchers collected demographic, comorbidity, and lab data for 553 PCR-positive patients and found that factors like age over 80 and certain comorbidities increased risk, but including dynamic changes in biomarkers significantly improved the predictive accuracy of the model. The study concludes by presenting a clinical decision tool that uses the most important factors for patient risk stratification based on available information at different stages of hospitalization.\n\n\n\nThe central problem this research aims to solve is how to accurately and quickly predict which hospitalized COVID-19 patients face the highest risk of death. Because hospitalization rates remained high and the disease caused millions of deaths globally, there was an urgent need to identify factors that predict severe disease and mortality to improve patient care and outcomes. The developed model is intended to allow for prioritization at the systems level and the individualization of care for each patient.\nA major shortcoming of previous prognostic tools was that they were static. Existing models generally looked at patient data and biomarkers only at the time of hospital admission to predict a single outcome, like death, at a single future time point. However, the sources note that a patient’s risk changes constantly during hospitalization, as biomarkers show outcome-specific dynamic changes. Existing models struggled to incorporate these dynamic changes and had difficulty handling the competing risks a patient faces, like remaining in the hospital, being discharged, or dying. Failing to account for these ongoing changes meant that risk predictions were often inaccurate in real-time clinical settings.\nTherefore, the authors developed a “Dynamic Bayesian Model” to overcome these limitations. This new model combines a patient’s initial information (demographics and comorbidities) with daily dynamic changes in laboratory test values throughout their hospital stay. By incorporating these time-dependent measurements, the model achieved dramatic improvements in predictive accuracy compared to models that relied only on admission data. This allows the model to provide daily adjustments to the patient’s in-hospital mortality risk, making it a more effective tool for clinicians deciding on appropriate care and resource allocation.\n\n\n\n\nCase selection\n\nPatients admitted to SUNY Downstate Medical Center, with COVID-19 related symptoms, between February 2020 and March 2020.\n\nSample\n\n553 PCR-positive patients included in the study.\nStratified into two groups: 200 patients who were discharged and 200 patients who died.\nData collected: demographic information, comorbidities, and laboratory test values.\n\nStatistical modeling approaches\n\nThe researchers conducted two main sets of analyses, both estimated using a Bayesian framework:\n\nDynamic Bayesian Markov Model\n\nThe primary goal was to develop a prognostic Markiv model that incorporates dynamic laboratory values with patients’ admission profiles.\nMarkov model accounts for competing risks (discharge vs. death) and allows for daily updates to mortality risk based on changing lab values.\nPredictor sets included:\n\nDemographics, comorbidities, admission, and lab values.\n\nThe model structure included:\n\nSecondary methodology: Logistic Regression Analysis\n\nThis analysis was conducted to determine factors most predictive of patient mortality but was specifically not intended to assess dynamic changes in mortality risk. This approach considered only patients’ outcomes, without factoring in the time taken for the outcome to occur.\n\nUnivariate Analysis (Baseline Comparison)\n\nBefore the multivariate analyses, univariate Cox survival analyses were used to illustrate the baseline patient characteristics and lab values upon admission that, individually, were the strongest determinants of risk.\n\n\n\nModel Validation\n\nInternal Validation: To assess the internal validity of the Markov model, the researchers performed k-fold cross-validation for each of the four regressions. The predictive accuracy increased significantly when dynamic test values were included, boosting accuracy from around 64–67% (for static models) to 83% (for the dynamic model).\nParameter Interpretation: Since the models were estimated in a Bayesian framework, the results are presented as probabilities representing the posterior probability that a given variable had an odds ratio exceeding one, removing the need for an arbitrary significance cutoff.\n\n\n\n\n\nThe performance results demonstrate that relying solely on static admission data (demographics, comorbidities, and initial lab values) yielded a mean predictive accuracy of only 64% to 67% across the initial regression sets. In contrast, incorporating dynamic changes in laboratory values (percentage changes relative to admission values) throughout hospitalization significantly boosted the model’s predictive power to an accuracy of 83%.\n\n\n\nThe article details a Dynamic Bayesian Markov Model that uses time-dependent changes in biomarkers to achieve 83% accuracy in identifying high-mortality risk in hospitalized COVID-19 patients, significantly outperforming static admission-only models (64–67% accuracy)."
  },
  {
    "objectID": "Summaries/et/paper5.html#summary",
    "href": "Summaries/et/paper5.html#summary",
    "title": "A Dynamic Bayesian Model for Identifying High-Mortality Risk in Hospitalized COVID-19 Patients",
    "section": "",
    "text": "THis paper describes a study that used a Bayesian Markov model to better predict mortality risk for hospitalized COVID-19 patients by incorporating dynamic changes in laboratory values over time, rather than relying solely on admission data. The researchers collected demographic, comorbidity, and lab data for 553 PCR-positive patients and found that factors like age over 80 and certain comorbidities increased risk, but including dynamic changes in biomarkers significantly improved the predictive accuracy of the model. The study concludes by presenting a clinical decision tool that uses the most important factors for patient risk stratification based on available information at different stages of hospitalization."
  },
  {
    "objectID": "Summaries/et/paper5.html#problem",
    "href": "Summaries/et/paper5.html#problem",
    "title": "A Dynamic Bayesian Model for Identifying High-Mortality Risk in Hospitalized COVID-19 Patients",
    "section": "",
    "text": "The central problem this research aims to solve is how to accurately and quickly predict which hospitalized COVID-19 patients face the highest risk of death. Because hospitalization rates remained high and the disease caused millions of deaths globally, there was an urgent need to identify factors that predict severe disease and mortality to improve patient care and outcomes. The developed model is intended to allow for prioritization at the systems level and the individualization of care for each patient.\nA major shortcoming of previous prognostic tools was that they were static. Existing models generally looked at patient data and biomarkers only at the time of hospital admission to predict a single outcome, like death, at a single future time point. However, the sources note that a patient’s risk changes constantly during hospitalization, as biomarkers show outcome-specific dynamic changes. Existing models struggled to incorporate these dynamic changes and had difficulty handling the competing risks a patient faces, like remaining in the hospital, being discharged, or dying. Failing to account for these ongoing changes meant that risk predictions were often inaccurate in real-time clinical settings.\nTherefore, the authors developed a “Dynamic Bayesian Model” to overcome these limitations. This new model combines a patient’s initial information (demographics and comorbidities) with daily dynamic changes in laboratory test values throughout their hospital stay. By incorporating these time-dependent measurements, the model achieved dramatic improvements in predictive accuracy compared to models that relied only on admission data. This allows the model to provide daily adjustments to the patient’s in-hospital mortality risk, making it a more effective tool for clinicians deciding on appropriate care and resource allocation."
  },
  {
    "objectID": "Summaries/et/paper5.html#methodology",
    "href": "Summaries/et/paper5.html#methodology",
    "title": "A Dynamic Bayesian Model for Identifying High-Mortality Risk in Hospitalized COVID-19 Patients",
    "section": "",
    "text": "Case selection\n\nPatients admitted to SUNY Downstate Medical Center, with COVID-19 related symptoms, between February 2020 and March 2020.\n\nSample\n\n553 PCR-positive patients included in the study.\nStratified into two groups: 200 patients who were discharged and 200 patients who died.\nData collected: demographic information, comorbidities, and laboratory test values.\n\nStatistical modeling approaches\n\nThe researchers conducted two main sets of analyses, both estimated using a Bayesian framework:\n\nDynamic Bayesian Markov Model\n\nThe primary goal was to develop a prognostic Markiv model that incorporates dynamic laboratory values with patients’ admission profiles.\nMarkov model accounts for competing risks (discharge vs. death) and allows for daily updates to mortality risk based on changing lab values.\nPredictor sets included:\n\nDemographics, comorbidities, admission, and lab values.\n\nThe model structure included:\n\nSecondary methodology: Logistic Regression Analysis\n\nThis analysis was conducted to determine factors most predictive of patient mortality but was specifically not intended to assess dynamic changes in mortality risk. This approach considered only patients’ outcomes, without factoring in the time taken for the outcome to occur.\n\nUnivariate Analysis (Baseline Comparison)\n\nBefore the multivariate analyses, univariate Cox survival analyses were used to illustrate the baseline patient characteristics and lab values upon admission that, individually, were the strongest determinants of risk.\n\n\n\nModel Validation\n\nInternal Validation: To assess the internal validity of the Markov model, the researchers performed k-fold cross-validation for each of the four regressions. The predictive accuracy increased significantly when dynamic test values were included, boosting accuracy from around 64–67% (for static models) to 83% (for the dynamic model).\nParameter Interpretation: Since the models were estimated in a Bayesian framework, the results are presented as probabilities representing the posterior probability that a given variable had an odds ratio exceeding one, removing the need for an arbitrary significance cutoff."
  },
  {
    "objectID": "Summaries/et/paper5.html#results-and-performance",
    "href": "Summaries/et/paper5.html#results-and-performance",
    "title": "A Dynamic Bayesian Model for Identifying High-Mortality Risk in Hospitalized COVID-19 Patients",
    "section": "",
    "text": "The performance results demonstrate that relying solely on static admission data (demographics, comorbidities, and initial lab values) yielded a mean predictive accuracy of only 64% to 67% across the initial regression sets. In contrast, incorporating dynamic changes in laboratory values (percentage changes relative to admission values) throughout hospitalization significantly boosted the model’s predictive power to an accuracy of 83%."
  },
  {
    "objectID": "Summaries/et/paper5.html#conclusion",
    "href": "Summaries/et/paper5.html#conclusion",
    "title": "A Dynamic Bayesian Model for Identifying High-Mortality Risk in Hospitalized COVID-19 Patients",
    "section": "",
    "text": "The article details a Dynamic Bayesian Markov Model that uses time-dependent changes in biomarkers to achieve 83% accuracy in identifying high-mortality risk in hospitalized COVID-19 patients, significantly outperforming static admission-only models (64–67% accuracy)."
  },
  {
    "objectID": "Summaries/et/paper3.html",
    "href": "Summaries/et/paper3.html",
    "title": "An introduction to using Bayesian linear regression with clinical data",
    "section": "",
    "text": "An introduction to using Bayesian linear regression with clinical data\nScott A. Baldwin, Michael J. Larson\nBehaviour Research and Therapy, Volume 98, 2017, Pages 58-75\nhttps://doi.org/10.1016/j.brat.2016.12.016\n\nSummary\nThis paper provides an introduction to Bayesian linear regression within the context of clinical data analysis. It contrasts Bayesian methodologies with traditional frequentist methods, highlighting the limitations of the latter, particularly in relation to p-values and confidence intervals. The article explains fundamental Bayesian concepts such as priors, likelihood, and posterior distributions. It presents an example using electroencephalogram (EEG) and anxiety study data to demonstrate the relationship between error-related negativity (ERN) and trait anxiety. It also covers practical aspects like Markov Chain Monte Carlo (MCMC) sampling, assessing model convergence, interpreting credible intervals, and evaluating model fit using WAIC and LOO-CV, extending the discussion to other models like logistic and Poisson regression.\n\n\nWhat is the goal of the paper?\nThe goal of the paper is to introduce Bayesian linear regression techniques to researchers in clinical psychology and related fields, demonstrating how these methods can provide more informative and nuanced insights compared to traditional frequentist approaches.\n\n\nWhat is the problem being addressed?\nThe problem being addressed is the limitations of frequentist statistical methods, particularly the reliance on p-values and confidence intervals, which can lead to misinterpretations and less informative results in clinical data analysis. ### Why is it important? It is important because clinical data often involve complexities such as small sample sizes, missing data, and hierarchical structures that traditional methods may not handle well. Bayesian methods offer a more flexible and robust framework for analyzing such data, leading to better-informed clinical decisions and research outcomes. ### What are the key results? Key results include a detailed explanation of Bayesian linear regression concepts, a practical example using EEG and anxiety data, and guidance on implementing Bayesian methods using software like R and Stan. The paper also discusses how to assess model convergence and fit, providing a comprehensive overview of Bayesian analysis in a clinical context. ### What are the limitations of the paper? Limitations of the paper include its focus on linear regression, which may not cover all types of clinical data analysis needs."
  },
  {
    "objectID": "Summaries/et/paper6.html",
    "href": "Summaries/et/paper6.html",
    "title": "Variable Selection Using Bayesian Additive Regression Trees",
    "section": "",
    "text": "Chuji Luo, Michael J. Daniels\nStatistical Science, Vol.39, No. 2, 286-304, May 2024\nhttps://arxiv.org/abs/2112.13998\n\n\nThis paper introduces a novel method for variable selection in regression models using Bayesian Additive Regression Trees (BART). The authors propose a permutation-based approach to assess the importance of each predictor variable by comparing the model’s performance with and without the variable. This method allows for the identification of relevant predictors while accounting for complex interactions and nonlinear relationships inherent in BART models. The authors demonstrate the effectiveness of their approach through simulations and real-world applications, showing that it outperforms traditional variable selection methods in terms of accuracy and interpretability.\n\n\n\nIn regression analysis, identifying the most relevant predictor variables is crucial for building accurate and interpretable models. Traditional variable selection methods often struggle with complex data structures, such as nonlinear relationships and interactions among predictors. Bayesian Additive Regression Trees (BART) offer a flexible modeling framework that can capture these complexities, but they lack built-in mechanisms for variable selection. The challenge addressed in this paper is to develop a robust method for selecting important variables within the BART framework, enabling researchers to leverage BART’s strengths while ensuring model interpretability.\n\n\n\nThe authors propose a permutation-based variable selection method within the BART framework. The key steps of the methodology are as follows: 1. Fit a BART model to the data using all predictor variables. 2. For each predictor variable, create a permuted version of the dataset by randomly shuffling the values of that variable while keeping the other variables unchanged. 3. Fit a BART model to the permuted dataset and evaluate its performance using a suitable metric (e.g., mean squared error). 4. Compare the performance of the original BART model with the permuted model. A significant drop in performance when a variable is permuted indicates that the variable is important for predicting the outcome. 5. Repeat the permutation process multiple times to obtain a distribution of performance metrics for each variable, allowing for statistical inference about variable importance. 6. Select variables based on their importance scores, using a threshold to determine which variables are considered relevant. 7. The authors validate their method through extensive simulations and applications to real-world datasets, demonstrating its effectiveness in identifying important predictors while accounting for complex interactions and nonlinearities.\n\n\n\nThe proposed permutation-based variable selection method using BART was evaluated through simulations and real-world applications. The results showed that the method effectively identified important predictor variables, outperforming traditional variable selection techniques in terms of accuracy and interpretability. The authors reported that their approach was able to capture complex interactions and nonlinear relationships among predictors, leading to improved model performance. Additionally, the method provided a clear ranking of variable importance, facilitating the interpretation of results. Overall, the findings suggest that the proposed method is a valuable tool for variable selection in regression models, particularly when using BART.\n\n\n\n\nThe paper introduced three new variable selection approaches: (1) a permutation-based approach using within-type BART Variable Inclusion Proportion (VIP), (2) a permutation-based approach using BART Metropolis Importance (MI), and (3) a backward selection procedure with two filters.\nThese new approaches were designed specifically to address issues with existing methods, such as being biased against categorical predictors and being conservative in including relevant predictors, particularly in data settings with mixed-type predictors (e.g., continuous and binary).\nBased on the simulation results, where success was defined as an excellent capability of including all relevant predictors (rmiss \\(\\le 0.1\\)) and acceptable capability of excluding irrelevant predictors (precision \\(\\ge 0.6\\)), the three proposed approaches consistently perform well in identifying all the relevant predictors and excluding irrelevant predictors.\n\nRanking of Successful Approaches (by Success Rate)\nThe backward selection approach with two filters achieves the highest success rate (70.8%) across the various simulation scenarios tested. The two new permutation-based approaches followed closely in performance: 1. Backward selection with two filters: 70.8% success rate. 2. Permutation-based approach using BART MI: 66.7% success rate. 3. Permutation-based approach using BART Within-Type VIP: 62.5% success rate.\nIdentified Drawbacks\n\nA significant drawback of the three proposed approaches, similar to existing BART-based variable selection methods, is that they suffer from multicollinearity (correlated predictors). This challenge is particularly noticeable when the noise is high or when dealing with a binary response variable.\nThe computational cost of the backward selection approach is a shortcoming because it requires running BART multiple times, although this cost can be reduced by fitting the models in parallel on multiple cores.\nWhile the permutation-based approach using BART within-type VIP improves upon the existing BART VIP approach for small numbers of mixed-type predictors, it also suffers from multicollinearity."
  },
  {
    "objectID": "Summaries/et/paper6.html#summary",
    "href": "Summaries/et/paper6.html#summary",
    "title": "Variable Selection Using Bayesian Additive Regression Trees",
    "section": "",
    "text": "This paper introduces a novel method for variable selection in regression models using Bayesian Additive Regression Trees (BART). The authors propose a permutation-based approach to assess the importance of each predictor variable by comparing the model’s performance with and without the variable. This method allows for the identification of relevant predictors while accounting for complex interactions and nonlinear relationships inherent in BART models. The authors demonstrate the effectiveness of their approach through simulations and real-world applications, showing that it outperforms traditional variable selection methods in terms of accuracy and interpretability."
  },
  {
    "objectID": "Summaries/et/paper6.html#problem",
    "href": "Summaries/et/paper6.html#problem",
    "title": "Variable Selection Using Bayesian Additive Regression Trees",
    "section": "",
    "text": "In regression analysis, identifying the most relevant predictor variables is crucial for building accurate and interpretable models. Traditional variable selection methods often struggle with complex data structures, such as nonlinear relationships and interactions among predictors. Bayesian Additive Regression Trees (BART) offer a flexible modeling framework that can capture these complexities, but they lack built-in mechanisms for variable selection. The challenge addressed in this paper is to develop a robust method for selecting important variables within the BART framework, enabling researchers to leverage BART’s strengths while ensuring model interpretability."
  },
  {
    "objectID": "Summaries/et/paper6.html#methodology",
    "href": "Summaries/et/paper6.html#methodology",
    "title": "Variable Selection Using Bayesian Additive Regression Trees",
    "section": "",
    "text": "The authors propose a permutation-based variable selection method within the BART framework. The key steps of the methodology are as follows: 1. Fit a BART model to the data using all predictor variables. 2. For each predictor variable, create a permuted version of the dataset by randomly shuffling the values of that variable while keeping the other variables unchanged. 3. Fit a BART model to the permuted dataset and evaluate its performance using a suitable metric (e.g., mean squared error). 4. Compare the performance of the original BART model with the permuted model. A significant drop in performance when a variable is permuted indicates that the variable is important for predicting the outcome. 5. Repeat the permutation process multiple times to obtain a distribution of performance metrics for each variable, allowing for statistical inference about variable importance. 6. Select variables based on their importance scores, using a threshold to determine which variables are considered relevant. 7. The authors validate their method through extensive simulations and applications to real-world datasets, demonstrating its effectiveness in identifying important predictors while accounting for complex interactions and nonlinearities."
  },
  {
    "objectID": "Summaries/et/paper6.html#results-and-performance",
    "href": "Summaries/et/paper6.html#results-and-performance",
    "title": "Variable Selection Using Bayesian Additive Regression Trees",
    "section": "",
    "text": "The proposed permutation-based variable selection method using BART was evaluated through simulations and real-world applications. The results showed that the method effectively identified important predictor variables, outperforming traditional variable selection techniques in terms of accuracy and interpretability. The authors reported that their approach was able to capture complex interactions and nonlinear relationships among predictors, leading to improved model performance. Additionally, the method provided a clear ranking of variable importance, facilitating the interpretation of results. Overall, the findings suggest that the proposed method is a valuable tool for variable selection in regression models, particularly when using BART."
  },
  {
    "objectID": "Summaries/et/paper6.html#conclusion",
    "href": "Summaries/et/paper6.html#conclusion",
    "title": "Variable Selection Using Bayesian Additive Regression Trees",
    "section": "",
    "text": "The paper introduced three new variable selection approaches: (1) a permutation-based approach using within-type BART Variable Inclusion Proportion (VIP), (2) a permutation-based approach using BART Metropolis Importance (MI), and (3) a backward selection procedure with two filters.\nThese new approaches were designed specifically to address issues with existing methods, such as being biased against categorical predictors and being conservative in including relevant predictors, particularly in data settings with mixed-type predictors (e.g., continuous and binary).\nBased on the simulation results, where success was defined as an excellent capability of including all relevant predictors (rmiss \\(\\le 0.1\\)) and acceptable capability of excluding irrelevant predictors (precision \\(\\ge 0.6\\)), the three proposed approaches consistently perform well in identifying all the relevant predictors and excluding irrelevant predictors.\n\nRanking of Successful Approaches (by Success Rate)\nThe backward selection approach with two filters achieves the highest success rate (70.8%) across the various simulation scenarios tested. The two new permutation-based approaches followed closely in performance: 1. Backward selection with two filters: 70.8% success rate. 2. Permutation-based approach using BART MI: 66.7% success rate. 3. Permutation-based approach using BART Within-Type VIP: 62.5% success rate.\nIdentified Drawbacks\n\nA significant drawback of the three proposed approaches, similar to existing BART-based variable selection methods, is that they suffer from multicollinearity (correlated predictors). This challenge is particularly noticeable when the noise is high or when dealing with a binary response variable.\nThe computational cost of the backward selection approach is a shortcoming because it requires running BART multiple times, although this cost can be reduced by fitting the models in parallel on multiple cores.\nWhile the permutation-based approach using BART within-type VIP improves upon the existing BART VIP approach for small numbers of mixed-type predictors, it also suffers from multicollinearity."
  }
]