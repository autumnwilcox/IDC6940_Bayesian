[
  {
    "objectID": "Summaries/et/paper6.html",
    "href": "Summaries/et/paper6.html",
    "title": "Variable Selection Using Bayesian Additive Regression Trees",
    "section": "",
    "text": "Chuji Luo, Michael J. Daniels\nStatistical Science, Vol.39, No. 2, 286-304, May 2024\nhttps://arxiv.org/abs/2112.13998\n\n\nThis paper introduces a novel method for variable selection in regression models using Bayesian Additive Regression Trees (BART). The authors propose a permutation-based approach to assess the importance of each predictor variable by comparing the model’s performance with and without the variable. This method allows for the identification of relevant predictors while accounting for complex interactions and nonlinear relationships inherent in BART models. The authors demonstrate the effectiveness of their approach through simulations and real-world applications, showing that it outperforms traditional variable selection methods in terms of accuracy and interpretability.\n\n\n\nIn regression analysis, identifying the most relevant predictor variables is crucial for building accurate and interpretable models. Traditional variable selection methods often struggle with complex data structures, such as nonlinear relationships and interactions among predictors. Bayesian Additive Regression Trees (BART) offer a flexible modeling framework that can capture these complexities, but they lack built-in mechanisms for variable selection. The challenge addressed in this paper is to develop a robust method for selecting important variables within the BART framework, enabling researchers to leverage BART’s strengths while ensuring model interpretability.\n\n\n\nThe authors propose a permutation-based variable selection method within the BART framework. The key steps of the methodology are as follows: 1. Fit a BART model to the data using all predictor variables. 2. For each predictor variable, create a permuted version of the dataset by randomly shuffling the values of that variable while keeping the other variables unchanged. 3. Fit a BART model to the permuted dataset and evaluate its performance using a suitable metric (e.g., mean squared error). 4. Compare the performance of the original BART model with the permuted model. A significant drop in performance when a variable is permuted indicates that the variable is important for predicting the outcome. 5. Repeat the permutation process multiple times to obtain a distribution of performance metrics for each variable, allowing for statistical inference about variable importance. 6. Select variables based on their importance scores, using a threshold to determine which variables are considered relevant. 7. The authors validate their method through extensive simulations and applications to real-world datasets, demonstrating its effectiveness in identifying important predictors while accounting for complex interactions and nonlinearities.\n\n\n\nThe proposed permutation-based variable selection method using BART was evaluated through simulations and real-world applications. The results showed that the method effectively identified important predictor variables, outperforming traditional variable selection techniques in terms of accuracy and interpretability. The authors reported that their approach was able to capture complex interactions and nonlinear relationships among predictors, leading to improved model performance. Additionally, the method provided a clear ranking of variable importance, facilitating the interpretation of results. Overall, the findings suggest that the proposed method is a valuable tool for variable selection in regression models, particularly when using BART.\n\n\n\n\nThe paper introduced three new variable selection approaches: (1) a permutation-based approach using within-type BART Variable Inclusion Proportion (VIP), (2) a permutation-based approach using BART Metropolis Importance (MI), and (3) a backward selection procedure with two filters.\nThese new approaches were designed specifically to address issues with existing methods, such as being biased against categorical predictors and being conservative in including relevant predictors, particularly in data settings with mixed-type predictors (e.g., continuous and binary).\nBased on the simulation results, where success was defined as an excellent capability of including all relevant predictors (rmiss \\(\\le 0.1\\)) and acceptable capability of excluding irrelevant predictors (precision \\(\\ge 0.6\\)), the three proposed approaches consistently perform well in identifying all the relevant predictors and excluding irrelevant predictors.\n\nRanking of Successful Approaches (by Success Rate)\nThe backward selection approach with two filters achieves the highest success rate (70.8%) across the various simulation scenarios tested. The two new permutation-based approaches followed closely in performance: 1. Backward selection with two filters: 70.8% success rate. 2. Permutation-based approach using BART MI: 66.7% success rate. 3. Permutation-based approach using BART Within-Type VIP: 62.5% success rate.\nIdentified Drawbacks\n\nA significant drawback of the three proposed approaches, similar to existing BART-based variable selection methods, is that they suffer from multicollinearity (correlated predictors). This challenge is particularly noticeable when the noise is high or when dealing with a binary response variable.\nThe computational cost of the backward selection approach is a shortcoming because it requires running BART multiple times, although this cost can be reduced by fitting the models in parallel on multiple cores.\nWhile the permutation-based approach using BART within-type VIP improves upon the existing BART VIP approach for small numbers of mixed-type predictors, it also suffers from multicollinearity."
  },
  {
    "objectID": "Summaries/et/paper6.html#summary",
    "href": "Summaries/et/paper6.html#summary",
    "title": "Variable Selection Using Bayesian Additive Regression Trees",
    "section": "",
    "text": "This paper introduces a novel method for variable selection in regression models using Bayesian Additive Regression Trees (BART). The authors propose a permutation-based approach to assess the importance of each predictor variable by comparing the model’s performance with and without the variable. This method allows for the identification of relevant predictors while accounting for complex interactions and nonlinear relationships inherent in BART models. The authors demonstrate the effectiveness of their approach through simulations and real-world applications, showing that it outperforms traditional variable selection methods in terms of accuracy and interpretability."
  },
  {
    "objectID": "Summaries/et/paper6.html#problem",
    "href": "Summaries/et/paper6.html#problem",
    "title": "Variable Selection Using Bayesian Additive Regression Trees",
    "section": "",
    "text": "In regression analysis, identifying the most relevant predictor variables is crucial for building accurate and interpretable models. Traditional variable selection methods often struggle with complex data structures, such as nonlinear relationships and interactions among predictors. Bayesian Additive Regression Trees (BART) offer a flexible modeling framework that can capture these complexities, but they lack built-in mechanisms for variable selection. The challenge addressed in this paper is to develop a robust method for selecting important variables within the BART framework, enabling researchers to leverage BART’s strengths while ensuring model interpretability."
  },
  {
    "objectID": "Summaries/et/paper6.html#methodology",
    "href": "Summaries/et/paper6.html#methodology",
    "title": "Variable Selection Using Bayesian Additive Regression Trees",
    "section": "",
    "text": "The authors propose a permutation-based variable selection method within the BART framework. The key steps of the methodology are as follows: 1. Fit a BART model to the data using all predictor variables. 2. For each predictor variable, create a permuted version of the dataset by randomly shuffling the values of that variable while keeping the other variables unchanged. 3. Fit a BART model to the permuted dataset and evaluate its performance using a suitable metric (e.g., mean squared error). 4. Compare the performance of the original BART model with the permuted model. A significant drop in performance when a variable is permuted indicates that the variable is important for predicting the outcome. 5. Repeat the permutation process multiple times to obtain a distribution of performance metrics for each variable, allowing for statistical inference about variable importance. 6. Select variables based on their importance scores, using a threshold to determine which variables are considered relevant. 7. The authors validate their method through extensive simulations and applications to real-world datasets, demonstrating its effectiveness in identifying important predictors while accounting for complex interactions and nonlinearities."
  },
  {
    "objectID": "Summaries/et/paper6.html#results-and-performance",
    "href": "Summaries/et/paper6.html#results-and-performance",
    "title": "Variable Selection Using Bayesian Additive Regression Trees",
    "section": "",
    "text": "The proposed permutation-based variable selection method using BART was evaluated through simulations and real-world applications. The results showed that the method effectively identified important predictor variables, outperforming traditional variable selection techniques in terms of accuracy and interpretability. The authors reported that their approach was able to capture complex interactions and nonlinear relationships among predictors, leading to improved model performance. Additionally, the method provided a clear ranking of variable importance, facilitating the interpretation of results. Overall, the findings suggest that the proposed method is a valuable tool for variable selection in regression models, particularly when using BART."
  },
  {
    "objectID": "Summaries/et/paper6.html#conclusion",
    "href": "Summaries/et/paper6.html#conclusion",
    "title": "Variable Selection Using Bayesian Additive Regression Trees",
    "section": "",
    "text": "The paper introduced three new variable selection approaches: (1) a permutation-based approach using within-type BART Variable Inclusion Proportion (VIP), (2) a permutation-based approach using BART Metropolis Importance (MI), and (3) a backward selection procedure with two filters.\nThese new approaches were designed specifically to address issues with existing methods, such as being biased against categorical predictors and being conservative in including relevant predictors, particularly in data settings with mixed-type predictors (e.g., continuous and binary).\nBased on the simulation results, where success was defined as an excellent capability of including all relevant predictors (rmiss \\(\\le 0.1\\)) and acceptable capability of excluding irrelevant predictors (precision \\(\\ge 0.6\\)), the three proposed approaches consistently perform well in identifying all the relevant predictors and excluding irrelevant predictors.\n\nRanking of Successful Approaches (by Success Rate)\nThe backward selection approach with two filters achieves the highest success rate (70.8%) across the various simulation scenarios tested. The two new permutation-based approaches followed closely in performance: 1. Backward selection with two filters: 70.8% success rate. 2. Permutation-based approach using BART MI: 66.7% success rate. 3. Permutation-based approach using BART Within-Type VIP: 62.5% success rate.\nIdentified Drawbacks\n\nA significant drawback of the three proposed approaches, similar to existing BART-based variable selection methods, is that they suffer from multicollinearity (correlated predictors). This challenge is particularly noticeable when the noise is high or when dealing with a binary response variable.\nThe computational cost of the backward selection approach is a shortcoming because it requires running BART multiple times, although this cost can be reduced by fitting the models in parallel on multiple cores.\nWhile the permutation-based approach using BART within-type VIP improves upon the existing BART VIP approach for small numbers of mixed-type predictors, it also suffers from multicollinearity."
  },
  {
    "objectID": "Summaries/et/paper3.html",
    "href": "Summaries/et/paper3.html",
    "title": "An introduction to using Bayesian linear regression with clinical data",
    "section": "",
    "text": "An introduction to using Bayesian linear regression with clinical data\nScott A. Baldwin, Michael J. Larson\nBehaviour Research and Therapy, Volume 98, 2017, Pages 58-75\nhttps://doi.org/10.1016/j.brat.2016.12.016\n\nSummary\nThis paper provides an introduction to Bayesian linear regression within the context of clinical data analysis. It contrasts Bayesian methodologies with traditional frequentist methods, highlighting the limitations of the latter, particularly in relation to p-values and confidence intervals. The article explains fundamental Bayesian concepts such as priors, likelihood, and posterior distributions. It presents an example using electroencephalogram (EEG) and anxiety study data to demonstrate the relationship between error-related negativity (ERN) and trait anxiety. It also covers practical aspects like Markov Chain Monte Carlo (MCMC) sampling, assessing model convergence, interpreting credible intervals, and evaluating model fit using WAIC and LOO-CV, extending the discussion to other models like logistic and Poisson regression.\n\n\nWhat is the goal of the paper?\nThe goal of the paper is to introduce Bayesian linear regression techniques to researchers in clinical psychology and related fields, demonstrating how these methods can provide more informative and nuanced insights compared to traditional frequentist approaches.\n\n\nWhat is the problem being addressed?\nThe problem being addressed is the limitations of frequentist statistical methods, particularly the reliance on p-values and confidence intervals, which can lead to misinterpretations and less informative results in clinical data analysis. ### Why is it important? It is important because clinical data often involve complexities such as small sample sizes, missing data, and hierarchical structures that traditional methods may not handle well. Bayesian methods offer a more flexible and robust framework for analyzing such data, leading to better-informed clinical decisions and research outcomes. ### What are the key results? Key results include a detailed explanation of Bayesian linear regression concepts, a practical example using EEG and anxiety data, and guidance on implementing Bayesian methods using software like R and Stan. The paper also discusses how to assess model convergence and fit, providing a comprehensive overview of Bayesian analysis in a clinical context. ### What are the limitations of the paper? Limitations of the paper include its focus on linear regression, which may not cover all types of clinical data analysis needs."
  },
  {
    "objectID": "Summaries/et/paper5.html",
    "href": "Summaries/et/paper5.html",
    "title": "A Dynamic Bayesian Model for Identifying High-Mortality Risk in Hospitalized COVID-19 Patients",
    "section": "",
    "text": "Amir Momeni-Boroujeni, Rachelle Mendoza, Isaac J. Stopard, Ben Lambert, and Alejandro Zuretti\nInfect. Dis. Rep. 2021, 13, 239–250. https://doi.org/10.3390/idr13010027\n\n\nTHis paper describes a study that used a Bayesian Markov model to better predict mortality risk for hospitalized COVID-19 patients by incorporating dynamic changes in laboratory values over time, rather than relying solely on admission data. The researchers collected demographic, comorbidity, and lab data for 553 PCR-positive patients and found that factors like age over 80 and certain comorbidities increased risk, but including dynamic changes in biomarkers significantly improved the predictive accuracy of the model. The study concludes by presenting a clinical decision tool that uses the most important factors for patient risk stratification based on available information at different stages of hospitalization.\n\n\n\nThe central problem this research aims to solve is how to accurately and quickly predict which hospitalized COVID-19 patients face the highest risk of death. Because hospitalization rates remained high and the disease caused millions of deaths globally, there was an urgent need to identify factors that predict severe disease and mortality to improve patient care and outcomes. The developed model is intended to allow for prioritization at the systems level and the individualization of care for each patient.\nA major shortcoming of previous prognostic tools was that they were static. Existing models generally looked at patient data and biomarkers only at the time of hospital admission to predict a single outcome, like death, at a single future time point. However, the sources note that a patient’s risk changes constantly during hospitalization, as biomarkers show outcome-specific dynamic changes. Existing models struggled to incorporate these dynamic changes and had difficulty handling the competing risks a patient faces, like remaining in the hospital, being discharged, or dying. Failing to account for these ongoing changes meant that risk predictions were often inaccurate in real-time clinical settings.\nTherefore, the authors developed a “Dynamic Bayesian Model” to overcome these limitations. This new model combines a patient’s initial information (demographics and comorbidities) with daily dynamic changes in laboratory test values throughout their hospital stay. By incorporating these time-dependent measurements, the model achieved dramatic improvements in predictive accuracy compared to models that relied only on admission data. This allows the model to provide daily adjustments to the patient’s in-hospital mortality risk, making it a more effective tool for clinicians deciding on appropriate care and resource allocation.\n\n\n\n\nCase selection\n\nPatients admitted to SUNY Downstate Medical Center, with COVID-19 related symptoms, between February 2020 and March 2020.\n\nSample\n\n553 PCR-positive patients included in the study.\nStratified into two groups: 200 patients who were discharged and 200 patients who died.\nData collected: demographic information, comorbidities, and laboratory test values.\n\nStatistical modeling approaches\n\nThe researchers conducted two main sets of analyses, both estimated using a Bayesian framework:\n\nDynamic Bayesian Markov Model\n\nThe primary goal was to develop a prognostic Markiv model that incorporates dynamic laboratory values with patients’ admission profiles.\nMarkov model accounts for competing risks (discharge vs. death) and allows for daily updates to mortality risk based on changing lab values.\nPredictor sets included:\n\nDemographics, comorbidities, admission, and lab values.\n\nThe model structure included:\n\nSecondary methodology: Logistic Regression Analysis\n\nThis analysis was conducted to determine factors most predictive of patient mortality but was specifically not intended to assess dynamic changes in mortality risk. This approach considered only patients’ outcomes, without factoring in the time taken for the outcome to occur.\n\nUnivariate Analysis (Baseline Comparison)\n\nBefore the multivariate analyses, univariate Cox survival analyses were used to illustrate the baseline patient characteristics and lab values upon admission that, individually, were the strongest determinants of risk.\n\n\n\nModel Validation\n\nInternal Validation: To assess the internal validity of the Markov model, the researchers performed k-fold cross-validation for each of the four regressions. The predictive accuracy increased significantly when dynamic test values were included, boosting accuracy from around 64–67% (for static models) to 83% (for the dynamic model).\nParameter Interpretation: Since the models were estimated in a Bayesian framework, the results are presented as probabilities representing the posterior probability that a given variable had an odds ratio exceeding one, removing the need for an arbitrary significance cutoff.\n\n\n\n\n\nThe performance results demonstrate that relying solely on static admission data (demographics, comorbidities, and initial lab values) yielded a mean predictive accuracy of only 64% to 67% across the initial regression sets. In contrast, incorporating dynamic changes in laboratory values (percentage changes relative to admission values) throughout hospitalization significantly boosted the model’s predictive power to an accuracy of 83%.\n\n\n\nThe article details a Dynamic Bayesian Markov Model that uses time-dependent changes in biomarkers to achieve 83% accuracy in identifying high-mortality risk in hospitalized COVID-19 patients, significantly outperforming static admission-only models (64–67% accuracy)."
  },
  {
    "objectID": "Summaries/et/paper5.html#summary",
    "href": "Summaries/et/paper5.html#summary",
    "title": "A Dynamic Bayesian Model for Identifying High-Mortality Risk in Hospitalized COVID-19 Patients",
    "section": "",
    "text": "THis paper describes a study that used a Bayesian Markov model to better predict mortality risk for hospitalized COVID-19 patients by incorporating dynamic changes in laboratory values over time, rather than relying solely on admission data. The researchers collected demographic, comorbidity, and lab data for 553 PCR-positive patients and found that factors like age over 80 and certain comorbidities increased risk, but including dynamic changes in biomarkers significantly improved the predictive accuracy of the model. The study concludes by presenting a clinical decision tool that uses the most important factors for patient risk stratification based on available information at different stages of hospitalization."
  },
  {
    "objectID": "Summaries/et/paper5.html#problem",
    "href": "Summaries/et/paper5.html#problem",
    "title": "A Dynamic Bayesian Model for Identifying High-Mortality Risk in Hospitalized COVID-19 Patients",
    "section": "",
    "text": "The central problem this research aims to solve is how to accurately and quickly predict which hospitalized COVID-19 patients face the highest risk of death. Because hospitalization rates remained high and the disease caused millions of deaths globally, there was an urgent need to identify factors that predict severe disease and mortality to improve patient care and outcomes. The developed model is intended to allow for prioritization at the systems level and the individualization of care for each patient.\nA major shortcoming of previous prognostic tools was that they were static. Existing models generally looked at patient data and biomarkers only at the time of hospital admission to predict a single outcome, like death, at a single future time point. However, the sources note that a patient’s risk changes constantly during hospitalization, as biomarkers show outcome-specific dynamic changes. Existing models struggled to incorporate these dynamic changes and had difficulty handling the competing risks a patient faces, like remaining in the hospital, being discharged, or dying. Failing to account for these ongoing changes meant that risk predictions were often inaccurate in real-time clinical settings.\nTherefore, the authors developed a “Dynamic Bayesian Model” to overcome these limitations. This new model combines a patient’s initial information (demographics and comorbidities) with daily dynamic changes in laboratory test values throughout their hospital stay. By incorporating these time-dependent measurements, the model achieved dramatic improvements in predictive accuracy compared to models that relied only on admission data. This allows the model to provide daily adjustments to the patient’s in-hospital mortality risk, making it a more effective tool for clinicians deciding on appropriate care and resource allocation."
  },
  {
    "objectID": "Summaries/et/paper5.html#methodology",
    "href": "Summaries/et/paper5.html#methodology",
    "title": "A Dynamic Bayesian Model for Identifying High-Mortality Risk in Hospitalized COVID-19 Patients",
    "section": "",
    "text": "Case selection\n\nPatients admitted to SUNY Downstate Medical Center, with COVID-19 related symptoms, between February 2020 and March 2020.\n\nSample\n\n553 PCR-positive patients included in the study.\nStratified into two groups: 200 patients who were discharged and 200 patients who died.\nData collected: demographic information, comorbidities, and laboratory test values.\n\nStatistical modeling approaches\n\nThe researchers conducted two main sets of analyses, both estimated using a Bayesian framework:\n\nDynamic Bayesian Markov Model\n\nThe primary goal was to develop a prognostic Markiv model that incorporates dynamic laboratory values with patients’ admission profiles.\nMarkov model accounts for competing risks (discharge vs. death) and allows for daily updates to mortality risk based on changing lab values.\nPredictor sets included:\n\nDemographics, comorbidities, admission, and lab values.\n\nThe model structure included:\n\nSecondary methodology: Logistic Regression Analysis\n\nThis analysis was conducted to determine factors most predictive of patient mortality but was specifically not intended to assess dynamic changes in mortality risk. This approach considered only patients’ outcomes, without factoring in the time taken for the outcome to occur.\n\nUnivariate Analysis (Baseline Comparison)\n\nBefore the multivariate analyses, univariate Cox survival analyses were used to illustrate the baseline patient characteristics and lab values upon admission that, individually, were the strongest determinants of risk.\n\n\n\nModel Validation\n\nInternal Validation: To assess the internal validity of the Markov model, the researchers performed k-fold cross-validation for each of the four regressions. The predictive accuracy increased significantly when dynamic test values were included, boosting accuracy from around 64–67% (for static models) to 83% (for the dynamic model).\nParameter Interpretation: Since the models were estimated in a Bayesian framework, the results are presented as probabilities representing the posterior probability that a given variable had an odds ratio exceeding one, removing the need for an arbitrary significance cutoff."
  },
  {
    "objectID": "Summaries/et/paper5.html#results-and-performance",
    "href": "Summaries/et/paper5.html#results-and-performance",
    "title": "A Dynamic Bayesian Model for Identifying High-Mortality Risk in Hospitalized COVID-19 Patients",
    "section": "",
    "text": "The performance results demonstrate that relying solely on static admission data (demographics, comorbidities, and initial lab values) yielded a mean predictive accuracy of only 64% to 67% across the initial regression sets. In contrast, incorporating dynamic changes in laboratory values (percentage changes relative to admission values) throughout hospitalization significantly boosted the model’s predictive power to an accuracy of 83%."
  },
  {
    "objectID": "Summaries/et/paper5.html#conclusion",
    "href": "Summaries/et/paper5.html#conclusion",
    "title": "A Dynamic Bayesian Model for Identifying High-Mortality Risk in Hospitalized COVID-19 Patients",
    "section": "",
    "text": "The article details a Dynamic Bayesian Markov Model that uses time-dependent changes in biomarkers to achieve 83% accuracy in identifying high-mortality risk in hospitalized COVID-19 patients, significantly outperforming static admission-only models (64–67% accuracy)."
  },
  {
    "objectID": "Summaries/nm/My_references.html",
    "href": "Summaries/nm/My_references.html",
    "title": "Bayesian Logistic Regression Capstone",
    "section": "",
    "text": "@article{Zeger2020, abstract = {The PCORI mission is to address questions about health care from the patients’ perspective, such as “What is my health status and its trajectory?” and “What are my treatment options and the expected benefits and harms of each?” The purpose of this PCORI-funded project is to make it easier for clinicians and patients to find valid answers to these and other clinical questions by using modern digital tools that support (1) learning from the experience of prior patients, and (2) translating what is learned to inform the decision at hand, taking into account each patient’s unique circumstances. For this project, we developed and implemented statistical methods called bayesian hierarchical models that combine existing data on past clinical experience from a reference population with new measurements for the individual. Clinicians currently use such methods when screening patients for disease. Modern technologies make it possible for this proven approach to extend far beyond its current use. The recent revolution in information technology has unleashed new types of health data, from DNA sequences to functional images of the brain to patient-reported outcomes. Furthermore, the electronic health record captures every patient’s sequence of health measurements, diagnoses, and treatments. The bayesian methods developed and reported on here combine even complex data to produce predictions about an individual patient’s health status, trajectory, and likely benefits and harms of interventions. In addition to developing novel methods, we facilitated their use by creating and locally disseminating a software package, OSLER inHealth, that will allow other researchers to apply this methodology. The software repository is open-source and includes the methodology developed as part of this research as well as other existing methods that facilitate individualized health prediction. We have tested the proposed methods and software on 3 case studies to (1) estimate the frequency with which various pathogens cause children’s pneumonia and predict which pathogen is likely to be causing a particular child’s pneumonia given her or his clinical data, potentially reducing unnecessary use of antibiotics; (2) infer whether a prostate cancer is indolent or aggressive for a patient under active surveillance; and (3) characterize the variation in multiple, time-varying symptoms of major mental disorders, including schizophrenia and depression, and then use this knowledge to provide patient-specific estimates of past and, likely, future trajectories. With this project, we have developed and demonstrated the value of combining even complex measurements on a population of patients, then translating this experience into more valid assessments of a new patient’s health status and trajectory. The model also supports inferences about the likely benefits and harms associated with available interventions. Copyright {} 2020. Johns Hopkins Bloomberg School of Public Health. All Rights Reserved.}, author = {Zeger, Scott L and Wu, Zhenke and Coley, Yates and Fojo, Anthony Todd and Carter, Bal and O’Brien, Katherine and Zandi, Peter and Cooke, Mary and Carey, Vince and Crainiceanu, Ciprian and Muscelli, John and Gherman, Adrian and Mekosh, Jason}, mendeley-groups = {CapStone_2025/CapStone_DS_2025}, number = {2020}, title = {{Using a Bayesian Approach to Predict Patients’ Health and Response to Treatment}}, url = {http://ovidsp.ovid.com/ovidweb.cgi?T=JS&PAGE=reference&D=medp&NEWS=N&AN=37708307}, year = {2020} }\n@article{Chatzimichail2023, abstract = {Medical diagnosis is the basis for treatment and management decisions in healthcare. Conventional methods for medical diagnosis commonly use established clinical criteria and fixed numerical thresholds. The limitations of such an approach may result in a failure to capture the intricate relations between diagnostic tests and the varying prevalence of diseases. To explore this further, we have developed a freely available specialized computational tool that employs Bayesian inference to calculate the posterior probability of disease diagnosis. This novel software comprises of three distinct modules, each designed to allow users to define and compare parametric and nonparametric distributions effectively. The tool is equipped to analyze datasets generated from two separate diagnostic tests, each performed on both diseased and nondiseased populations. We demonstrate the utility of this software by analyzing fasting plasma glucose, and glycated hemoglobin A1c data from the National Health and Nutrition Examination Survey. Our results are validated using the oral glucose tolerance test as a reference standard, and we explore both parametric and nonparametric distribution models for the Bayesian diagnosis of diabetes mellitus.}, author = {Chatzimichail, Theodora and Hatjimihail, Aristides T.}, doi = {10.3390/DIAGNOSTICS13193135,}, file = {:C:/Users/Namita/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chatzimichail, Hatjimihail - 2023 - A Bayesian Inference Based Computational Tool for Parametric and Nonparametric Medical Diagnosis.pdf:pdf}, issn = {20754418}, journal = {Diagnostics}, keywords = {Bayesian diagnosis,Bayesian inference,copula distribution,diabetes mellitus,kernel density estimator,likelihood,nonparametric distribution,parametric distribution,posterior probability,prior probability,probability density function}, mendeley-groups = {CapStone_2025}, month = {oct}, number = {19}, publisher = {Multidisciplinary Digital Publishing Institute (MDPI)}, title = {{A Bayesian Inference Based Computational Tool for Parametric and Nonparametric Medical Diagnosis}}, url = {https://pubmed.ncbi.nlm.nih.gov/37835877/}, volume = {13}, year = {2023} }\n@article{VandeSchoot2021, abstract = {Bayesian statistics is an approach to data analysis based on Bayes’ theorem, where available knowledge about parameters in a statistical model is updated with the information in observed data. The background knowledge is expressed as a prior distribution and combined with observational data in the form of a likelihood function to determine the posterior distribution. The posterior can also be used for making predictions about future events. This Primer describes the stages involved in Bayesian analysis, from specifying the prior and data models to deriving inference, model checking and refinement. We discuss the importance of prior and posterior predictive checking, selecting a proper technique for sampling from a posterior distribution, variational inference and variable selection. Examples of successful applications of Bayesian analysis across various research fields are provided, including in social sciences, ecology, genetics, medicine and more. We propose strategies for reproducibility and reporting standards, outlining an updated WAMBS (when to Worry and how to Avoid the Misuse of Bayesian Statistics) checklist. Finally, we outline the impact of Bayesian analysis on artificial intelligence, a major goal in the next decade.}, author = {van de Schoot, Rens and Depaoli, Sarah and King, Ruth and Kramer, Bianca and M{\"{a}}rtens, Kaspar and Tadesse, Mahlet G. and Vannucci, Marina and Gelman, Andrew and Veen, Duco and Willemsen, Joukje and Yau, Christopher}, doi = {10.1038/s43586-020-00001-2}, file = {:C:/Users/Namita/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/van de Schoot et al. - 2021 - Bayesian statistics and modelling.pdf:pdf}, issn = {2662-8449}, journal = {Nature Reviews Methods Primers}, keywords = {Scientific community,Statistics}, mendeley-groups = {CapStone_2025}, month = {jan}, number = {1}, pages = {1}, publisher = {Springer Nature}, title = {{Bayesian statistics and modelling}}, url = {https://www.nature.com/articles/s43586-020-00001-2}, volume = {1}, year = {2021} }\n@article{Klauenberg2015, abstract = {Regression is a common task in metrology and often applied to calibrate instruments, evaluate inter-laboratory comparisons or determine fundamental constants, for example. Yet, a regression model cannot be uniquely formulated as a measurement function, and consequently the Guide to the Expression of Uncertainty in Measurement (GUM) and its supplements are not applicable directly. Bayesian inference, however, is well suited to regression tasks, and has the advantage of accounting for additional a priori information, which typically robustifies analyses. Furthermore, it is anticipated that future revisions of the GUM shall also embrace the Bayesian view. Guidance on Bayesian inference for regression tasks is largely lacking in metrology. For linear regression models with Gaussian measurement errors this tutorial gives explicit guidance. Divided into three steps, the tutorial first illustrates how a priori knowledge, which is available from previous experiments, can be translated into prior distributions from a specific class. These prior distributions have the advantage of yielding analytical, closed form results, thus avoiding the need to apply numerical methods such as Markov Chain Monte Carlo. Secondly, formulas for the posterior results are given, explained and illustrated, and software implementations are provided. In the third step, Bayesian tools are used to assess the assumptions behind the suggested approach. These three steps (prior elicitation, posterior calculation, and robustness to prior uncertainty and model adequacy) are critical to Bayesian inference. The general guidance given here for Normal linear regression tasks is accompanied by a simple, but real-world, metrological example. The calibration of a flow device serves as a running example and illustrates the three steps. It is shown that prior knowledge from previous calibrations of the same sonic nozzle enables robust predictions even for extrapolations.}, author = {Klauenberg, Katy and W{\"{u}}bbeler, Gerd and Mickan, Bodo and Harris, Peter and Elster, Clemens}, doi = {10.1088/0026-1394/52/6/878}, file = {:C:/Users/Namita/Downloads/Klauenberg_2015_Metrologia_52_878.pdf:pdf;:C:/Users/Namita/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Klauenberg et al. - 2015 - A tutorial on Bayesian Normal linear regression.pdf:pdf}, issn = {16817575}, journal = {Metrologia}, keywords = {Bayesian inference,Gaussian measurement error,Normal inverse Gamma distribution,conjugate prior distribution,linear regression,prior knowledge,sonic nozzle calibration}, mendeley-groups = {CapStone_2025}, number = {6}, pages = {878–892}, publisher = {IOP Publishing}, title = {{A tutorial on Bayesian Normal linear regression}}, volume = {52}, year = {2015} }\n@article{DeLeeuw2012a, abstract = {In most research, linear regression analyses are performed without taking into account published results (i.e., reported summary statistics) of similar previous studies. Although the prior density in Bayesian linear regression could accommodate such prior knowledge, formal models for doing so are absent from the literature. The goal of this article is therefore to develop a Bayesian model in which a linear regression analysis on current data is augmented with the reported regression coefficients (and standard errors) of previous studies. Two versions of this model are presented. The first version incorporates previous studies through the prior density and is applicable when the current and all previous studies are exchangeable. The second version models all studies in a hierarchical structure and is applicable when studies are not exchangeable. Both versions of the model are assessed using simulation studies. Performance for each in estimating the regression coefficients is consistently superior to using current data alone and is close to that of an equivalent model that uses the data from previous studies rather than reported regression coefficients. Overall the results show that augmenting data with results from previous studies is viable and yields significant improvements in the parameter estimation. {} 2012 Copyright Taylor and Francis Group, LLC.}, author = {de Leeuw, Christiaan and Klugkist, Irene}, doi = {10.1080/00273171.2012.673957}, file = {:C:/Users/Namita/Downloads/Augmenting Data With Published Results in Bayesian Linear Regression.pdf:pdf}, issn = {00273171}, journal = {Multivariate Behavioral Research}, mendeley-groups = {CapStone_2025}, number = {3}, pages = {369–391}, title = {{Augmenting Data With Published Results in Bayesian Linear Regression}}, volume = {47}, year = {2012} }\n@article{Liu2013, abstract = {Background: A Bayesian clinical reasoning model was developed to predict an individual risk for cardiovascular disease (CVD) for desk-top reference. Methods: Three Bayesian models were constructed to estimate the CVD risk by sequentially incorporating demographic features (basic), six metabolic syndrome components (metabolic score) and conventional risk factors (enhanced model). By considering clinical weights (regression coefficients) of each model as normal distribution, individual risk can be predicted making allowance for uncertainty of clinical weights. A community-based cohort that enrolled 64,489 participants free of CVD at baseline and followed up over five years to ascertain newly diagnosed CVD cases during the period through 2000 to 2004 was used for the illustration of the three proposed models (full empirical data are available from website http://homepage.ntu.edu.tw/\\(\\sim\\)chenlin/CVD-prediction-data.rar). Results: The proposed models can be applied to predicting the CVD risk with any combination of risk factors. For a 47-year-old man, the five-year risk for CVD with the basic model was 11.2% (95% CI: 7.8%-15.6%). His metabolic syndrome score, leading to 1.488 of likelihood ratio, enhanced the risk for CVD up to 15.8% (95% CI: 11.0%-21.5%) and put him in highest deciles. As with the habit of smoking over 2 packs per-day and family history of CVD, yielding the likelihood ratios of 1.62 and 1.47, respectively, the risk was further raised to 30.9% (95% CI: 20.7%-39.8%). Conclusions: We demonstrate how to make individual risk prediction for CVD by incorporating routine information with a sequential Bayesian clinical reasoning approach. {} 2012 Elsevier Ireland Ltd.}, author = {Liu, Yi Ming and Chen, Sam Li Sheng and Yen, Amy Ming Fang and Chen, Hsiu Hsi}, doi = {10.1016/J.IJCARD.2012.05.016}, file = {:C:/Users/Namita/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu et al. - 2013 - Individual risk prediction model for incident cardiovascular disease A Bayesian clinical reasoning approach(9).pdf:pdf}, issn = {0167-5273}, journal = {International Journal of Cardiology}, keywords = {Bayes’ theorem,Bayesian,Cardiovascular disease,Likelihood ratio,Metabolic syndrome,Prediction model}, mendeley-groups = {CapStone_2025}, month = {sep}, number = {5}, pages = {2008–2012}, pmid = {22658349}, publisher = {Elsevier}, title = {{Individual risk prediction model for incident cardiovascular disease: A Bayesian clinical reasoning approach}}, url = {https://www.sciencedirect.com/science/article/pii/S0167527312006274}, volume = {167}, year = {2013} }"
  },
  {
    "objectID": "Summaries/nm/r code.html",
    "href": "Summaries/nm/r code.html",
    "title": "making subsets for each dataset",
    "section": "",
    "text": "NHANES DATASET -https://wwwn.cdc.gov/nchs/nhanes/continuousnhanes/default.aspx?BeginYear=2013 # loading packages\nlibrary(tidyverse) library(knitr) library(ggthemes) library(ggrepel) library(dslabs) library(Hmisc) library(dplyr) library(tidyr) library(forcats) library(ggplot2) library (“nhanesA”)\noptions(repos = c(CRAN = “https://cloud.r-project.org”))\ninstall.packages(“nhanesA”)\n\nmaking subsets for each dataset\n                   nhanesTables('EXAM', 2013)\n                   nhanesTables('QUESTIONNAIRE', 2013)\n                   nhanesTables('DEMOGRAPHICS', 2013)\n                   \n                   \nnhanesCodebook(“BMX_H”) nhanesCodebook(“SMQ_H”)\n# .xpt files read ( 2013–2014) bmx_h &lt;- nhanes(“BMX_H”) #Exam smq_h &lt;- nhanes(“SMQ_H”) #Quest demo_h &lt;- nhanes(“DEMO_H”) #Demo diq_h &lt;- nhanes(“DIQ_H”) #diabetes\n\n\nvariables of interest\nexam_sub &lt;- bmx_h %&gt;% select(SEQN, BMDBMIC) demo_sub &lt;- demo_h %&gt;% select(SEQN, RIDAGEYR, RIAGENDR, RIDRETH1, SDMVPSU, SDMVSTRA, WTMEC2YR) diq_sub &lt;- diq_h %&gt;% select (SEQN, DIQ240)\n\n\nNames of all variables selected for analysis\nnames(exam_sub) names(demo_sub) names(diq_sub)\n\n\nmerged dataframe\nmerged_data &lt;- exam_sub %&gt;% left_join(demo_sub, by = “SEQN”) %&gt;% left_join(diq_sub, by = “SEQN”) head(merged_data)\nnhanesCodebook(“DEMO_H”,‘RIDRETH1’) nhanesCodebook(“DEMO_H”,‘RIAGENDR’) nhanesCodebook(“DEMO_H”,‘RIDAGEYR’) nhanesCodebook(“DIQ_H”,“DIQ240”) nhanesCodebook(“BMX_H”,‘BMDBMIC’)"
  },
  {
    "objectID": "Summaries/nm/Summaries.html",
    "href": "Summaries/nm/Summaries.html",
    "title": "Bayesian Logistic Regression Capstone",
    "section": "",
    "text": "All 6 summaries and references & citation"
  },
  {
    "objectID": "Summaries/nm/Summaries.html#namitas-literature",
    "href": "Summaries/nm/Summaries.html#namitas-literature",
    "title": "Bayesian Logistic Regression Capstone",
    "section": "Namita’s Literature",
    "text": "Namita’s Literature"
  },
  {
    "objectID": "Summaries/nm/Summaries.html#introduction",
    "href": "Summaries/nm/Summaries.html#introduction",
    "title": "Bayesian Logistic Regression Capstone",
    "section": "Introduction",
    "text": "Introduction\n\nBayesian Hierarchical Model (Disease reclassification and prediction)\n\nWhat is the goal of the paper?\nThe authors develop a Bayesian hierarchical model for multivariate longitudinal data to predict health status, trajectories, and intervention effects at the individual level in the PCORI mission to address questions about health status from patients and clinicians.\nWhy is it important?\nHealthcare data (DNA sequences, functional images of the brain, patient-reported outcomes, and electronic health records with patients’ sequences of health measurements, diagnoses, and treatments) are complex, and the standard approaches are not adequate for clinical data analysis. Electronic health records (EHRs) could improve diagnostic accuracy and predict treatment effects. Visualizations of characteristics of posterior distributions can be immediately understood by clinicians and patients as relevant to their decision. Combining prior knowledge and patient data with evidence could predict the patient’s health status, trajectory, and/or likely benefits of interventions.\nHow is it solved?\nMethod: The authors applied Bayesian hierarchical regression for multivariate longitudinal patient data using open-source R-packages and developed 2 levels—time within person and persons within a population\nThe model combined exogenous (eg, age, clinical history) factors and endogenous (eg, current treatment) variables on the individual’s multivariate health measurements and the effects of health measurements at one time on subsequent interventions.\nThe model produced an estimate of the posterior distribution for each value of the predictor variable and an estimate of the marginal distribution of the regression coefficients for each coefficient that measures the outcome (health status) associated with its predictor variables. In a larger sample, the likelihood dominates the prior distribution for regression coefficients and Bayesian hierarchical model used a likelihood-based approach, used priors (prior laboratory and clinical trials data) that provided the assay sensitivities, which through the prior assumptions, made the model identifiable and the integration of Markov chain Monte Carlo (MCMC) estimates the posterior distributions, avoided missing data and complex outcome measurements.\nResults/limitations\nThree case studies: pneumonia etiology in children, prostate cancer, and mental disorders chosen for model development, identified low-risk patient population, reduced the risk of overtreatment, complications, adverse effects, and financial burden for patients (Disease Reclassification). Prostate cancer software was then implemented within the JHM HER.\nLimitation:\nModels were entirely parametric, and extensions to nonparametric or more flexible parametric models were recommended to improve approaches for neuroimage or genomic data.\nApplications:\n\nto scale a tool that addresses a particular unmet need across a larger, more diverse population of patients and clinicians\nuse in autoimmune diseases, sudden cardiac arrest, and diabetes.\nembed a collection of tools to acquire and use the most relevant information, agnostic to its level of measurement, to improve population and individual health decisions that cause better outcomes at more affordable costs. @Zeger2020\n\n\nBayesian Inference (parametric vs non-parametric)\n\nWhat is the goal of the paper?\nThe authors calculated the posterior probability of disease diagnosis and applied Bayesian inference to develop three modules comparing parametric (with a fixed set of parameters) and nonparametric distributions (which do not make a priori assumptions) by analyzing the National Health and Nutrition Examination Survey dataset from two separate diagnostic tests on both diseased and non-diseased populations.\nWhy is it important?\nMedical diagnosis, treatment, and management decisions are crucial, and conventional methods for diagnosis using clinical criteria and fixed numerical thresholds limit the capture of other information related to the intricate relationship between diagnostic tests and the varying prevalence of diseases. The probability distributions associated with quantitative diagnostic test outcomes often demonstrate some overlap between the diseased and nondiseased groups. The dichotomous method fails to capture the complexity and heterogeneity of disease presentations across diverse populations. The applicability of the normal distribution (conventional method) is critiqued, especially in dealing with clinical measurands having skewness, bimodality, or multimodality.\nHow is it solved?\nMethods: The Authors developed models employing Bayesian inference (Bayesian diagnostic approach) to calculate the posterior probability of @Liu2013"
  },
  {
    "objectID": "Summaries/nm/Comparing conventional and Bayesian.html",
    "href": "Summaries/nm/Comparing conventional and Bayesian.html",
    "title": "Bayesian Logistic Regression Capstone",
    "section": "",
    "text": "Comparing conventional and Bayesian workflows for clinical outcome prediction modelling with an exemplar cohort study of severe COVID-19 infection incorporating clinical biomarker test results\nRef: Sullivan, B., Barker, E., MacGregor, L. et al. Comparing conventional and Bayesian workflows for clinical outcome prediction modelling with an exemplar cohort study of severe COVID-19 infection incorporating clinical biomarker test results. BMC Med Inform Decis Mak 25, 123 (2025). https://doi.org/10.1186/s12911-025-02955-3\nProblem: Data curation is challenging as clinical data are heterogeneous in multiple ways. Biomarkers are recorded for different reasons. If missing data imputation is performed, it raises another decision point on whether to impute the continuous or the transformed categorical data.\nAim: Estimating predictive risk factors for disease outcomes with explainable statistical models is desirable for clinical use and decision making. The author provide a guide for modern Bayesian approaches for joint risk factor analysis and variable selection.\nStudy design: Retrospective observational cohort design, lab data linked to the patient data for laboratory markers and clinical outcomes from three hospitals in the Southwest region of England, UK on adults admitted between March to October 2020 and tested positive for SARS CoV-2 by PCR.\nMethod: Analyze range of laboratory blood marker values, Develop cross-validated logistic regression prediction models using the candidate biomarkers, highlighting biomarkers worthy of future research. Employed selection techniques, comparing LASSO frequentist method and Projective Prediction approach on Bayesian logistic regression models with horseshoe priors to illustrate the process of creating a reduced model. They considered models deliver good prediction performance with a small amount of biomarker data.\nVairables (1) Predictors: Includes a variety of clinical severity indices, lab biomarkers (microbiological, immunological, haematological and biochemistry) as parameters used as predictive variables in the regression models.\n\nOutcome: The primary prediction outcome was death or transfer to the ICU.\n\nData management: Continuous biomarkers were trannsfromed into categorical variables (reference ranges for clinical use).\nStaistical calcukation: Analytics carried out using the R statistical language using- Standard logistic regression analyses used the R Stats GLM package (v4.4.1); LASSO analyses, GLMnet (v4.1.8); and for Bayesian analyses, BRMS (v2.22.0) and ProjPred (v2.8.0).\nBefore running full regression models, the independent contribution of individual biomarkers were examined in the training dataset predicting ICU entry or death via standard logistic regressions and Bayesian logistic regressions with either a flat (aka uniform) or horseshoe prior and calculated p-values and odds ratios for each biomarker.\nPer biomarker, patients with and without the outcome were separated and then these groups were shuffled and split into 5 equal subgroups. These groups were then paired at random, to ensure training and test datasets have the same proportion of patients with a severe outcome as in full the sample for that biomarker. This was to improve the chance of convergence for biomarkers with high data missingness and only complete cases of training data available for each biomarker were considered for the study.\nEach individual biomarker model including age and gender (except univariate age and gender models) were compared against a standard model including only age and gender. Regressions were fit using all associated dummy variables for a given biomarker (e.g. ‘Mild’, ‘Moderate’, ‘Severe’) using ‘Normal’ as the reference.\nAnalysis using all valid biomarker data: After individual biomarker evaluation, logistic regression models considering all valid biomarkers (Prediction using individual variables section) and demographic variables were fit to the data and the predictions were tested via internal and external validation using the stratified cross-validation procedures.\nAnalysis using reduced variable models: Considering that eben though a model using all biomarker data may have strong predictive power, but clinically a strong prediction with the least amount of biomarkers might save on time, money and other resources, they used LASSO and Bayesian Projective Prediction methodologies to choose reduced variable models to predict COVID-19 severe outcomes.\nTo estimate variability in model performance and allow comparison between models, we compute inter-quantile AUC difference ranges using 5-fold 20-repeat cross-validation of models\nReduced variable models: LASSO and projective prediction performed for creation of reduced models with fewer biomarkers.\nModel performance evaluation and dissemination They chose to peform cross-validated estimates of AUC, sensitivity, and specificity and the Inter-quartile intervals over these measures.\nRecommendations: Categorization is worth critical consideration in model planning Reduced number of variables Imputation Bayesian approaches should include that coefficients estimated via Bayes should on average deliver better predictive performance than standard GLM"
  },
  {
    "objectID": "Summaries/aw/paper2_summary.html",
    "href": "Summaries/aw/paper2_summary.html",
    "title": "Summary: An Introduction to Using Bayesian Linear Regression with Clinical Data",
    "section": "",
    "text": "Scott A. Baldwin and Michael J. Larson (2017)\n\n\nTraditional statistics training in psychology emphasizes frequentist methods, and researchers often feel unsure about Bayesian alternatives. This paper aims to introduce Bayesian linear regression in a clear and practical way, using real clinical data to help researchers understand and apply Bayesian tools.\n\n\n\nThe authors walk readers through setting up practical Bayesian regression models. They explain how to choose and justify priors, run models using software (like R and Stan), check convergence, and interpret results—including posterior distributions, credible intervals, and predictive outputs. They also compare Bayesian and frequentist approaches and offer useful model-comparison tools. The article includes EEG-anxiety data, and it comes with the data and R code for easy replication.\n\n\n\nThe paper shows how Bayesian regression can be applied step-by-step and how outputs like posterior distributions and interval estimates offer more intuitive, flexible insights than traditional methods. Including data and code makes the method transparent and replicable, while model diagnostics (like convergence checks) ensure reliability.\n\n\n\nThe paper is introductory; it doesn’t dive into advanced modeling extensions or deeply mathematical derivations. It focuses on a single case study (EEG data and anxiety) so broader generalization requires additional application. Expertise is still needed to adapt methods to other complex datasets.\n\n\n\nThe authors illustrate methods using clinical EEG data related to error-related negativity (ERN) and trait anxiety. They provide both the actual dataset and R/Stan code for readers to replicate and learn directly from the example."
  },
  {
    "objectID": "Summaries/aw/paper2_summary.html#problem-the-article-is-addressing",
    "href": "Summaries/aw/paper2_summary.html#problem-the-article-is-addressing",
    "title": "Summary: An Introduction to Using Bayesian Linear Regression with Clinical Data",
    "section": "",
    "text": "Traditional statistics training in psychology emphasizes frequentist methods, and researchers often feel unsure about Bayesian alternatives. This paper aims to introduce Bayesian linear regression in a clear and practical way, using real clinical data to help researchers understand and apply Bayesian tools."
  },
  {
    "objectID": "Summaries/aw/paper2_summary.html#how-it-has-been-solved",
    "href": "Summaries/aw/paper2_summary.html#how-it-has-been-solved",
    "title": "Summary: An Introduction to Using Bayesian Linear Regression with Clinical Data",
    "section": "",
    "text": "The authors walk readers through setting up practical Bayesian regression models. They explain how to choose and justify priors, run models using software (like R and Stan), check convergence, and interpret results—including posterior distributions, credible intervals, and predictive outputs. They also compare Bayesian and frequentist approaches and offer useful model-comparison tools. The article includes EEG-anxiety data, and it comes with the data and R code for easy replication."
  },
  {
    "objectID": "Summaries/aw/paper2_summary.html#results",
    "href": "Summaries/aw/paper2_summary.html#results",
    "title": "Summary: An Introduction to Using Bayesian Linear Regression with Clinical Data",
    "section": "",
    "text": "The paper shows how Bayesian regression can be applied step-by-step and how outputs like posterior distributions and interval estimates offer more intuitive, flexible insights than traditional methods. Including data and code makes the method transparent and replicable, while model diagnostics (like convergence checks) ensure reliability."
  },
  {
    "objectID": "Summaries/aw/paper2_summary.html#limitations",
    "href": "Summaries/aw/paper2_summary.html#limitations",
    "title": "Summary: An Introduction to Using Bayesian Linear Regression with Clinical Data",
    "section": "",
    "text": "The paper is introductory; it doesn’t dive into advanced modeling extensions or deeply mathematical derivations. It focuses on a single case study (EEG data and anxiety) so broader generalization requires additional application. Expertise is still needed to adapt methods to other complex datasets."
  },
  {
    "objectID": "Summaries/aw/paper2_summary.html#datasets",
    "href": "Summaries/aw/paper2_summary.html#datasets",
    "title": "Summary: An Introduction to Using Bayesian Linear Regression with Clinical Data",
    "section": "",
    "text": "The authors illustrate methods using clinical EEG data related to error-related negativity (ERN) and trait anxiety. They provide both the actual dataset and R/Stan code for readers to replicate and learn directly from the example."
  },
  {
    "objectID": "Summaries/aw/paper4_summary.html",
    "href": "Summaries/aw/paper4_summary.html",
    "title": "Summary: Dropout as a Bayesian Approximation—Representing Model Uncertainty in Deep Learning",
    "section": "",
    "text": "Yarin Gal & Zoubin Ghahramani (2016)\n\n\nDeep neural networks achieve strong predictive accuracy but typically do not quantify uncertainty. This limitation is critical in applications like healthcare or autonomous systems, where overconfidence can lead to serious consequences. Traditional Bayesian neural networks can model uncertainty but are often computationally expensive and hard to train.\n\n\n\nThe authors demonstrate that standard dropout, a common regularization technique, can serve as an approximate Bayesian inference method. By keeping dropout active during both training and testing and performing multiple stochastic forward passes, the resulting distribution of predictions can be interpreted as samples from a posterior. This provides an efficient, scalable way to estimate model uncertainty without altering model architecture.\n\n\n\nThe paper shows that “Monte Carlo Dropout” (MC-Dropout) yields well-calibrated uncertainty estimates and competitive accuracy on tasks such as image classification (MNIST, CIFAR-10) and regression. The method improves out-of-distribution detection, active learning, and decision-making under uncertainty.\n\n\n\nThe uncertainty estimates depend on the dropout rate and can underestimate epistemic uncertainty for small or biased datasets. The approach is an approximation, not a full Bayesian posterior, so results may deviate from exact inference. Additional calibration steps may still be required.\n\n\n\nExperiments include standard image datasets (MNIST, CIFAR-10) and regression benchmarks from the UCI repository. Predictions are made using multiple forward passes with dropout enabled to approximate posterior mean and variance."
  },
  {
    "objectID": "Summaries/aw/paper4_summary.html#problem-the-article-is-addressing",
    "href": "Summaries/aw/paper4_summary.html#problem-the-article-is-addressing",
    "title": "Summary: Dropout as a Bayesian Approximation—Representing Model Uncertainty in Deep Learning",
    "section": "",
    "text": "Deep neural networks achieve strong predictive accuracy but typically do not quantify uncertainty. This limitation is critical in applications like healthcare or autonomous systems, where overconfidence can lead to serious consequences. Traditional Bayesian neural networks can model uncertainty but are often computationally expensive and hard to train."
  },
  {
    "objectID": "Summaries/aw/paper4_summary.html#how-it-has-been-solved",
    "href": "Summaries/aw/paper4_summary.html#how-it-has-been-solved",
    "title": "Summary: Dropout as a Bayesian Approximation—Representing Model Uncertainty in Deep Learning",
    "section": "",
    "text": "The authors demonstrate that standard dropout, a common regularization technique, can serve as an approximate Bayesian inference method. By keeping dropout active during both training and testing and performing multiple stochastic forward passes, the resulting distribution of predictions can be interpreted as samples from a posterior. This provides an efficient, scalable way to estimate model uncertainty without altering model architecture."
  },
  {
    "objectID": "Summaries/aw/paper4_summary.html#results",
    "href": "Summaries/aw/paper4_summary.html#results",
    "title": "Summary: Dropout as a Bayesian Approximation—Representing Model Uncertainty in Deep Learning",
    "section": "",
    "text": "The paper shows that “Monte Carlo Dropout” (MC-Dropout) yields well-calibrated uncertainty estimates and competitive accuracy on tasks such as image classification (MNIST, CIFAR-10) and regression. The method improves out-of-distribution detection, active learning, and decision-making under uncertainty."
  },
  {
    "objectID": "Summaries/aw/paper4_summary.html#limitations",
    "href": "Summaries/aw/paper4_summary.html#limitations",
    "title": "Summary: Dropout as a Bayesian Approximation—Representing Model Uncertainty in Deep Learning",
    "section": "",
    "text": "The uncertainty estimates depend on the dropout rate and can underestimate epistemic uncertainty for small or biased datasets. The approach is an approximation, not a full Bayesian posterior, so results may deviate from exact inference. Additional calibration steps may still be required."
  },
  {
    "objectID": "Summaries/aw/paper4_summary.html#datasets",
    "href": "Summaries/aw/paper4_summary.html#datasets",
    "title": "Summary: Dropout as a Bayesian Approximation—Representing Model Uncertainty in Deep Learning",
    "section": "",
    "text": "Experiments include standard image datasets (MNIST, CIFAR-10) and regression benchmarks from the UCI repository. Predictions are made using multiple forward passes with dropout enabled to approximate posterior mean and variance."
  },
  {
    "objectID": "Summaries/aw/paper6_summary.html",
    "href": "Summaries/aw/paper6_summary.html",
    "title": "Summary: Bayesian Model Averaging: A Practical Review",
    "section": "",
    "text": "Jennifer A. Hoeting, David Madigan, Adrian E. Raftery, & Chris T. Volinsky (1999)\n\n\nIn applied statistics and data science, analysts often face uncertainty about which model best represents the data. Standard practice is to select a single “best” model using criteria like AIC or BIC, but this ignores model uncertainty and leads to overconfident inferences and biased predictions. The paper addresses how to properly account for model uncertainty in statistical inference.\n\n\n\nThe authors propose Bayesian Model Averaging (BMA), a framework that combines predictions from multiple models weighted by their posterior probabilities. Instead of committing to a single model, BMA integrates over all plausible models using Bayes’ theorem. This approach accounts for uncertainty in both model selection and parameter estimation, yielding more robust predictions.\n\n\n\nBMA is shown to improve predictive performance and reduce overfitting across a variety of domains, including regression, classification, and time-series modeling. The method provides posterior model probabilities and model-averaged parameter estimates, offering a principled way to handle model uncertainty. The paper also presents examples and computational techniques for implementing BMA, such as Markov Chain Monte Carlo Model Composition (MC³).\n\n\n\nBMA can be computationally demanding when the number of candidate models is large, as it requires calculating and storing posterior probabilities for each. Its success also depends on reasonable prior specifications over both models and parameters. Simplified approximations (like Occam’s window) may introduce bias but are often necessary for feasibility.\n\n\n\nThe authors apply BMA to several real-world and simulated datasets, including linear regression examples and ecological modeling case studies. The paper focuses on demonstrating methodology rather than a specific dataset, emphasizing reproducible model-averaging workflows and sensitivity analysis."
  },
  {
    "objectID": "Summaries/aw/paper6_summary.html#problem-the-article-is-addressing",
    "href": "Summaries/aw/paper6_summary.html#problem-the-article-is-addressing",
    "title": "Summary: Bayesian Model Averaging: A Practical Review",
    "section": "",
    "text": "In applied statistics and data science, analysts often face uncertainty about which model best represents the data. Standard practice is to select a single “best” model using criteria like AIC or BIC, but this ignores model uncertainty and leads to overconfident inferences and biased predictions. The paper addresses how to properly account for model uncertainty in statistical inference."
  },
  {
    "objectID": "Summaries/aw/paper6_summary.html#how-it-has-been-solved",
    "href": "Summaries/aw/paper6_summary.html#how-it-has-been-solved",
    "title": "Summary: Bayesian Model Averaging: A Practical Review",
    "section": "",
    "text": "The authors propose Bayesian Model Averaging (BMA), a framework that combines predictions from multiple models weighted by their posterior probabilities. Instead of committing to a single model, BMA integrates over all plausible models using Bayes’ theorem. This approach accounts for uncertainty in both model selection and parameter estimation, yielding more robust predictions."
  },
  {
    "objectID": "Summaries/aw/paper6_summary.html#results",
    "href": "Summaries/aw/paper6_summary.html#results",
    "title": "Summary: Bayesian Model Averaging: A Practical Review",
    "section": "",
    "text": "BMA is shown to improve predictive performance and reduce overfitting across a variety of domains, including regression, classification, and time-series modeling. The method provides posterior model probabilities and model-averaged parameter estimates, offering a principled way to handle model uncertainty. The paper also presents examples and computational techniques for implementing BMA, such as Markov Chain Monte Carlo Model Composition (MC³)."
  },
  {
    "objectID": "Summaries/aw/paper6_summary.html#limitations",
    "href": "Summaries/aw/paper6_summary.html#limitations",
    "title": "Summary: Bayesian Model Averaging: A Practical Review",
    "section": "",
    "text": "BMA can be computationally demanding when the number of candidate models is large, as it requires calculating and storing posterior probabilities for each. Its success also depends on reasonable prior specifications over both models and parameters. Simplified approximations (like Occam’s window) may introduce bias but are often necessary for feasibility."
  },
  {
    "objectID": "Summaries/aw/paper6_summary.html#datasets",
    "href": "Summaries/aw/paper6_summary.html#datasets",
    "title": "Summary: Bayesian Model Averaging: A Practical Review",
    "section": "",
    "text": "The authors apply BMA to several real-world and simulated datasets, including linear regression examples and ecological modeling case studies. The paper focuses on demonstrating methodology rather than a specific dataset, emphasizing reproducible model-averaging workflows and sensitivity analysis."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk Using NHANES 2013–2014 Data",
    "section": "",
    "text": "Slides: slides.html (Edit slides.qmd.)"
  },
  {
    "objectID": "index.html#bayesian-logistic-regression",
    "href": "index.html#bayesian-logistic-regression",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk Using NHANES 2013–2014 Data",
    "section": "Bayesian Logistic Regression",
    "text": "Bayesian Logistic Regression\nThis study employs Bayesian logistic regression to estimate the association between predictors and a binary outcome.\nThe Bayesian framework integrates prior knowledge with observed data to generate posterior distributions, allowing parameters to be interpreted directly in probabilistic terms.\nUnlike traditional frequentist approaches that yield single-point estimates and p-values, Bayesian methods represent parameters as random variables with full probability distributions.\nThis provides greater flexibility, incorporates parameter uncertainty, and produces credible intervals that directly quantify the probability that a parameter lies within a given range."
  },
  {
    "objectID": "index.html#model-structure",
    "href": "index.html#model-structure",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk Using NHANES 2013–2014 Data",
    "section": "Model Structure",
    "text": "Model Structure\nBayesian logistic regression models the log-odds of a binary outcome as a linear combination of predictors:\n\\[\n\\text{logit}(P(Y = 1)) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_k X_k\n\\]\nwhere\n\n\\(P(Y = 1)\\) is the probability of the event of interest,\n\\(\\beta_0\\) is the intercept (log-odds when all predictors are zero), and\n\\(\\beta_j\\) represents the effect of predictor \\(X_j\\) on the log-odds of the outcome, holding other predictors constant.\n\nIn the Bayesian framework, model parameters (\\(\\boldsymbol{\\beta}\\)) are treated as random variables and assigned prior distributions that reflect existing knowledge or plausible ranges before observing the data. After incorporating the observed evidence, the priors are updated through Bayes’ theorem (Leeuw and Klugkist 2012; Klauenberg et al. 2015):\n\\[\n\\text{Posterior} \\propto \\text{Likelihood} \\times \\text{Prior}\n\\]\n\nLikelihood: represents the probability of the observed data given the model parameters—it captures how well different parameter values explain the data.\nPrior: expresses beliefs or existing information about the parameters before observing the data.\nPosterior: combines both, representing the updated distribution of parameter values after accounting for the data.\n\nThis formulation allows uncertainty to propagate naturally through the model, producing posterior distributions for each coefficient that can be directly interpreted as probabilities."
  },
  {
    "objectID": "index.html#prior-specification",
    "href": "index.html#prior-specification",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk Using NHANES 2013–2014 Data",
    "section": "Prior Specification",
    "text": "Prior Specification\nWeakly informative priors were used to regularize estimation without imposing strong assumptions:\n\nRegression coefficients: \\(N(0, 2.5)\\), providing gentle regularization while allowing substantial variation in plausible effects (Gelman et al. 2008; Vande Schoot et al. 2021).\nIntercept: Student’s t-distribution prior, \\(t(3, 0, 10)\\) (Schoot et al. 2013; Vande Schoot et al. 2021), which has\n\n3 degrees of freedom (heavy tails to allow occasional large effects),\nmean 0 (no bias toward positive or negative effects), and\nscale 10 (broad range of possible values).\n\n\nSuch priors help stabilize estimation in the presence of multicollinearity, limited sample size, or potential outliers."
  },
  {
    "objectID": "index.html#advantages-of-bayesian-logistic-regression",
    "href": "index.html#advantages-of-bayesian-logistic-regression",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk Using NHANES 2013–2014 Data",
    "section": "Advantages of Bayesian Logistic Regression",
    "text": "Advantages of Bayesian Logistic Regression\n\nUncertainty quantification: Produces full posterior distributions instead of single estimates.\nCredible intervals: Provide the range within which a parameter lies with a specified probability (e.g., 95%).\nFlexible priors: Allow integration of expert knowledge or findings from prior studies.\nProbabilistic predictions: Posterior predictive distributions yield direct probabilities for new or future observations.\nModel evaluation: Posterior predictive checks (PPCs) assess how well simulated outcomes reproduce observed data."
  },
  {
    "objectID": "index.html#posterior-predictions",
    "href": "index.html#posterior-predictions",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk Using NHANES 2013–2014 Data",
    "section": "Posterior Predictions",
    "text": "Posterior Predictions\nPosterior distributions of regression coefficients were used to estimate the probability of the outcome for given predictor values. This allows statements such as: &gt; “Given the predictors, the probability of the outcome lies between X% and Y%.”\nPosterior predictions account for two key sources of uncertainty:\n\nParameter uncertainty: Variability in estimated model coefficients.\nPredictive uncertainty: Variability in possible future outcomes given those parameters.\n\nIn Bayesian analysis, all unknown quantities—coefficients, means, variances, or probabilities—are treated as random variables described by their posterior distributions."
  },
  {
    "objectID": "index.html#model-evaluation-and-diagnostics",
    "href": "index.html#model-evaluation-and-diagnostics",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk Using NHANES 2013–2014 Data",
    "section": "Model Evaluation and Diagnostics",
    "text": "Model Evaluation and Diagnostics\nModel quality and convergence were assessed using standard Bayesian diagnostics:\n\nPosterior sampling: Conducted via Markov Chain Monte Carlo (MCMC) using the No-U-Turn Sampler (NUTS), a variant of Hamiltonian Monte Carlo (HMC) (Austin et al. 2021). Four chains were run with sufficient warm-up iterations to ensure convergence.\nConvergence metrics: The potential scale reduction factor (\\(\\hat{R}\\)) and effective sample size (ESS) were used to verify stability and mixing across chains.\nAutocorrelation checks: Ensured independence between successive draws.\nPosterior predictive checks (PPCs): Compared simulated outcomes to observed data to evaluate fit.\nBayesian \\(R^2\\): Quantified the proportion of variance explained by predictors, incorporating posterior uncertainty."
  },
  {
    "objectID": "index.html#data-preparation",
    "href": "index.html#data-preparation",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk Using NHANES 2013–2014 Data",
    "section": "Data Preparation",
    "text": "Data Preparation\nWe analyzed NHANES 2013–2014 public-use data from the CDC’s National Center for Health Statistics (National Center for Health Statistics (NCHS) 2014). Three component files were merged: demographics (DEMO_H), body measures (BMX_H), and the diabetes questionnaire (DIQ_H). All variables were coerced to consistent numeric or factor types prior to merging to ensure atomic columns suitable for survey analysis and modeling.\n\nImport and Merge Datasets\n\n\n\nPreview of merged NHANES 2013–2014 dataset limited to analysis variables (source columns only).\n\n\nRIDAGEYR\nBMXBMI\nRIAGENDR\nRIDRETH1\nDIQ010\n\n\n\n\n69\n26.7\n1\n4\n1\n\n\n54\n28.6\n1\n3\n1\n\n\n72\n28.9\n1\n3\n1\n\n\n9\n17.1\n1\n3\n2\n\n\n73\n19.7\n2\n3\n2\n\n\n56\n41.7\n1\n1\n2\n\n\n0\nNA\n1\n3\nNA\n\n\n61\n35.7\n2\n3\n2\n\n\n42\nNA\n1\n2\n2\n\n\n56\n26.5\n2\n3\n2\n\n\n\n\n\nThis preview shows the raw NHANES columns before transformation. Each variable is retained for later use in analysis and renamed or standardized as appropriate.\nThe merged dataset contains 10,175 participants. It integrates demographic, examination, and diabetes questionnaire data. We then restrict the sample to adults (age ≥ 20) to define the analytic cohort used in subsequent analyses. A small proportion of records have missing values in BMI and diabetes status, which will be addressed later through multiple imputation.\n\n\nAdult Cohort Definition\n\n\n\n\nTable 1: Variable Descriptions: Adult Analytic Dataset (NHANES 2013–2014)\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nNHANES_Source\nDescription\nType\n\n\n\n\nage\nRIDAGEYR\nParticipant age in years (adults aged 20 years and older)\nContinuous\n\n\nbmi\nBMXBMI\nBody Mass Index (BMI, kg/m²) measured during examination\nContinuous\n\n\nsex\nRIAGENDR\nSex of participant (Male or Female)\nCategorical\n\n\nrace4\nRIDRETH1\nRace/ethnicity collapsed into White, Black, Hispanic, and Other/Multi categories\nCategorical\n\n\ndiabetes_ind\nDIQ010\nDoctor-diagnosed diabetes indicator (1 = Yes, 0 = No)\nBinary\n\n\n\n\n\n\n\n\nThese summaries confirm that only BMI and diabetes indicators contain missing values, supporting the need for multiple imputation while keeping other variables complete.\n\n\nCode\n# Show structure and first few participants\n\nstr(adult)\n'data.frame':   5769 obs. of  11 variables:\n $ SDMVPSU     : num  1 1 1 2 1 1 2 1 2 2 ...\n $ SDMVSTRA    : num  112 108 109 116 111 114 106 112 112 113 ...\n $ WTMEC2YR    : num  13481 24472 57193 65542 25345 ...\n $ diabetes_ind: num  1 1 1 0 0 0 0 0 0 0 ...\n $ bmi         : num  26.7 28.6 28.9 19.7 41.7 35.7 NA 26.5 22 20.3 ...\n $ age         : num  69 54 72 73 56 61 42 56 65 26 ...\n $ sex         : Factor w/ 2 levels \"Female\",\"Male\": 2 2 2 1 2 1 2 1 2 1 ...\n $ race        : Factor w/ 5 levels \"Mexican American\",..: 2 3 3 3 1 3 4 3 3 3 ...\n $ age_c       : num [1:5769, 1] 1.132 0.278 1.303 1.36 0.392 ...\n  ..- attr(*, \"scaled:center\")= num 49.1\n  ..- attr(*, \"scaled:scale\")= num 17.6\n $ bmi_c       : num [1:5769, 1] -0.3359 -0.0703 -0.0283 -1.3144 1.761 ...\n  ..- attr(*, \"scaled:center\")= num 29.1\n  ..- attr(*, \"scaled:scale\")= num 7.15\n $ race4       : Factor w/ 4 levels \"White\",\"Hispanic\",..: 3 1 1 1 2 1 2 1 1 1 ...\nhead(adult, 10)\n   SDMVPSU SDMVSTRA WTMEC2YR diabetes_ind  bmi age    sex             race\n1        1      112 13481.04            1 26.7  69   Male         NH Black\n2        1      108 24471.77            1 28.6  54   Male         NH White\n3        1      109 57193.29            1 28.9  72   Male         NH White\n4        2      116 65541.87            0 19.7  73 Female         NH White\n5        1      111 25344.99            0 41.7  56   Male Mexican American\n6        1      114 61758.65            0 35.7  61 Female         NH White\n7        2      106     0.00            0   NA  42   Male   Other Hispanic\n8        1      112 17480.12            0 26.5  56 Female         NH White\n9        2      112 34795.43            0 22.0  65   Male         NH White\n10       2      113 91523.52            0 20.3  26 Female         NH White\n        age_c       bmi_c    race4\n1   1.1324183 -0.33588609    Black\n2   0.2783598 -0.07028101    White\n3   1.3032300 -0.02834336    White\n4   1.3601672 -1.31443114    White\n5   0.3922343  1.76099614 Hispanic\n6   0.6769204  0.92224325    White\n7  -0.4048870          NA Hispanic\n8   0.3922343 -0.36384452    White\n9   0.9046694 -0.99290919    White\n10 -1.3158827 -1.23055585    White\n\n\nDescriptive statistics for continuous and categorical variables are presented below.\n\n\n\nTable 1a. Continuous variables (age, BMI): N, missing, mean (SD), range.\n\n\nVariable\nN\nMissing\nMean\nSD\nMin\nMax\n\n\n\n\nage\n5769\n0\n49.11\n17.56\n20.0\n80.0\n\n\nbmi\n5520\n249\n29.10\n7.15\n14.1\n82.9\n\n\n\n\n\n\nTable 1b. Categorical variables (sex, race4, diabetes_ind): counts and percentages.\n\n\nVariable\nLevel\nn\npct\n\n\n\n\ndiabetes_ind\nNo\n4870\n84.4\n\n\ndiabetes_ind\nYes\n722\n12.5\n\n\ndiabetes_ind\n(Missing)\n177\n3.1\n\n\nrace4\nWhite\n2472\n42.8\n\n\nrace4\nHispanic\n1275\n22.1\n\n\nrace4\nBlack\n1177\n20.4\n\n\nrace4\nOther\n845\n14.6\n\n\nsex\nFemale\n3011\n52.2\n\n\nsex\nMale\n2758\n47.8\n\n\n\n\n\nTable 1a and 1b summarize the analytic variables included in subsequent models. Mean age and BMI values indicate an adult cohort spanning a wide range of body composition, while categorical summaries confirm balanced sex representation and sufficient sample sizes across race/ethnicity categories. These variables were standardized and used as predictors in all modeling frameworks.\n\n\n\n\nTable 2: Excerpt of the NHANES 2013–2014 adult cohort (age ≥ 20; N = 5,769) with derived and standardized variables.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSDMVPSU\nSDMVSTRA\nWTMEC2YR\ndiabetes_ind\nbmi\nage\nsex\nrace\nage_c\nbmi_c\nrace4\n\n\n\n\n1\n112\n13481.04\n1\n26.7\n69\nMale\nNH Black\n1.1324183\n-0.33588609\nBlack\n\n\n1\n108\n24471.77\n1\n28.6\n54\nMale\nNH White\n0.2783598\n-0.07028101\nWhite\n\n\n1\n109\n57193.29\n1\n28.9\n72\nMale\nNH White\n1.3032300\n-0.02834336\nWhite\n\n\n2\n116\n65541.87\n0\n19.7\n73\nFemale\nNH White\n1.3601672\n-1.31443114\nWhite\n\n\n1\n111\n25344.99\n0\n41.7\n56\nMale\nMexican American\n0.3922343\n1.76099614\nHispanic\n\n\n1\n114\n61758.65\n0\n35.7\n61\nFemale\nNH White\n0.6769204\n0.92224325\nWhite\n\n\n\n\n\n\n\n\nAs shown in Table 2, the analytic adult cohort (N = 5,769) includes standardized variables for age and BMI (age_c, bmi_c), categorical indicators for sex and race/ethnicity (race4), and a binary doctor-diagnosed diabetes variable (diabetes_ind).\n\n\nMissing Data Summary\n\n\n\n\n\n\n\n\nFigure 1: Missing data pattern for analytic variables (outcome and predictors only).\n\n\n\n\n\nThe missingness plot visually confirms that BMI and diabetes status have modest proportions of missing data, with no evident systematic pattern across records.\nMissingness was low (~4%) and primarily affected BMI and diabetes status. Patterns likely reflect data missing at random (MAR), as older adults or those with health limitations may be less likely to complete physical exams, supporting the use of MICE for bias reduction.\n\n\nExploratory Data Summary\nThe adult analytic cohort was broadly representative of the U.S. population, with a majority identifying as Non-Hispanic White. Age and BMI distributions were right-skewed, with most participants classified as overweight or obese. Visual exploration revealed a clear positive relationship between age, BMI, and diabetes prevalence. Non-Hispanic Black and Hispanic participants exhibited higher proportions of diabetes compared to Non-Hispanic Whites. Missingness was minimal and primarily limited to BMI and diabetes status, supporting the use of multiple imputation for these variables.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Age distribution (age ≥ 20).\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: BMI distribution.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Sex composition.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Race/ethnicity composition (race4).\n\n\n\n\n\nThe EDA missingness summary shows approximately 4.3% missing BMI and 3.1% missing diabetes status (diabetes_ind). All design variables (WTMEC2YR, SDMVPSU, SDMVSTRA), as well as age, sex, and race4, are complete—sex and race NAs are encoded as explicit “(Missing)” levels in the EDA view."
  },
  {
    "objectID": "index.html#modeling-frameworks",
    "href": "index.html#modeling-frameworks",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk Using NHANES 2013–2014 Data",
    "section": "Modeling Frameworks",
    "text": "Modeling Frameworks\nThree modeling frameworks were compared using identical predictors (standardized age, BMI, sex, and race4) and the binary outcome diabetes_ind: (1) survey-weighted logistic regression to incorporate the NHANES complex sampling design, (2) multiple imputation (MICE) to address missing BMI values, and (3) Bayesian logistic regression with weakly informative priors to quantify uncertainty.\n\nSurvey-Weighted Logistic Regression (Design-Based MLE)\n\n\n'data.frame':   5349 obs. of  5 variables:\n $ diabetes_ind: num  1 1 1 0 0 0 0 0 0 1 ...\n $ sex         : Factor w/ 2 levels \"Female\",\"Male\": 2 2 2 1 2 1 1 2 1 2 ...\n $ race4       : Factor w/ 4 levels \"White\",\"Hispanic\",..: 3 1 1 1 2 1 1 1 1 1 ...\n $ age_c       : num  1.132 0.278 1.303 1.36 0.392 ...\n $ bmi_c       : num  -0.3359 -0.0703 -0.0283 -1.3144 1.761 ...\n\n\nDesign-based odds ratios are summarized in Table 3.\n\n\n\n\nTable 3: Survey-weighted logistic regression: odds ratios (OR) and 95% confidence intervals for diabetes diagnosis among adults (NHANES 2013–2014).\n\n\n\n\n\n\nterm\nOR\nLCL\nUCL\np.value\n\n\n\n\nage_c\n2.977668\n2.704677\n3.278212\n0.0000000\n\n\nbmi_c\n1.930284\n1.679190\n2.218924\n0.0000021\n\n\nsexMale\n1.287236\n1.018664\n1.626618\n0.0373081\n\n\nrace4Hispanic\n1.809943\n1.428957\n2.292507\n0.0003024\n\n\nrace4Black\n1.599844\n1.157393\n2.211434\n0.0094754\n\n\nrace4Other\n2.195356\n1.461219\n3.298334\n0.0017976\n\n\n\n\n\n\n\n\nThe NHANES 2013–2014 data use a complex, multistage probability design involving strata (SDMVSTRA), primary sampling units (PSUs; SDMVPSU), and examination weights (WTMEC2YR) to ensure nationally representative estimates (National Center for Health Statistics (NCHS) 2014).\nEstimates are population-weighted using NHANES survey design variables (WTMEC2YR, SDMVSTRA, SDMVPSU). Odds ratios are reported per one standard deviation (1 SD) increase in age and BMI, with reference groups Male and White.\n\n\nMultiple Imputation (MICE)\nMultiple Imputation by Chained Equations (MICE) was used as a principled approach for handling missing data (Stef van Buuren and Groothuis-Oudshoorn 2011; S. van Buuren 2012).\nMICE iteratively imputes each incomplete variable using regression models based on other variables in the dataset, producing multiple completed datasets that reflect uncertainty due to missingness. Estimates are then pooled across imputations using Rubin’s rules to generate final parameter estimates and confidence intervals.\nMICE, as an alternative to the Bayesian approach, effectively manages missing data through chained regression equations without requiring full joint modeling of all variables.\nFor large sample sizes (n ≥ 400), even in the presence of high percentages (up to 75%) of missing data in one variable, non-normal distributions such as flat densities, heavy tails, skewness, and multimodality do not materially affect mean structure estimation performance (S. van Buuren 2012).\nIn this study, continuous variables (age and BMI) were imputed using predictive mean matching (PMM) to preserve realistic distributions, while categorical variables (sex and race4) were imputed using logistic and polytomous regression models, respectively. Diabetes status (diabetes_ind) was treated as an outcome variable and was not imputed. Twenty imputations were generated to reduce Monte Carlo error and maintain robust variance estimation.\n\n\n\n\nTable 4: Bayesian logistic regression: posterior odds ratios (OR) with 95% credible intervals.\n\n\n\n\n\n\nterm\nOR\nLCL\nUCL\n\n\n\n\nIntercept\n0.06\n0.05\n0.07\n\n\nage_c\n2.93\n2.62\n3.30\n\n\nbmi_c\n1.92\n1.76\n2.10\n\n\nsexMale\n1.28\n1.06\n1.55\n\n\nrace4Hispanic\n1.82\n1.40\n2.38\n\n\nrace4Black\n1.62\n1.23\n2.12\n\n\nrace4Other\n2.11\n1.46\n2.99\n\n\n\n\n\n\n\n\nMultiple imputation preserves sample size and reduces bias from missing BMI values. Results closely mirror the survey-weighted model, confirming robustness to imputation.\n\n\n\n\nTable 5: Multiple Imputation (MICE): pooled odds ratios (OR) and 95% confidence intervals after imputing missing BMI (PMM) (m = 20); diabetes status was not imputed. Odds ratios are per 1 SD for age and BMI.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nOR\nstd.error\nstatistic\ndf\np.value\nLCL\nUCL\nconf.low\nconf.high\n\n\n\n\n2\nscale(age)\n2.8956646\n0.0524433\n20.273592\n5499.574\n0.0000000\n2.6127544\n3.2092084\n2.6127544\n3.2092084\n\n\n3\nscale(bmi)\n1.8053391\n0.0430294\n13.728961\n3877.226\n0.0000000\n1.6592839\n1.9642506\n1.6592839\n1.9642506\n\n\n4\nrelevel(sex, “Male”)Female\n0.8056102\n0.0872427\n-2.477631\n5545.705\n0.0132553\n0.6789653\n0.9558776\n0.6789653\n0.9558776\n\n\n5\nrelevel(race4, “White”)Hispanic\n2.0741944\n0.1128115\n6.467183\n5562.036\n0.0000000\n1.6626591\n2.5875915\n1.6626591\n2.5875915\n\n\n6\nrelevel(race4, “White”)Black\n1.7931172\n0.1137153\n5.135240\n5508.172\n0.0000003\n1.4348045\n2.2409110\n1.4348045\n2.2409110\n\n\n7\nrelevel(race4, “White”)Other\n2.0011166\n0.1443011\n4.807344\n5464.535\n0.0000016\n1.5080503\n2.6553940\n1.5080503\n2.6553940\n\n\n\n\n\n\n\n\n\n\nBayesian Logistic Regression\nBayesian logistic regression was implemented using the following model specification:\nFormula:\ndiabetes_ind | weights(wt_norm) ~ age_c + bmi_c + sex + race4\n\n\n\n\n\n\n\n\n\n\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 10.5 seconds.\nChain 2 finished in 10.3 seconds.\nChain 3 finished in 10.8 seconds.\nChain 4 finished in 10.7 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 10.6 seconds.\nTotal execution time: 42.7 seconds.\n\n\nPosterior odds ratios and credible intervals from the Bayesian logistic regression are shown in Table 4.\nAs shown in Table 4, the Bayesian logistic regression model estimated the log-odds of diabetes using standardized predictors. Weakly informative priors (\\(N(0, 2.5)\\) for slopes, Student-t(3, 0, 10) for the intercept) stabilized estimation and prevented overfitting. The model used normalized NHANES exam weights as importance weights to approximate design-based inference. Posterior means and 95% credible intervals provided full uncertainty quantification for each predictor.\nPosterior summaries were further evaluated using the Bayesian \\(R^2\\), which estimates the proportion of outcome variance explained by model predictors.\nModel-level performance is summarized in Table 6.\n\n\n\n\nTable 6: Bayesian R² summary.\n\n\n\n\n\n\n\nEstimate\nEst.Error\nQ2.5\nQ97.5\n\n\n\n\nR2\n0.1380082\n0.0118579\n0.115269\n0.1616909\n\n\n\n\n\n\n\n\n\n\n\n\nTable 7: MCMC diagnostics (R-hat and Effective Sample Sizes) for model parameters (including intercept).\n\n\n\n\n\n\nParameter\nRhat\nBulk_ESS\nTail_ESS\n\n\n\n\nb_Intercept\n1\n2721.2\n2817.8\n\n\nb_age_c\n1\n2605.7\n2704.7\n\n\nb_bmi_c\n1\n3229.5\n2783.0\n\n\nb_sexMale\n1\n3613.7\n3128.7\n\n\nb_race4Hispanic\n1\n3805.4\n3109.7\n\n\nb_race4Black\n1\n3685.6\n3133.0\n\n\nb_race4Other\n1\n3513.0\n2637.5\n\n\n\n\n\n\n\n\nAll model parameters achieved R̂ values approximately equal to 1.00 and bulk/tail effective sample sizes exceeding 2,000, confirming strong convergence and well-mixed chains. The Bayesian R² was approximately 0.13, indicating that age, BMI, sex, and race collectively explained about 13% of variability in diabetes risk at the population level.\nModel comparison results using leave-one-out cross-validation are presented below.\n\n\n\nBayesian model comparison (LOO): base model vs. models without race or without sex.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nelpd_diff\nse_diff\nelpd_loo\nse_elpd_loo\np_loo\nse_p_loo\nlooic\nse_looic\n\n\n\n\nbayes_fit\nbayes_fit\n0.000000\n0.000000\n-1574.573\n56.98453\n8.291612\n0.5425672\n3149.146\n113.9691\n\n\nfit_no_sex\nfit_no_sex\n-2.126325\n3.296627\n-1576.699\n57.10511\n6.569758\n0.4419557\n3153.399\n114.2102\n\n\nfit_no_race\nfit_no_race\n-14.748276\n6.302564\n-1589.321\n54.52053\n5.467927\n0.3638458\n3178.643\n109.0411\n\n\n\n\n\nLeave-one-out (LOO) cross-validation showed that models excluding race or sex had lower expected log predictive density (elpd), indicating a poorer fit. This supports the inclusion of both variables as meaningful contributors to predictive performance and overall model adequacy.\nFigures below visualize posterior distributions, MCMC diagnostics, and model fit.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: Posterior distributions (95% credible mass) for slope parameters.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7: Trace plots for slope parameters (chain mixing and stationarity).\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8: Autocorrelation plots for posterior samples of age and BMI coefficients (MCMC diagnostics).\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 9: Posterior density areas (95% credible mass) for age, BMI, sex, and race coefficients.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 10: Posterior predictive check: observed vs. replicated outcome distribution (bars).\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 11: Posterior predictive checks for mean and standard deviation of the binary outcome.\n\n\n\n\n\n\n\n\n\n\n\nFigure 12: Posterior predictive checks for mean and standard deviation of the binary outcome.\n\n\n\n\n\n\n\nModel Fit and Calibration\nPosterior predictive checks showed that simulated outcome distributions closely matched the observed diabetes prevalence, indicating strong model calibration. Both the mean and standard deviation of replicated outcomes aligned with observed data, suggesting the model adequately captured central tendency and dispersion. These results provide graphical evidence of good fit and reinforce that the priors did not unduly constrain the posterior.\n\n\n\n\n\n\n\n\nFigure 13: Posterior odds ratios (points) with 95% credible intervals (lines).\n\n\n\n\n\nCalibration between predicted and observed diabetes probabilities is displayed in Figure 14.\n\n\n\n\n\n\n\n\nFigure 14: Observed outcome vs. mean predicted probability (calibration scatter with smoother).\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 15: Posterior predictive distribution of diabetes prevalence compared to observed NHANES prevalence.\n\n\n\n\n\n\n\n\n\n\n\nFigure 16: Posterior predictive distribution of diabetes prevalence compared to observed NHANES prevalence.\n\n\n\n\n\nThe posterior predictive distribution of diabetes prevalence closely mirrored the survey-estimated prevalence, with the posterior mean aligning within 1% of the observed rate. This indicates that the Bayesian model accurately reproduced the population-level prevalence and supports its calibration for epidemiologic inference.\n\n\n\n\n\n\n\n\nFigure 17: Population (NHANES survey-weighted) vs posterior predictive diabetes prevalence.\n\n\n\n\n\n\n\n[1] \"b_age_c\"         \"b_bmi_c\"         \"b_Intercept\"     \"b_race4Black\"   \n[5] \"b_race4Hispanic\" \"b_race4Other\"    \"b_sexMale\"      \n\n\n\n\n\n\nNo matching prior/posterior parameters found to overlay.\n\n\nFigure 18: Prior (dashed) vs posterior (solid) densities for selected coefficients.\n\n\n\n\n\n\n\n\nSkipped: no matching prior/posterior draws to plot.\n\n\nFigure 19: Prior vs Posterior Distributions (ggplot2 version).\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 20\n\n\n\n\n\nFor age and BMI, the posterior densities shift notably away from the N(0, 2.5) prior toward positive values and are narrower, indicating strong information from the data; for sex, the posterior remains closer to the prior with more overlap, indicating weaker evidence.\nThe overlay of prior and posterior densities illustrates that informative updates occurred primarily for BMI, age, and race coefficients, which showed distinct posterior shifts relative to the priors. In contrast, weaker predictors such as sex displayed overlapping distributions, indicating that inference for those parameters was more influenced by prior uncertainty than by the observed data. This balance confirms appropriate regularization rather than overfitting."
  },
  {
    "objectID": "index.html#results",
    "href": "index.html#results",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk Using NHANES 2013–2014 Data",
    "section": "Results",
    "text": "Results\nA concise summary of posterior estimates is provided below.\n\nPopulation-level interpretation (posterior, odds ratios)\n\nConvergence. All R-hat ≈ 1.00; large ESS → excellent mixing.\nBaseline risk. Male, White, mean age/BMI: ~5.2% predicted diabetes prevalence.\nAge. +1 SD → 2.93× (95% CrI 2.62–3.30; CrI excludes 1).\nBMI. +1 SD → 1.92× (95% CrI 1.76–2.10; CrI excludes 1).\nFemale vs. Male. NA× (95% CrI NA–NA; CrI overlaps 1).\nBlack vs. White. 1.62× (95% CrI 1.23–2.12; CrI excludes 1).\nHispanic vs. White. 1.82× (95% CrI 1.40–2.38; CrI excludes 1).\nOther/Multi vs. White. 2.11× (95% CrI 1.46–2.99; CrI excludes 1).\n\nComparative odds ratios across frameworks are shown in Table 8.\n\n\n\n\nTable 8: Comparison of odds ratios (per 1 SD for age and BMI) and 95% intervals across survey-weighted, MICE, and Bayesian frameworks.\n\n\n\n\n\n\n\n\n\n\n\nModel\nterm\nOR_CI\n\n\n\n\nSurvey-weighted (MLE)\nAge (per 1 SD)\n2.98 (2.70 – 3.28)\n\n\nSurvey-weighted (MLE)\nBMI (per 1 SD)\n1.93 (1.68 – 2.22)\n\n\nSurvey-weighted (MLE)\nMale (vs. Female)\n1.29 (1.02 – 1.63)\n\n\nSurvey-weighted (MLE)\nHispanic (vs. White)\n1.81 (1.43 – 2.29)\n\n\nSurvey-weighted (MLE)\nBlack (vs. White)\n1.60 (1.16 – 2.21)\n\n\nSurvey-weighted (MLE)\nOther (vs. White)\n2.20 (1.46 – 3.30)\n\n\nMICE Pooled\nscale(age)\n2.90 (2.61 – 3.21)\n\n\nMICE Pooled\nscale(bmi)\n1.81 (1.66 – 1.96)\n\n\nMICE Pooled\nrelevel(sex, “Male”)Female\n0.81 (0.68 – 0.96)\n\n\nMICE Pooled\nrelevel(race4, “White”)Hispanic\n2.07 (1.66 – 2.59)\n\n\nMICE Pooled\nrelevel(race4, “White”)Black\n1.79 (1.43 – 2.24)\n\n\nMICE Pooled\nrelevel(race4, “White”)Other\n2.00 (1.51 – 2.66)\n\n\nBayesian\nAge (per 1 SD)\n2.93 (2.62 – 3.30)\n\n\nBayesian\nBMI (per 1 SD)\n1.92 (1.76 – 2.10)\n\n\nBayesian\nMale (vs. Female)\n1.28 (1.06 – 1.55)\n\n\nBayesian\nHispanic (vs. White)\n1.82 (1.40 – 2.38)\n\n\nBayesian\nBlack (vs. White)\n1.62 (1.23 – 2.12)\n\n\nBayesian\nOther (vs. White)\n2.11 (1.46 – 2.99)\n\n\n\n\n\n\n\n\nThis table summarizes results from the survey-weighted (design-based), multiple-imputation, and Bayesian models.\nThe Bayesian model’s credible intervals closely align with the frequentist confidence intervals but provide a more direct probabilistic interpretation of uncertainty.\nAcross all three frameworks—survey-weighted (MLE), multiple imputation, and Bayesian—age and BMI were consistently associated with higher odds of doctor-diagnosed diabetes. Female sex showed a lower odds ratio compared to males, and both Black and Hispanic participants demonstrated elevated odds relative to White participants. The similarity of effect sizes across frameworks underscores the robustness of these predictors to different modeling assumptions and missing-data treatments. Bayesian credible intervals largely overlapped frequentist confidence intervals, confirming stability of inference while allowing direct probabilistic interpretation."
  },
  {
    "objectID": "index.html#discussion-and-limitations",
    "href": "index.html#discussion-and-limitations",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk Using NHANES 2013–2014 Data",
    "section": "Discussion and Limitations",
    "text": "Discussion and Limitations\n\nInterpretation\nThe Bayesian logistic regression framework produced results that were highly consistent with both the survey-weighted and MICE-pooled frequentist models. Age and BMI remained the most influential predictors of doctor-diagnosed diabetes, each showing a strong and positive association with diabetes risk.\nUnlike classical maximum likelihood estimation, the Bayesian approach directly quantified uncertainty through posterior distributions, offering richer interpretability and more transparent probability statements. The alignment between Bayesian and design-based estimates supports the robustness of these associations and highlights the practicality of Bayesian modeling for complex, weighted population data.\nPosterior predictive checks confirmed that simulated diabetes prevalence closely matched the observed NHANES estimate, supporting good model calibration. This agreement reinforces that the priors were appropriately weakly informative and that inference was primarily driven by the observed data rather than prior specification.\nOverall, this study demonstrates that Bayesian inference complements traditional epidemiologic methods by maintaining interpretability while enhancing stability and explicitly quantifying uncertainty in complex survey data. These consistent findings across modeling frameworks underscore the robustness of core risk factors and support the use of Bayesian inference for epidemiologic research involving complex survey data.\n\n\nLimitations\nWhile this analysis demonstrates the value of Bayesian logistic regression for epidemiologic modeling, several limitations should be acknowledged.\nFirst, the use of a single imputed dataset for the Bayesian model—rather than full joint modeling of imputation uncertainty—may understate total variance.\nSecond, NHANES exam weights were normalized and treated as importance weights, which approximate but do not fully reproduce design-based inference.\nThird, the weakly informative priors \\(N(0, 2.5)\\) for slopes and Student-t(3, 0, 10) for the intercept were not empirically tuned; alternative prior specifications could slightly alter posterior intervals.\nFinally, although convergence diagnostics (R̂ ≈ 1, sufficient effective sample sizes, and stable posterior predictive checks) indicated good model performance, results are conditional on the 2013–2014 NHANES cycle and may not generalize to later datasets or longitudinal analyses."
  },
  {
    "objectID": "index.html#conclusion",
    "href": "index.html#conclusion",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk Using NHANES 2013–2014 Data",
    "section": "Conclusion",
    "text": "Conclusion\nThe Bayesian, survey-weighted, and imputed logistic regression frameworks all identified consistent predictors of diabetes risk in U.S. adults: advancing age, higher BMI, sex (lower odds for females), and non-White race/ethnicity.\nThe Bayesian model produced estimates nearly identical in direction and magnitude to the frequentist results while providing a more comprehensive assessment of uncertainty through posterior distributions and credible intervals.\nThese consistent findings across modeling frameworks underscore the robustness of core risk factors and support the use of Bayesian inference for epidemiologic research involving complex survey data.\nBy incorporating prior information and using MCMC to sample from the full posterior distribution, Bayesian inference enhances model transparency and interpretability.\nIts agreement with traditional approaches underscores that Bayesian methods can be applied confidently in large-scale public health datasets such as NHANES.\nFuture extensions could integrate hierarchical priors, multiple NHANES cycles, or Bayesian model averaging to better capture population heterogeneity, temporal trends, and evolving diabetes risk patterns."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Us",
    "section": "",
    "text": "Dr. Namita Mishra is a physician, Head and Neck surgeon, and public health researcher with a strong foundation in medicine, epidemiology, and data science. She is currently a graduate student in Data Science (Health Analytics).\nHer work focuses on the early detection and prevention of non-communicable diseases (cancer, obesity) and on health disparities at the community level. She has conducted research on salivary gland tumors, cardiac implants, and community-based healthy food access. Leveraging her skills in Data Science, she integrates statistical modeling and Bayesian methods into her analyses. Her bioinformatics expertise includes using geodata visualization tools (3D maps and GIS) for presentations. Passionate about bridging clinical insight with data-driven approaches, she is dedicated to advancing sustainable, evidence-based solutions in epidemiology and community health.\nOutside of work, she enjoys gardening, cooking, singing, and sewing.\nDeveloped the project plan, content draft, analytic coding, and coordinated commits and collaboration with the group on GitHub.\n📧 Contact: nmishra@uwf.edu\n\nAutumn S. Wilcox is a U.S. Navy veteran and Data Science graduate student at the University of West Florida, specializing in Analytics and Modeling. She has over nine years of experience in Network Operations and Technical Writing, including her current role at Navy Federal Credit Union, where she supports enterprise technology and process documentation initiatives. Autumn also holds certification in Clinical Research Quality Management (CRQM) and has contributed to quality oversight and compliance efforts in clinical research settings.\nHer background bridges technology, analytics, and healthcare, with a focus on applying data-driven approaches to improve communication and systems reliability. Outside of work, Autumn enjoys traveling, photography, and finding creative inspiration through music.\nContributed to analytic coding, content draft, structured the project workflow, and collaborated actively via GitHub.\n📧 Contact: awr12@students.uwf.edu"
  },
  {
    "objectID": "Summaries/aw/paper3_summary.html",
    "href": "Summaries/aw/paper3_summary.html",
    "title": "Bayesian Logistic Regression Capstone",
    "section": "",
    "text": "Paper 3: Abdullah, Hassan, & Mustafa (2022). “A Review on Bayesian Deep Learning in Healthcare: Applications and Challenges”\nGoal\nThe paper systematically reviews how Bayesian deep learning (BDL) is being applied in healthcare: its use cases, methodological approaches, and the challenges and future directions.\nImportance\nHealthcare data is often uncertain (noisy measurements, missing data, variability) and involves high stakes where mistakes can cost lives. While deep learning is powerful, it typically lacks mechanisms for representing uncertainty or handling limited data in a principled way. Bayesian techniques address these gaps by incorporating uncertainty, prior knowledge, and probabilistic reasoning—making models potentially safer, more trustworthy, and interpretable in clinical settings.\nMethods\n- Conduct a literature survey of recent work combining Bayesian methods with deep learning in healthcare.\n- Categorize approaches such as variational inference, Monte Carlo dropout, ensemble methods, Gaussian processes, and Bayesian neural networks.\n- Map these methods to healthcare tasks like diagnosis, prognosis, and treatment planning.\n- Evaluate strengths/weaknesses in terms of data availability, computational costs, interpretability, uncertainty calibration, and privacy.\nResults & Limitations\n- Results: BDL has shown success in disease classification, survival analysis, medical image segmentation, and predictive modeling. It often improves uncertainty quantification, may improve generalization, and provides clinicians with added information (e.g., confidence in predictions).\n- Limitations: High computational demands, scalability issues, difficulty specifying priors, and interpretability challenges remain. Many studies are proof-of-concept and lack validation in real-world clinical environments. Regulatory, privacy, and workflow integration concerns also limit deployment."
  },
  {
    "objectID": "Summaries/aw/paper1_summary.html",
    "href": "Summaries/aw/paper1_summary.html",
    "title": "Summary: Bayesian Data Analysis for Newcomers",
    "section": "",
    "text": "John K. Kruschke and Torrin M. Liddell (2017)\n\n\nThe paper tackles the issue that Bayesian data analysis is often viewed as too complicated or intimidating for newcomers. Traditional explanations rely heavily on mathematical formulas, which discourages students and researchers who want to understand the ideas without advanced math. The authors aim to show that Bayesian reasoning is actually natural and aligns with how people already think in everyday life.\n\n\n\nThe authors present Bayesian concepts in plain language with minimal technical notation. They explain the main idea of Bayesian analysis as reallocating credibility among possible explanations as new data comes in. Simple examples are used to show how priors, likelihoods, and posteriors work together. They also clarify common misunderstandings, such as the role of prior distributions and how Bayesian inference differs from frequentist methods like null hypothesis significance testing.\n\n\n\nThe article provides readers with a clear conceptual framework for Bayesian analysis. Instead of being overwhelmed by formulas, the reader can see how Bayesian methods produce intuitive and interpretable results. Examples highlight how Bayesian outputs, such as posterior distributions and credible intervals, can be directly applied to research questions.\n\n\n\nBecause the article is written as an introduction, it does not go into technical detail or advanced modeling. It is not a substitute for mathematical training or for learning to implement Bayesian models in practice. The focus is on clarity rather than rigor.\n\n\n\nThe paper does not use any real-world datasets. Instead, it relies on simple examples meant to illustrate the principles of Bayesian reasoning. These examples are conceptual rather than empirical, making the ideas easier to grasp for newcomers like myself."
  },
  {
    "objectID": "Summaries/aw/paper1_summary.html#problem-the-article-is-addressing",
    "href": "Summaries/aw/paper1_summary.html#problem-the-article-is-addressing",
    "title": "Summary: Bayesian Data Analysis for Newcomers",
    "section": "",
    "text": "The paper tackles the issue that Bayesian data analysis is often viewed as too complicated or intimidating for newcomers. Traditional explanations rely heavily on mathematical formulas, which discourages students and researchers who want to understand the ideas without advanced math. The authors aim to show that Bayesian reasoning is actually natural and aligns with how people already think in everyday life."
  },
  {
    "objectID": "Summaries/aw/paper1_summary.html#how-it-has-been-solved",
    "href": "Summaries/aw/paper1_summary.html#how-it-has-been-solved",
    "title": "Summary: Bayesian Data Analysis for Newcomers",
    "section": "",
    "text": "The authors present Bayesian concepts in plain language with minimal technical notation. They explain the main idea of Bayesian analysis as reallocating credibility among possible explanations as new data comes in. Simple examples are used to show how priors, likelihoods, and posteriors work together. They also clarify common misunderstandings, such as the role of prior distributions and how Bayesian inference differs from frequentist methods like null hypothesis significance testing."
  },
  {
    "objectID": "Summaries/aw/paper1_summary.html#results",
    "href": "Summaries/aw/paper1_summary.html#results",
    "title": "Summary: Bayesian Data Analysis for Newcomers",
    "section": "",
    "text": "The article provides readers with a clear conceptual framework for Bayesian analysis. Instead of being overwhelmed by formulas, the reader can see how Bayesian methods produce intuitive and interpretable results. Examples highlight how Bayesian outputs, such as posterior distributions and credible intervals, can be directly applied to research questions."
  },
  {
    "objectID": "Summaries/aw/paper1_summary.html#limitations",
    "href": "Summaries/aw/paper1_summary.html#limitations",
    "title": "Summary: Bayesian Data Analysis for Newcomers",
    "section": "",
    "text": "Because the article is written as an introduction, it does not go into technical detail or advanced modeling. It is not a substitute for mathematical training or for learning to implement Bayesian models in practice. The focus is on clarity rather than rigor."
  },
  {
    "objectID": "Summaries/aw/paper1_summary.html#datasets",
    "href": "Summaries/aw/paper1_summary.html#datasets",
    "title": "Summary: Bayesian Data Analysis for Newcomers",
    "section": "",
    "text": "The paper does not use any real-world datasets. Instead, it relies on simple examples meant to illustrate the principles of Bayesian reasoning. These examples are conceptual rather than empirical, making the ideas easier to grasp for newcomers like myself."
  },
  {
    "objectID": "Summaries/aw/paper5_summary.html",
    "href": "Summaries/aw/paper5_summary.html",
    "title": "Summary: The No-U-Turn Sampler (NUTS)—Adaptively Setting Path Lengths in Hamiltonian Monte Carlo",
    "section": "",
    "text": "Matthew D. Hoffman & Andrew Gelman (2014)\n\n\nHamiltonian Monte Carlo (HMC) offers efficient sampling from complex posteriors but requires manual tuning of the trajectory length (number of leapfrog steps). Poor tuning can cause slow convergence or biased sampling, making HMC difficult for non-experts to use effectively.\n\n\n\nThe authors introduce the No-U-Turn Sampler (NUTS), an adaptive extension of HMC that automatically determines the optimal trajectory length. NUTS expands the trajectory until it detects a “U-turn” in parameter space—meaning further movement would retrace steps—and then stops. Combined with dual-averaging step-size adaptation, this removes nearly all manual tuning while preserving the sampling efficiency of HMC.\n\n\n\nAcross hierarchical models and logistic regression tasks, NUTS produces higher effective sample sizes per second and improved convergence diagnostics compared to traditional HMC or Gibbs sampling. It is robust across diverse posterior geometries and scales well with model complexity, forming the default MCMC algorithm in Stan.\n\n\n\nWhile adaptive, NUTS still relies on differentiable log-posteriors and can be computationally intensive for extremely complex or multimodal distributions. Performance depends on the choice of priors and model reparameterization, and it remains unsuitable for discrete or non-differentiable models.\n\n\n\nThe paper evaluates NUTS on simulated hierarchical models and real logistic regression datasets, reporting metrics like effective sample size (ESS) and Gelman–Rubin convergence diagnostics (()) to assess efficiency and accuracy."
  },
  {
    "objectID": "Summaries/aw/paper5_summary.html#problem-the-article-is-addressing",
    "href": "Summaries/aw/paper5_summary.html#problem-the-article-is-addressing",
    "title": "Summary: The No-U-Turn Sampler (NUTS)—Adaptively Setting Path Lengths in Hamiltonian Monte Carlo",
    "section": "",
    "text": "Hamiltonian Monte Carlo (HMC) offers efficient sampling from complex posteriors but requires manual tuning of the trajectory length (number of leapfrog steps). Poor tuning can cause slow convergence or biased sampling, making HMC difficult for non-experts to use effectively."
  },
  {
    "objectID": "Summaries/aw/paper5_summary.html#how-it-has-been-solved",
    "href": "Summaries/aw/paper5_summary.html#how-it-has-been-solved",
    "title": "Summary: The No-U-Turn Sampler (NUTS)—Adaptively Setting Path Lengths in Hamiltonian Monte Carlo",
    "section": "",
    "text": "The authors introduce the No-U-Turn Sampler (NUTS), an adaptive extension of HMC that automatically determines the optimal trajectory length. NUTS expands the trajectory until it detects a “U-turn” in parameter space—meaning further movement would retrace steps—and then stops. Combined with dual-averaging step-size adaptation, this removes nearly all manual tuning while preserving the sampling efficiency of HMC."
  },
  {
    "objectID": "Summaries/aw/paper5_summary.html#results",
    "href": "Summaries/aw/paper5_summary.html#results",
    "title": "Summary: The No-U-Turn Sampler (NUTS)—Adaptively Setting Path Lengths in Hamiltonian Monte Carlo",
    "section": "",
    "text": "Across hierarchical models and logistic regression tasks, NUTS produces higher effective sample sizes per second and improved convergence diagnostics compared to traditional HMC or Gibbs sampling. It is robust across diverse posterior geometries and scales well with model complexity, forming the default MCMC algorithm in Stan."
  },
  {
    "objectID": "Summaries/aw/paper5_summary.html#limitations",
    "href": "Summaries/aw/paper5_summary.html#limitations",
    "title": "Summary: The No-U-Turn Sampler (NUTS)—Adaptively Setting Path Lengths in Hamiltonian Monte Carlo",
    "section": "",
    "text": "While adaptive, NUTS still relies on differentiable log-posteriors and can be computationally intensive for extremely complex or multimodal distributions. Performance depends on the choice of priors and model reparameterization, and it remains unsuitable for discrete or non-differentiable models."
  },
  {
    "objectID": "Summaries/aw/paper5_summary.html#datasets",
    "href": "Summaries/aw/paper5_summary.html#datasets",
    "title": "Summary: The No-U-Turn Sampler (NUTS)—Adaptively Setting Path Lengths in Hamiltonian Monte Carlo",
    "section": "",
    "text": "The paper evaluates NUTS on simulated hierarchical models and real logistic regression datasets, reporting metrics like effective sample size (ESS) and Gelman–Rubin convergence diagnostics (()) to assess efficiency and accuracy."
  },
  {
    "objectID": "Summaries/nm/My_articles.html",
    "href": "Summaries/nm/My_articles.html",
    "title": "Bayesian Logistic Regression Capstone",
    "section": "",
    "text": "Namita’s Literature"
  },
  {
    "objectID": "Summaries/nm/My_articles.html#introduction",
    "href": "Summaries/nm/My_articles.html#introduction",
    "title": "Bayesian Logistic Regression Capstone",
    "section": "Introduction",
    "text": "Introduction\n\nBayesian Hierarchical Model (Disease reclassification and prediction)\n\nWhat is the goal of the paper?\nThe authors develop a Bayesian hierarchical model for multivariate longitudinal data to predict health status, trajectories, and intervention effects at the individual level in the PCORI mission to address questions about health status from patients and clinicians.\nWhy is it important?\nHealthcare data (DNA sequences, functional images of the brain, patient-reported outcomes, and electronic health records with patients’ sequences of health measurements, diagnoses, and treatments) are complex, and the standard approaches are not adequate for clinical data analysis. Electronic health records (EHRs) could improve diagnostic accuracy and predict treatment effects. Visualizations of characteristics of posterior distributions can be immediately understood by clinicians and patients as relevant to their decision. Combining prior knowledge and patient data with evidence could predict the patient’s health status, trajectory, and/or likely benefits of interventions.\nHow is it solved?\nMethod: The authors applied Bayesian hierarchical regression for multivariate longitudinal patient data using open-source R-packages and developed 2 levels—time within person and persons within a population\nThe model combined exogenous (eg, age, clinical history) factors and endogenous (eg, current treatment) variables on the individual’s multivariate health measurements and the effects of health measurements at one time on subsequent interventions.\nThe model produced an estimate of the posterior distribution for each value of the predictor variable and an estimate of the marginal distribution of the regression coefficients for each coefficient that measures the outcome (health status) associated with its predictor variables. In a larger sample, the likelihood dominates the prior distribution for regression coefficients and Bayesian hierarchical model used a likelihood-based approach, used priors (prior laboratory and clinical trials data) that provided the assay sensitivities, which through the prior assumptions, made the model identifiable and the integration of Markov chain Monte Carlo (MCMC) estimates the posterior distributions, avoided missing data and complex outcome measurements.\nResults\nThree case studies: pneumonia etiology in children, prostate cancer, and mental disorders chosen for model development, identified low-risk patient population, reduced the risk of overtreatment, complications, adverse effects, and financial burden for patients (Disease Reclassification). Prostate cancer software was then implemented within the JHM HER.\nLimitation:\nModels were entirely parametric, and extensions to nonparametric or more flexible parametric models were recommended to improve approaches for neuroimage or genomic data.\nneuroimage or genomic data.\nApplications:\n\nto scale a tool that addresses a particular unmet need across a larger, more diverse population of patients and clinicians\nuse in autoimmune diseases, sudden cardiac arrest, and diabetes.\nembed a collection of tools to acquire and use the most relevant information, agnostic to its level of measurement, to improve population and individual health decisions that cause better outcomes at more affordable costs. @Zeger2020\n\n\nBayesian Inference (parametric vs non-parametric)\n\nWhat is the goal of the paper?\nThe authors calculated the posterior probability of disease diagnosis and applied Bayesian inference to develop three modules comparing parametric (with a fixed set of parameters) and nonparametric distributions (which do not make a priori assumptions) by analyzing the National Health and Nutrition Examination Survey dataset from two separate diagnostic tests on both diseased and non-diseased populations.\nWhy is it important?\nMedical diagnosis, treatment, and management decisions are crucial, and conventional methods for diagnosis using clinical criteria and fixed numerical thresholds limit the capture of other information related to the intricate relationship between diagnostic tests and the varying prevalence of diseases. The probability distributions associated with quantitative diagnostic test outcomes often demonstrate some overlap between the diseased and nondiseased groups. The dichotomous method fails to capture the complexity and heterogeneity of disease presentations across diverse populations. The applicability of the normal distribution (conventional method) is critiqued, especially in dealing with clinical measurands having skewness, bimodality, or multimodality.\nHow is it solved?\nMethods: The Authors developed models employing Bayesian inference (Bayesian diagnostic approach) to calculate the posterior probability of disease diagnosis in the Wolfram Language and integrated prior probabilities of disease with distributions of diagnostic measurands in both diseased and nondiseased populations. The approach enabled the evaluation of combined data from multiple diagnostic tests and improved the diagnostic accuracy, precision and adaptability, The model showed flexibility, adaptability, and versatility in the diagnostic.\nResults\nNonparametric Bayesian models tend to fit data distributions better, especially given limited existing literature, and are more robust in capturing complex data patterns.\nThese models produce multimodal probability patterns for disease, unlike the bimodal, double-sigmoidal curves seen with parametric models.\nLimitations\n\nReliance on parametric models: A need to extend to nonparametric or more flexible parametric models for medical data.\nLimited scholarly publications and over-dependence on prior probabilities increase uncertainties, resulting in broader confidence intervals for posterior probabilities. Systemic bias (unrepresentative datasets) compromises the accuracy of Bayesian calculations. For Incomplete datasets, Bayesian methods combined with other statistical and computational techniques could enhance diagnostic capabilities.\nThe foundational data is crucial to compare new diagnostic measurements. Absence of normative data compromises the reliability and validity of Bayesian diagnostic methods. @Chatzimichail2023\n\n\nBayesian model stages, development and advantages, and temporal models\nWhat is the goal of the paper?\n\nThe study describes the stages of Bayesian analysis, specifying the importance of the priors, data modeling, inferences, model checking and refinement, selecting a proper sampling technique from a posterior distribution, variational inferences, variable selection and its application across various research fields. The study proposes strategies for reproducibility and reporting standards, outlining an updated WAMBS (when to Worry and how to Avoid the Misuse of Bayesian Statistics) checklist and outlining the impact of Bayesian analysis on artificial intelligence in the future.\nWhy is it important?\nBayesian statistics is suitable for quantitative researchers accross different fields who have at least some knowledge of regression modelling.\nHow is it solved?\nExamples of successful applications of Bayesian analysis across various research fields (social sciences, ecology, genetics, medicine) and the advantages and disadvantages of the Bayesian model are provided here, and overview of the current and future use of Bayesian statistics.\nThe study mention priors into three categories (informative, weakly informative and diffuse) based on the degree of (un)certainty (hyperparameters) surrounding the population parameter. The prior distribution is as - N( μ0 , σ^ 20) where a larger variance represents a greater amount of uncertainty surrounding.\nPrior elicitation (experts, generic expert, data-based, sample data using maximum likelihood or sample statistics, etc) construct a prior distribution.\nPrior sensitivity analysis of the likelihood helps examine different forms of the model, assesses how the priors and the likelihood align and have an impact on posterior estimates, reflecting variations not captured by the prior or the likelihood alone.\nPrior estimation allows data-informed shrinkage, enacts regularization or influence algorithms towards a likely high-density region, and improves estimation efficiency.\nKnowing the exact probabilistic specification of the priors for a complex model with smaller sample sizes is important. A small sample conveys less information compared to the priors that quantify the strength of support the observed data lends to possible value(s) for the unknown parameter(s).\nFrequentists do not consider the probability of the unknown parameters as useful, and they are considered to be fixed; the likelihood is the conditional probability distribution p(y|θ) of the data (y), given fixed parameters (θ). In Bayesian inference, unknown parameters (random variables) have varied values, while the (observed) data have fixed values, and the likelihood is a function of θ for the fixed data y.\nTherefore, the likelihood function summarizes a statistical model that stochastically generates a range of possible values for θ and the observed data y. With priors and the likelihood of the observed data, the resulting posterior distribution provides an estimate of the unknown parameters, capturing the primary factors and improving our understanding. Monte Carlo technique provides integrals of sampled values from a given distribution through computer simulations. The packages BRMS and Blavaan in R are used for the probabilistic programming language Stan.\nVariable selection after checking correlations among the variables in the model (Eg: gene-to-gene interaction) aids in the prediction of genes in biomedical research (genome-wide association studies).\nSpatial and temporal variability are factored in Bayesian general linear models. A posterior distribution can simulate new data conditional on this distribution, assess, and provide valid predictions to be used for extrapolating to future events.\nResults\nThe Bayesian approach analyzes large-scale cancer genomic data, identifies novel molecular changes in cancer initiation and progression, the interactions between mutated genes and captured mutational signatures, highlighting key genetic interactions components, allowing genomic-based patient stratification both in clinical trials, in the personalized use of therapeutics, and in understanding cancer and its evolutionary processes.\nLimitations:\n\nIn temporal models, posterior inference challenges are inherent in the spatial and/or temporal dependencies (autocorrelation of parameters over time). @VandeSchoot2021\n\n\nBayesian Normal linear regression\n\nWhat is the goal of the paper?\nThe author provides guidance on Bayesian inference by performing Bayesian Normal linear regression in metrology to calibrate instruments and to evaluate inter-laboratory comparisons in determining fundamental constants.\nWhy is it important?\nThe measurement errors are assumed to be additive, independent, and identically distributed according to a Gaussian distribution with mean zero and variance σ2, which is usually unknown.\nRegression is used to calibrate instruments, evaluate inter-laboratory comparisons, or determine fundamental constants, but the regression model cannot be uniquely formulated as a measurement function. Guide to the Expression of Uncertainty in Measurement (GUM) and its supplements are not applicable directly.\nHow is it solved?\nMethods: Bayesian inference has the advantage of accounting for additional a priori information, which robustifies the analyses.\nThree steps (prior elicitation, posterior calculation, and robustness to prior uncertainty and model adequacy) and model assumptions are critical to Bayesian inference.\nIn Bayesian inference, all unknowns—observables (data) as well as unobservables (parameters and auxiliary variables) are considered to be random, are assigned probability distributions to summarize the available information, and to update prior knowledge about the unobservables with information about them contained in the data. The prior distribution and likelihood function provided by simple graphical displays, sensitivity analyses, or model checking enhance the elicitation and interpretation process.\nFor Normal linear regression problems\n(1) a family of prior distributions for θ and σ2 is (Normal inverse Gamma (NIG) distribution to a posterior from the same family of (NIG) distributions or\n(2) alternative families of prior distributions (hierarchical priors) assign an additional layer of distributions to uncertain prior parameters or non-para- metric priors.\nThe NIG prior with known variance σ2 of observations is a conjugate prior distribution. Vague or non-informative prior distributions can be derived from the NIG prior.\nBayesian inference is influenced by\n\nthe uncertainty in the transformation of prior knowledge to prior distributions\nthe assumptions of the statistical model\nthe mistakes in data acquisition\n\nResults\nThe knowledge from related previous experiments (Normal inverse Gamma distributions) allow for analytic posterior calculations of many quantities of interest. @Klauenberg2015\n\nBayesian linear regression and priors (exchangeable and unexchangeable)\n\nWhat is the goal of the paper?\nThe study developed a test of a formal method for augmenting data in linear regression analyses, by incorporating both exchangeable and unexchangeable information on regression coefficients (and standard errors) of previous studies.\nWhy is it important?\nThe frequent combination of multiple testing has relatively low statistical power, which is problematic in null-hypothesis significance testing. Linear regression analyses do not account for the published results and summary statistics from similar previous studies. Ignoring information on parameters from previous studies (relevant and readily available), affects the stability and precision of the parameter estimates and results in lower values than they could have been, resulting in conclusions that are less certain and are affected by sampling variation.\nMultiple linear regression with separate significance tests for all regression coefficients, and with the modest sample sizes, different studies find different sets of statistically significant predictors, and addressing the issue on larger samples is practically unrealistic.\nHow is it solved?\nMethods: Bayesian linear regression accommodates prior knowledge, overcoming the absence of formal studies, handles the issues of increasing the sample size, and augments the data of a new study with previous results (regression coefficients and standard errors) from similar studies.\nThe authors used Bayesian linear regression to solve the issue of the univariate case analysis by combining evidence of specific predictors from different linear regression analyses (as in meta-analysis) and found it a better method to resolve the issue of simultaneously combining multiple regression parameters per study, which ignored the relationship between the regression coefficients. Includes summary statistics from previous studies, Bayesian linear regression provided a more acceptable solution when data from previous studies were not (realistically) obtainable.\nModels in the study as categorized the model into - (1) Exchangable - when the current data and previous studies have the same set of predictors. (2) Unexchangable – when the predictors were different in the two.\nTo yield the posterior density that reflects the updated knowledge about the model parameters after having observed the data, the steps to Bayesian linear regression steps are mentioned -\n(1) To calculate the probability density function for the data given the unknown model parameters;\n(2) Taken as a function of those model parameters, the likelihood function is the second part of the prior density function of the model parameters. It quantifies what is assumed to be known about the model parameters before observing the data. The study applied the Standard multiple linear regression model, and with the integration of the prior, provided the joint posterior density using the Gibbs sampler. An Ordinary Least Squares linear regression was then applied to each of these samples to obtain the estimated regression coefficients B and the corresponding standard errors.\n(3) A hierarchical model version was developed in analyzing parameters where studies are not exchangeable.\nResults\nIncorporating priors from previous studies in a linear regression on new data yielded a significantly better parameter estimate with an adequate approximation.\nThe gains in comparison to using just the new data, and the large effects were obtained when the data from previous studies were available, resulting in encouraging performance.\nPerformance of the two versions (exchangeable and unexchangeable) of the replication model was consistently superior to using current data alone.\nThe model developed in the article offers the possibility of obtaining significantly better parameter estimates in a linear regression setting without needing to expend a prohibitive amount of time and energy to obtain data from the previous studies.\nHierarchical version (unexchangeable) of the model offers the advantage of being able to address questions about differences between studies and thus allows for explicit testing of the exchangeability assumption.\nLimitations:\n\nAll studies need to have the same set of predictors.\nThe issue of predictor variables that are correlated. @DeLeeuw2012a\n\n\nBayesian logistic regression (Sequential clinical reasoning approach)\n\nWhat is the goal of the paper?\nThe study aimed to develop models using a longitudinal prospective cohort to predict the risk of incident cardiovascular disease by incorporating demographic features (basic), six metabolic syndrome components (metabolic score), and conventional risk factors (enhanced model). The study participants free of CVD at baseline were followed up over five years, and a Bayesian clinical reasoning model was applied to diagnose new CVD cases.\nWhy is it important?\nEarly diagnosis, prevention, by identifying subjects under the high-risk category for cardiovascular disease (CVD), impacts health interventions.\nLimited availability of molecular information in clinical practice due to being costly and unavailability affects efficient disease diagnosis. To efficiently identify a high-risk population based on the routinely checked biological markers before doing these expensive molecular testsThe requires an alternative approach to analyze data.\nThe Tailored Framingham Risk Score method, for the purpose, is not sufficient because of the differences present in ethnic groups, location, and socio-economic status, as they require the construction of their own models. Heterogeneity (geographic, ethnic group, variations, and different characteristics of social contextual network) often is unobservable and unmeasurable.\nHow is it solved?\nMethods: The study evaluated subjects enrolled in a Keelung Community-based Integrated Screening (KCIS) Program, for mass screening (20–79 years) in the Keelung city of Taiwan, who were followed for 5 years to identify incident cancers and chronic diseases (cardiovascular disease).\nThe study classified the risk of having incident CVD cases or death from CVD by dint of available calculated standardized risk score of the MetS components (fasting glucose, blood pressure, HDL-C, triglyceride and waist circumference) together with conventional risk factors (gender, heredity, smoking, alcohol drinking, family history of parent’s CVD and betel quid and other relevant factors).\nEmulating a clinician’s evaluation process, the Bayesian clinical reasoning approach in a sequential manner was applied and three models were developed. The approach considered the normal distribution of regression coefficients of all predictors, allowing for uncertainty of clinical weights. The credible intervals of predicted risk estimates were obtained by averaging out. In the model, the individual risk is elicited by prior speculation (first impression) that is updated by objective observed data (patient’s history and laboratory findings), the regression coefficients for computing risk score were treated as random variable with a certain statistical distribution (e.g. normal distribution) rather than a fixed value (traditional risk prediction model by frequentist). The updated prior distribution with the likelihood of the current data provided a posterior distribution to predict the risk for a specific disease. The sequential approach included -\n\nThe (basic) basic model developed via logistic regression used prior information constructed on gender, age, age2, and time period.\nThe second (classical) model (metabolic score model: MS model) was based on six MetS components.\nThe third (enhanced model) incorporated information on smoking, drinking, betel-quid, and family history of CVD.\n\nResults\nCompared to the basic model and classical model, the enhanced model had better performance. The proposed models predicted CVD incidence at the individual level by incorporating routine information with a sequential Bayesian clinical reasoning approach. Patients’ background significantly contributes to baseline risk. Even with ecological heterogeneity, the regression model adopts individual characteristics and makes individual risk prediction for the CVD incidence.\nLimitations:\n\nWhether the interactions between age, gender, metabolic score, and other risk factors should be included.\nThe use of an enhanced model should be validated through external validation by applying the proposed models to new subjects not included in the training of the model parameters. @Liu2013"
  },
  {
    "objectID": "Summaries/nm/DATA.html",
    "href": "Summaries/nm/DATA.html",
    "title": "Bayesian Logistic Regression Capstone",
    "section": "",
    "text": "Data Source: NHANES merging the below files make 10175 sample size but have to clean it so maybe we will reduce the sample size 1. DEMO_H.xpt 2. DSQTOT_H.xpt 3. BMX_H.xpt 4. TCHOL_H.xpt 5. CDQ_H.xpt 6. SMQ_H.xpt 7. MCQ_H.xpt"
  },
  {
    "objectID": "Summaries/nm/My articles.html",
    "href": "Summaries/nm/My articles.html",
    "title": "Bayesian Logistic Regression Capstone",
    "section": "",
    "text": "Namita’s Literature"
  },
  {
    "objectID": "Summaries/nm/My articles.html#introduction",
    "href": "Summaries/nm/My articles.html#introduction",
    "title": "Bayesian Logistic Regression Capstone",
    "section": "Introduction",
    "text": "Introduction\n\nBayesian Hierarchical Model (Disease reclassification and prediction)\n\nWhat is the goal of the paper?\nThe authors develop a Bayesian hierarchical model for multivariate longitudinal data to predict health status, trajectories, and intervention effects at the individual level in the PCORI mission to address questions about health status from patients and clinicians.\nWhy is it important?\nHealthcare data (DNA sequences, functional images of the brain, patient-reported outcomes, and electronic health records with patients’ sequences of health measurements, diagnoses, and treatments) are complex, and the standard approaches are not adequate for clinical data analysis. Electronic health records (EHRs) could improve diagnostic accuracy and predict treatment effects. Visualizations of characteristics of posterior distributions can be immediately understood by clinicians and patients as relevant to their decision. Combining prior knowledge and patient data with evidence could predict the patient’s health status, trajectory, and/or likely benefits of interventions.\nHow is it solved?\nMethod: The authors applied Bayesian hierarchical regression for multivariate longitudinal patient data using open-source R-packages and developed 2 levels—time within person and persons within a population\nThe model combined exogenous (eg, age, clinical history) factors and endogenous (eg, current treatment) variables on the individual’s multivariate health measurements and the effects of health measurements at one time on subsequent interventions.\nThe model produced an estimate of the posterior distribution for each value of the predictor variable and an estimate of the marginal distribution of the regression coefficients for each coefficient that measures the outcome (health status) associated with its predictor variables. In a larger sample, the likelihood dominates the prior distribution for regression coefficients and Bayesian hierarchical model used a likelihood-based approach, used priors (prior laboratory and clinical trials data) that provided the assay sensitivities, which through the prior assumptions, made the model identifiable and the integration of Markov chain Monte Carlo (MCMC) estimates the posterior distributions, avoided missing data and complex outcome measurements.\nResults\nThree case studies: pneumonia etiology in children, prostate cancer, and mental disorders chosen for model development, identified low-risk patient population, reduced the risk of overtreatment, complications, adverse effects, and financial burden for patients (Disease Reclassification). Prostate cancer software was then implemented within the JHM HER.\nLimitation:\nModels were entirely parametric, and extensions to nonparametric or more flexible parametric models were recommended to improve approaches for neuroimage or genomic data.\nneuroimage or genomic data.\nApplications:\n\nto scale a tool that addresses a particular unmet need across a larger, more diverse population of patients and clinicians\nuse in autoimmune diseases, sudden cardiac arrest, and diabetes.\nembed a collection of tools to acquire and use the most relevant information, agnostic to its level of measurement, to improve population and individual health decisions that cause better outcomes at more affordable costs. @Zeger2020\n\n\nBayesian Inference (parametric vs non-parametric)\n\nWhat is the goal of the paper?\nThe authors calculated the posterior probability of disease diagnosis and applied Bayesian inference to develop three modules comparing parametric (with a fixed set of parameters) and nonparametric distributions (which do not make a priori assumptions) by analyzing the National Health and Nutrition Examination Survey dataset from two separate diagnostic tests on both diseased and non-diseased populations.\nWhy is it important?\nMedical diagnosis, treatment, and management decisions are crucial, and conventional methods for diagnosis using clinical criteria and fixed numerical thresholds limit the capture of other information related to the intricate relationship between diagnostic tests and the varying prevalence of diseases. The probability distributions associated with quantitative diagnostic test outcomes often demonstrate some overlap between the diseased and nondiseased groups. The dichotomous method fails to capture the complexity and heterogeneity of disease presentations across diverse populations. The applicability of the normal distribution (conventional method) is critiqued, especially in dealing with clinical measurands having skewness, bimodality, or multimodality.\nHow is it solved?\nMethods: The Authors developed models employing Bayesian inference (Bayesian diagnostic approach) to calculate the posterior probability of disease diagnosis in the Wolfram Language and integrated prior probabilities of disease with distributions of diagnostic measurands in both diseased and nondiseased populations. The approach enabled the evaluation of combined data from multiple diagnostic tests and improved the diagnostic accuracy, precision and adaptability, The model showed flexibility, adaptability, and versatility in the diagnostic.\nResults\nNonparametric Bayesian models tend to fit data distributions better, especially given limited existing literature, and are more robust in capturing complex data patterns.\nThese models produce multimodal probability patterns for disease, unlike the bimodal, double-sigmoidal curves seen with parametric models.\nLimitations\n\nReliance on parametric models: A need to extend to nonparametric or more flexible parametric models for medical data.\nLimited scholarly publications and over-dependence on prior probabilities increase uncertainties, resulting in broader confidence intervals for posterior probabilities. Systemic bias (unrepresentative datasets) compromises the accuracy of Bayesian calculations. For Incomplete datasets, Bayesian methods combined with other statistical and computational techniques could enhance diagnostic capabilities.\nThe foundational data is crucial to compare new diagnostic measurements. Absence of normative data compromises the reliability and validity of Bayesian diagnostic methods. @Chatzimichail2023\n\n\nBayesian model stages, development and advantages, and temporal models\nWhat is the goal of the paper?\n\nThe study describes the stages of Bayesian analysis, specifying the importance of the priors, data modeling, inferences, model checking and refinement, selecting a proper sampling technique from a posterior distribution, variational inferences, variable selection and its application across various research fields. The study proposes strategies for reproducibility and reporting standards, outlining an updated WAMBS (when to Worry and how to Avoid the Misuse of Bayesian Statistics) checklist and outlining the impact of Bayesian analysis on artificial intelligence in the future.\nWhy is it important?\nBayesian statistics is suitable for quantitative researchers accross different fields who have at least some knowledge of regression modelling.\nHow is it solved?\nExamples of successful applications of Bayesian analysis across various research fields (social sciences, ecology, genetics, medicine) and the advantages and disadvantages of the Bayesian model are provided here, and overview of the current and future use of Bayesian statistics.\nThe study mention priors into three categories (informative, weakly informative and diffuse) based on the degree of (un)certainty (hyperparameters) surrounding the population parameter. The prior distribution is as - N( μ0 , σ^ 20) where a larger variance represents a greater amount of uncertainty surrounding.\nPrior elicitation (experts, generic expert, data-based, sample data using maximum likelihood or sample statistics, etc) construct a prior distribution.\nPrior sensitivity analysis of the likelihood helps examine different forms of the model, assesses how the priors and the likelihood align and have an impact on posterior estimates, reflecting variations not captured by the prior or the likelihood alone.\nPrior estimation allows data-informed shrinkage, enacts regularization or influence algorithms towards a likely high-density region, and improves estimation efficiency.\nKnowing the exact probabilistic specification of the priors for a complex model with smaller sample sizes is important. A small sample conveys less information compared to the priors that quantify the strength of support the observed data lends to possible value(s) for the unknown parameter(s).\nFrequentists do not consider the probability of the unknown parameters as useful, and they are considered to be fixed; the likelihood is the conditional probability distribution p(y|θ) of the data (y), given fixed parameters (θ). In Bayesian inference, unknown parameters (random variables) have varied values, while the (observed) data have fixed values, and the likelihood is a function of θ for the fixed data y.\nTherefore, the likelihood function summarizes a statistical model that stochastically generates a range of possible values for θ and the observed data y. With priors and the likelihood of the observed data, the resulting posterior distribution provides an estimate of the unknown parameters, capturing the primary factors and improving our understanding. Monte Carlo technique provides integrals of sampled values from a given distribution through computer simulations. The packages BRMS and Blavaan in R are used for the probabilistic programming language Stan.\nVariable selection after checking correlations among the variables in the model (Eg: gene-to-gene interaction) aids in the prediction of genes in biomedical research (genome-wide association studies).\nSpatial and temporal variability are factored in Bayesian general linear models. A posterior distribution can simulate new data conditional on this distribution, assess, and provide valid predictions to be used for extrapolating to future events.\nResults\nThe Bayesian approach analyzes large-scale cancer genomic data, identifies novel molecular changes in cancer initiation and progression, the interactions between mutated genes and captured mutational signatures, highlighting key genetic interactions components, allowing genomic-based patient stratification both in clinical trials, in the personalized use of therapeutics, and in understanding cancer and its evolutionary processes.\nLimitations:\n\nIn temporal models, posterior inference challenges are inherent in the spatial and/or temporal dependencies (autocorrelation of parameters over time). @VandeSchoot2021\n\n\nBayesian Normal linear regression\n\nWhat is the goal of the paper?\nThe author provides guidance on Bayesian inference by performing Bayesian Normal linear regression in metrology to calibrate instruments and to evaluate inter-laboratory comparisons in determining fundamental constants.\nWhy is it important?\nThe measurement errors are assumed to be additive, independent, and identically distributed according to a Gaussian distribution with mean zero and variance σ2, which is usually unknown.\nRegression is used to calibrate instruments, evaluate inter-laboratory comparisons, or determine fundamental constants, but the regression model cannot be uniquely formulated as a measurement function. Guide to the Expression of Uncertainty in Measurement (GUM) and its supplements are not applicable directly.\nHow is it solved?\nMethods: Bayesian inference has the advantage of accounting for additional a priori information, which robustifies the analyses.\nThree steps (prior elicitation, posterior calculation, and robustness to prior uncertainty and model adequacy) and model assumptions are critical to Bayesian inference.\nIn Bayesian inference, all unknowns—observables (data) as well as unobservables (parameters and auxiliary variables) are considered to be random, are assigned probability distributions to summarize the available information, and to update prior knowledge about the unobservables with information about them contained in the data. The prior distribution and likelihood function provided by simple graphical displays, sensitivity analyses, or model checking enhance the elicitation and interpretation process.\nFor Normal linear regression problems\n(1) a family of prior distributions for θ and σ2 is (Normal inverse Gamma (NIG) distribution to a posterior from the same family of (NIG) distributions or\n(2) alternative families of prior distributions (hierarchical priors) assign an additional layer of distributions to uncertain prior parameters or non-para- metric priors.\nThe NIG prior with known variance σ2 of observations is a conjugate prior distribution. Vague or non-informative prior distributions can be derived from the NIG prior.\nBayesian inference is influenced by\n\nthe uncertainty in the transformation of prior knowledge to prior distributions\nthe assumptions of the statistical model\nthe mistakes in data acquisition\n\nResults\nThe knowledge from related previous experiments (Normal inverse Gamma distributions) allow for analytic posterior calculations of many quantities of interest. @Klauenberg2015\n\nBayesian linear regression and priors (exchangeable and unexchangeable)\n\nWhat is the goal of the paper?\nThe study developed a test of a formal method for augmenting data in linear regression analyses, by incorporating both exchangeable and unexchangeable information on regression coefficients (and standard errors) of previous studies.\nWhy is it important?\nThe frequent combination of multiple testing has relatively low statistical power, which is problematic in null-hypothesis significance testing. Linear regression analyses do not account for the published results and summary statistics from similar previous studies. Ignoring information on parameters from previous studies (relevant and readily available), affects the stability and precision of the parameter estimates and results in lower values than they could have been, resulting in conclusions that are less certain and are affected by sampling variation.\nMultiple linear regression with separate significance tests for all regression coefficients, and with the modest sample sizes, different studies find different sets of statistically significant predictors, and addressing the issue on larger samples is practically unrealistic.\nHow is it solved?\nMethods: Bayesian linear regression accommodates prior knowledge, overcoming the absence of formal studies, handles the issues of increasing the sample size, and augments the data of a new study with previous results (regression coefficients and standard errors) from similar studies.\nThe authors used Bayesian linear regression to solve the issue of the univariate case analysis by combining evidence of specific predictors from different linear regression analyses (as in meta-analysis) and found it a better method to resolve the issue of simultaneously combining multiple regression parameters per study, which ignored the relationship between the regression coefficients. Includes summary statistics from previous studies, Bayesian linear regression provided a more acceptable solution when data from previous studies were not (realistically) obtainable.\nModels in the study as categorized the model into - (1) Exchangable - when the current data and previous studies have the same set of predictors. (2) Unexchangable – when the predictors were different in the two.\nTo yield the posterior density that reflects the updated knowledge about the model parameters after having observed the data, the steps to Bayesian linear regression steps are mentioned -\n(1) To calculate the probability density function for the data given the unknown model parameters;\n(2) Taken as a function of those model parameters, the likelihood function is the second part of the prior density function of the model parameters. It quantifies what is assumed to be known about the model parameters before observing the data. The study applied the Standard multiple linear regression model, and with the integration of the prior, provided the joint posterior density using the Gibbs sampler. An Ordinary Least Squares linear regression was then applied to each of these samples to obtain the estimated regression coefficients B and the corresponding standard errors.\n(3) A hierarchical model version was developed in analyzing parameters where studies are not exchangeable.\nResults\nIncorporating priors from previous studies in a linear regression on new data yielded a significantly better parameter estimate with an adequate approximation.\nThe gains in comparison to using just the new data, and the large effects were obtained when the data from previous studies were available, resulting in encouraging performance.\nPerformance of the two versions (exchangeable and unexchangeable) of the replication model was consistently superior to using current data alone.\nThe model developed in the article offers the possibility of obtaining significantly better parameter estimates in a linear regression setting without needing to expend a prohibitive amount of time and energy to obtain data from the previous studies.\nHierarchical version (unexchangeable) of the model offers the advantage of being able to address questions about differences between studies and thus allows for explicit testing of the exchangeability assumption.\nLimitations:\n\nAll studies need to have the same set of predictors.\nThe issue of predictor variables that are correlated. @DeLeeuw2012a\n\n\nBayesian logistic regression (Sequential clinical reasoning approach)\n\nWhat is the goal of the paper?\nThe study aimed to develop models using a longitudinal prospective cohort to predict the risk of incident cardiovascular disease by incorporating demographic features (basic), six metabolic syndrome components (metabolic score), and conventional risk factors (enhanced model). The study participants free of CVD at baseline were followed up over five years, and a Bayesian clinical reasoning model was applied to diagnose new CVD cases.\nWhy is it important?\nEarly diagnosis, prevention, by identifying subjects under the high-risk category for cardiovascular disease (CVD), impacts health interventions.\nLimited availability of molecular information in clinical practice due to being costly and unavailability affects efficient disease diagnosis. To efficiently identify a high-risk population based on the routinely checked biological markers before doing these expensive molecular testsThe requires an alternative approach to analyze data.\nThe Tailored Framingham Risk Score method, for the purpose, is not sufficient because of the differences present in ethnic groups, location, and socio-economic status, as they require the construction of their own models. Heterogeneity (geographic, ethnic group, variations, and different characteristics of social contextual network) often is unobservable and unmeasurable.\nHow is it solved?\nMethods: The study evaluated subjects enrolled in a Keelung Community-based Integrated Screening (KCIS) Program, for mass screening (20–79 years) in the Keelung city of Taiwan, who were followed for 5 years to identify incident cancers and chronic diseases (cardiovascular disease).\nThe study classified the risk of having incident CVD cases or death from CVD by dint of available calculated standardized risk score of the MetS components (fasting glucose, blood pressure, HDL-C, triglyceride and waist circumference) together with conventional risk factors (gender, heredity, smoking, alcohol drinking, family history of parent’s CVD and betel quid and other relevant factors).\nEmulating a clinician’s evaluation process, the Bayesian clinical reasoning approach in a sequential manner was applied and three models were developed. The approach considered the normal distribution of regression coefficients of all predictors, allowing for uncertainty of clinical weights. The credible intervals of predicted risk estimates were obtained by averaging out. In the model, the individual risk is elicited by prior speculation (first impression) that is updated by objective observed data (patient’s history and laboratory findings), the regression coefficients for computing risk score were treated as random variable with a certain statistical distribution (e.g. normal distribution) rather than a fixed value (traditional risk prediction model by frequentist). The updated prior distribution with the likelihood of the current data provided a posterior distribution to predict the risk for a specific disease. The sequential approach included -\n\nThe (basic) basic model developed via logistic regression used prior information constructed on gender, age, age2, and time period.\nThe second (classical) model (metabolic score model: MS model) was based on six MetS components.\nThe third (enhanced model) incorporated information on smoking, drinking, betel-quid, and family history of CVD.\n\nResults\nCompared to the basic model and classical model, the enhanced model had better performance. The proposed models predicted CVD incidence at the individual level by incorporating routine information with a sequential Bayesian clinical reasoning approach. Patients’ background significantly contributes to baseline risk. Even with ecological heterogeneity, the regression model adopts individual characteristics and makes individual risk prediction for the CVD incidence.\nLimitations:\n\nWhether the interactions between age, gender, metabolic score, and other risk factors should be included.\nThe use of an enhanced model should be validated through external validation by applying the proposed models to new subjects not included in the training of the model parameters. @Liu2013"
  },
  {
    "objectID": "Summaries/et/paper1.html",
    "href": "Summaries/et/paper1.html",
    "title": "Bayesian Nonparametric Regression for Healthcare Claims Modeling",
    "section": "",
    "text": "Bayesian Nonparametric Regression for Healthcare Claims Modeling\nRichardson, R., & Hartman, B. (2018). Bayesian nonparametric regression models for modeling and predicting healthcare claims. Insurance, Mathematics & Economics, 79, 1-13.\n\n\nSummary\nThis paper introduces Bayesian nonparametric regression models to effectively model and predict healthcare claims data, which often exhibit complex characteristics such as skewness, heavy tails, and excess zeros. The authors propose using Dirichlet process mixtures to flexibly capture the underlying distribution of healthcare claims without assuming a specific parametric form. The models are designed to handle the unique features of healthcare claims data, including the presence of many zero claims (non-claimants) and the variability in claim amounts among claimants.\nThe authors apply their models to a real-world dataset of healthcare claims, demonstrating the models’ ability to accurately predict claim amounts and identify high-risk individuals. The Bayesian framework allows for the incorporation of prior information and provides a coherent way to quantify uncertainty in predictions. The results show that the proposed nonparametric regression models outperform traditional parametric models in terms of predictive accuracy and flexibility.\n\n\nProblem\n\nStandard regression limitations\n\nInsufficient for complex relationships in healthcare claims data\nAssumes Gaussianity, independence, linearity\nAssumptions often not met in insurance data\nDifficulty in capturing skewness, heavy tails, outliers, and bimodality present in claims data\n\n\n\n\nSolution\n\nBayesian nonparametric regression models\n\nFlexible regression model\nRelaxes Normality and linearity assumptions\nPoweful tool for non-Gaussian densities\nIncreased predictive accuracy\nHandles complex error distribution characteristics\nApplicable to all insurance regression problems\n\n\n\n\nMethodology\n\nDependent Dirichlet Process (DDP) ANOVA Model\n\nDirichlet Process (DP) (3.1)\n\nStandard building block for Bayesian nonparametric models\nConstructed via stick-breaking process\nDiscrete distribution with countably infinite atoms\nDP mixture of normals for continuous settings\nAllows for infinite mixture models\n\nDDP (3.2)\n\nBasis for fully nonparametric regression model\nPrior on space of random probability measures\nPoint masses are realizations of stochastic processes\nInfinite mixture of Gaussian processes\n\nDDP ANOVA model (3.3)\n\nExtends DDP to include covariate information\nAtoms are regression coefficients and covariance\nFlexible relationships and error structure\nUses Gibbs sampling for posterior inference\n\n\n\n\n\nApplication\n\nHealthcare claims by ETG\n\nDataset: Episode Treatment Groups (ETG) from a large health insurer\nETG\n\nClassification system for medical conditions\nPredicts healthcare costs\nUncertainty in cost predictions is important\n\nCovariates\n\nage, gender, healthcare charges\n\nFocus on prediction of new observations\n\n\n\n\nResults and performance\n\nOutperforms standard linear model\nOutperforms Generalized Beta 2 (GB2) regression (all but 11 of 347 ETGs)\nBetter predictive accuracy\nDIC (Deviance Information Criterion) consistently lower for BNP\nLower averarge CRPS for out-of-sample predictions\nAccurately captures tail behavior (useful for reserving/risk assessment)\n\n\n\nCase studies\n\nConjunctivitis\n\nLarge dataset (160,000+ observations)\nDistribution is highly non-Gaussian, bimodal\nBNP captures gender effect, standard models fail\nLowest DIC (10,600 vs 12,600 for BLM and 12,300 for GB2)\nAccurately predicts extra mode in left tail\n\nLung transplant\n\nSmaller dataset\nData skewed, Gaussian assumption inappropriate\nBNP/GB2 predict thicker tail for outliers\nGB2 slightly better DIC (1002 vs 1005 for BNP, and 1080 for BLM)\nLess advantage from flexible models in smaller datasets\n\n\n\n\nLimitations\n\nBNP more computationally intensive than standard linear regression\nBut BNP regression scales at the same reate as linear regression\n\n\n\nConclusion\n\nPowerful tool for healthcare claims modeling\nVastly outperforms linear and GB2 regression\nUseful for reserving due to improved distributional fit\nDDP ANOVA useful for continuous independent variables\nCan be extended(e.g. covariate-dependent weights)\nAvailable in R package DPpackage"
  },
  {
    "objectID": "Summaries/et/paper4.html",
    "href": "Summaries/et/paper4.html",
    "title": "Bayesian Logistic Regression Capstone",
    "section": "",
    "text": "What is BART-Survival?\n\nIt’s a Python software package developed by the CDC.\nThe package is for survival analysis: studying time-to-event data (how long until something happens, or doesn’t happen).\nIt uses a machine learning method called Bayesian Additive Regression Trees (BART).\n\nHow does it work (in simple terms)? - “Discrete-time” means it splits up time into chunks (for example, weeks, months, etc.). The model checks at each time‐point whether the event has happened yet or not. - BART is “non-parametric”, which means the model doesn’t assume a fixed mathematical form (like linear or exponential) for how risk changes over time. It lets the data itself shape the risk patterns more flexibly. - Because it’s Bayesian, it gives not just a prediction but also measures of uncertainty (how confident the model is) about those predictions.\nWhat makes it useful / better in some cases - Traditional survival analysis (like the Cox model) often assumes certain things (e.g. that hazards are proportional over time). If those assumptions are wrong, the models can be misleading. BART-Survival is more flexible and can perform better when assumptions of older methods fail. - It provides a friendly API, and tools to dive deeper into the model (e.g. inspecting results, uncertainty) when needed.\nWhen / where we might use it - When you have data about when events happen (or don’t happen) and want to model that. For example, time until recovery, time until equipment failure, time until some event in public health etc. - When you suspect that the standard assumptions of simpler survival models may not hold (e.g. that risk doesn’t change proportionally over time). - When you want flexible models that can give you both predictions and uncertainty.\n\nReference\n\nSparapani, R. A., Logan, B. R., McCulloch, R., & Laud, P. W. (2020). Nonparametric survival analysis using Bayesian additive regression trees (BART). Statistics in Medicine, 39(20), 2526-2546. https://doi.org/10.1002/sim.8523\nBART-Survival GitHub Repository"
  },
  {
    "objectID": "Summaries/et/paper2.html",
    "href": "Summaries/et/paper2.html",
    "title": "Bayesian parametric models for survival prediction in medical applications",
    "section": "",
    "text": "Bayesian parametric models for survival prediction in medical applications\nIwan Paolucci, Yuan-Mao Lin, Jessica Albuquerque Marques Silva, Kristy K. Brock & Bruno C. Odisio\nBMC Medical Research Methodology volume 23, Article number: 250 (2023)\nThis research article, published in BMC Medical Research Methodology, introduces and evaluates Bayesian parametric survival models for predicting patient outcomes in medical applications. The authors, Paolucci et al., highlight the advantages of Bayesian models, such as their ability to provide uncertainty measures, require less hyperparameter tuning, and offer a natural mechanism for model updating using Bayes’ rule without needing original training data due to privacy concerns. The study compares these Bayesian models against conventional survival prediction methods like Cox Proportional Hazards and Random Survival Forests, demonstrating comparable performance while exhibiting less overfitting. The article details the mathematical background of these models, their implementation, and presents results from experiments on various public medical datasets to support their utility in personalized medicine.\n\nProblem\nSurvival analysis is crucial in medical research for predicting patient outcomes, such as time until death or disease recurrence. Traditional models like Cox Proportional Hazards (CoxPH) and Random Survival Forests (RSF) have limitations, including assumptions about hazard ratios and challenges with interpretability. Bayesian parametric models offer a promising alternative by providing uncertainty measures, requiring less hyperparameter tuning, and allowing for model updates without needing original training data, which is beneficial for privacy concerns.\nA major gap in current predictive models for medical applications is the lack of a measure of uncertainty associated with predictions, which is crucial for physicians making high-stakes clinical decisions, especially when treatments have differing side effect profiles or costs.\nThe study tackles practical and technical challenges inherent in medical machine learning. Medical datasets are often limited in size due to the subclassification of diseases, making models highly prone to overfitting. Many existing machine learning algorithms also require extensive hyperparameter tuning to implement regularization and prevent this overfitting.\n\n\nMethodology\nThe authors introduce Bayesian parametric survival models, which are based on the assumption that survival times follow a specific statistical distribution (e.g., Exponential, Weibull, Lognormal). These models use Bayesian inference to estimate the parameters of the chosen distribution, allowing for the incorporation of prior knowledge and the quantification of uncertainty in predictions.\n\nBayesian Parametric Survival Models (BPS)\n\nImplemented using the PyMC library in Python.\nModels include Exponential and Weibull distributions.\nParameters are estimated using Linear combinations and neural networks to predict the parameters of the distributions.\n\nTraining\n\nPyMC framework\nBayes Rule for updating (posterior as prior)\n\nEvaluation\n\nPublic Datasets: AIDS Clinical Trials Group (ACTG), German Breast Cancer Study (GBCS), Veteran lung cancer (Veteran), Worcester Heart Attack Study (WHAS), and primary biliary cirrhosis (PBC)\nExperiment 1: Comparison of Bayesian models with CoxPH, RSF, and DeepSurv\nExperiment 2: Model updating vs retraining from scratch\nExperiment 3: Impact of dataset size on model performance\nMetrics: Concordance Index (C-index), Integrated Brier Score (IBS)\nComparison with Cox Proportional Hazards, Random Survival Forests, and DeepSurv\n\n\n\n\nResults\nThe results demonstrate that Bayesian parametric survival models perform comparably to traditional methods like CoxPH and RSF, with some advantages in terms of reduced overfitting and the ability to provide uncertainty estimates. The models also showed robustness when updating with new data, maintaining performance without needing to retrain from scratch.\n\nPerformance comparison\n\nBPS models performed well, no consistent best model\nBPS Wb NN superior for PBC dataset\n\nOverfitting\n\nBPS and CoxPH & Weibull based models overfit least\nDeepSurv & RSF showed more overfitting\n\nUncertainty estimation beneficial for clinical decision-making\n\n\n\nConclusion\n\nBPS models are competitive, robust, and well-suited for medical applications.\nKey advantages: less overfitting, uncertainty quantification, reduced hyperparameter tuning.\nEfficient model updating without original data (preserves privacy).\nLimitations: datasets used in the study are relatively small, computation time not compared."
  }
]