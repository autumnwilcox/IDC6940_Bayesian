[
  {
    "objectID": "Summaries/et/paper6.html",
    "href": "Summaries/et/paper6.html",
    "title": "Variable Selection Using Bayesian Additive Regression Trees",
    "section": "",
    "text": "Chuji Luo, Michael J. Daniels\nStatistical Science, Vol.39, No. 2, 286-304, May 2024\nhttps://arxiv.org/abs/2112.13998\n\n\nThis paper introduces a novel method for variable selection in regression models using Bayesian Additive Regression Trees (BART). The authors propose a permutation-based approach to assess the importance of each predictor variable by comparing the model’s performance with and without the variable. This method allows for the identification of relevant predictors while accounting for complex interactions and nonlinear relationships inherent in BART models. The authors demonstrate the effectiveness of their approach through simulations and real-world applications, showing that it outperforms traditional variable selection methods in terms of accuracy and interpretability.\n\n\n\nIn regression analysis, identifying the most relevant predictor variables is crucial for building accurate and interpretable models. Traditional variable selection methods often struggle with complex data structures, such as nonlinear relationships and interactions among predictors. Bayesian Additive Regression Trees (BART) offer a flexible modeling framework that can capture these complexities, but they lack built-in mechanisms for variable selection. The challenge addressed in this paper is to develop a robust method for selecting important variables within the BART framework, enabling researchers to leverage BART’s strengths while ensuring model interpretability.\n\n\n\nThe authors propose a permutation-based variable selection method within the BART framework. The key steps of the methodology are as follows: 1. Fit a BART model to the data using all predictor variables. 2. For each predictor variable, create a permuted version of the dataset by randomly shuffling the values of that variable while keeping the other variables unchanged. 3. Fit a BART model to the permuted dataset and evaluate its performance using a suitable metric (e.g., mean squared error). 4. Compare the performance of the original BART model with the permuted model. A significant drop in performance when a variable is permuted indicates that the variable is important for predicting the outcome. 5. Repeat the permutation process multiple times to obtain a distribution of performance metrics for each variable, allowing for statistical inference about variable importance. 6. Select variables based on their importance scores, using a threshold to determine which variables are considered relevant. 7. The authors validate their method through extensive simulations and applications to real-world datasets, demonstrating its effectiveness in identifying important predictors while accounting for complex interactions and nonlinearities.\n\n\n\nThe proposed permutation-based variable selection method using BART was evaluated through simulations and real-world applications. The results showed that the method effectively identified important predictor variables, outperforming traditional variable selection techniques in terms of accuracy and interpretability. The authors reported that their approach was able to capture complex interactions and nonlinear relationships among predictors, leading to improved model performance. Additionally, the method provided a clear ranking of variable importance, facilitating the interpretation of results. Overall, the findings suggest that the proposed method is a valuable tool for variable selection in regression models, particularly when using BART.\n\n\n\n\nThe paper introduced three new variable selection approaches: (1) a permutation-based approach using within-type BART Variable Inclusion Proportion (VIP), (2) a permutation-based approach using BART Metropolis Importance (MI), and (3) a backward selection procedure with two filters.\nThese new approaches were designed specifically to address issues with existing methods, such as being biased against categorical predictors and being conservative in including relevant predictors, particularly in data settings with mixed-type predictors (e.g., continuous and binary).\nBased on the simulation results, where success was defined as an excellent capability of including all relevant predictors (rmiss \\(\\le 0.1\\)) and acceptable capability of excluding irrelevant predictors (precision \\(\\ge 0.6\\)), the three proposed approaches consistently perform well in identifying all the relevant predictors and excluding irrelevant predictors.\n\nRanking of Successful Approaches (by Success Rate)\nThe backward selection approach with two filters achieves the highest success rate (70.8%) across the various simulation scenarios tested. The two new permutation-based approaches followed closely in performance: 1. Backward selection with two filters: 70.8% success rate. 2. Permutation-based approach using BART MI: 66.7% success rate. 3. Permutation-based approach using BART Within-Type VIP: 62.5% success rate.\nIdentified Drawbacks\n\nA significant drawback of the three proposed approaches, similar to existing BART-based variable selection methods, is that they suffer from multicollinearity (correlated predictors). This challenge is particularly noticeable when the noise is high or when dealing with a binary response variable.\nThe computational cost of the backward selection approach is a shortcoming because it requires running BART multiple times, although this cost can be reduced by fitting the models in parallel on multiple cores.\nWhile the permutation-based approach using BART within-type VIP improves upon the existing BART VIP approach for small numbers of mixed-type predictors, it also suffers from multicollinearity."
  },
  {
    "objectID": "Summaries/et/paper6.html#summary",
    "href": "Summaries/et/paper6.html#summary",
    "title": "Variable Selection Using Bayesian Additive Regression Trees",
    "section": "",
    "text": "This paper introduces a novel method for variable selection in regression models using Bayesian Additive Regression Trees (BART). The authors propose a permutation-based approach to assess the importance of each predictor variable by comparing the model’s performance with and without the variable. This method allows for the identification of relevant predictors while accounting for complex interactions and nonlinear relationships inherent in BART models. The authors demonstrate the effectiveness of their approach through simulations and real-world applications, showing that it outperforms traditional variable selection methods in terms of accuracy and interpretability."
  },
  {
    "objectID": "Summaries/et/paper6.html#problem",
    "href": "Summaries/et/paper6.html#problem",
    "title": "Variable Selection Using Bayesian Additive Regression Trees",
    "section": "",
    "text": "In regression analysis, identifying the most relevant predictor variables is crucial for building accurate and interpretable models. Traditional variable selection methods often struggle with complex data structures, such as nonlinear relationships and interactions among predictors. Bayesian Additive Regression Trees (BART) offer a flexible modeling framework that can capture these complexities, but they lack built-in mechanisms for variable selection. The challenge addressed in this paper is to develop a robust method for selecting important variables within the BART framework, enabling researchers to leverage BART’s strengths while ensuring model interpretability."
  },
  {
    "objectID": "Summaries/et/paper6.html#methodology",
    "href": "Summaries/et/paper6.html#methodology",
    "title": "Variable Selection Using Bayesian Additive Regression Trees",
    "section": "",
    "text": "The authors propose a permutation-based variable selection method within the BART framework. The key steps of the methodology are as follows: 1. Fit a BART model to the data using all predictor variables. 2. For each predictor variable, create a permuted version of the dataset by randomly shuffling the values of that variable while keeping the other variables unchanged. 3. Fit a BART model to the permuted dataset and evaluate its performance using a suitable metric (e.g., mean squared error). 4. Compare the performance of the original BART model with the permuted model. A significant drop in performance when a variable is permuted indicates that the variable is important for predicting the outcome. 5. Repeat the permutation process multiple times to obtain a distribution of performance metrics for each variable, allowing for statistical inference about variable importance. 6. Select variables based on their importance scores, using a threshold to determine which variables are considered relevant. 7. The authors validate their method through extensive simulations and applications to real-world datasets, demonstrating its effectiveness in identifying important predictors while accounting for complex interactions and nonlinearities."
  },
  {
    "objectID": "Summaries/et/paper6.html#results-and-performance",
    "href": "Summaries/et/paper6.html#results-and-performance",
    "title": "Variable Selection Using Bayesian Additive Regression Trees",
    "section": "",
    "text": "The proposed permutation-based variable selection method using BART was evaluated through simulations and real-world applications. The results showed that the method effectively identified important predictor variables, outperforming traditional variable selection techniques in terms of accuracy and interpretability. The authors reported that their approach was able to capture complex interactions and nonlinear relationships among predictors, leading to improved model performance. Additionally, the method provided a clear ranking of variable importance, facilitating the interpretation of results. Overall, the findings suggest that the proposed method is a valuable tool for variable selection in regression models, particularly when using BART."
  },
  {
    "objectID": "Summaries/et/paper6.html#conclusion",
    "href": "Summaries/et/paper6.html#conclusion",
    "title": "Variable Selection Using Bayesian Additive Regression Trees",
    "section": "",
    "text": "The paper introduced three new variable selection approaches: (1) a permutation-based approach using within-type BART Variable Inclusion Proportion (VIP), (2) a permutation-based approach using BART Metropolis Importance (MI), and (3) a backward selection procedure with two filters.\nThese new approaches were designed specifically to address issues with existing methods, such as being biased against categorical predictors and being conservative in including relevant predictors, particularly in data settings with mixed-type predictors (e.g., continuous and binary).\nBased on the simulation results, where success was defined as an excellent capability of including all relevant predictors (rmiss \\(\\le 0.1\\)) and acceptable capability of excluding irrelevant predictors (precision \\(\\ge 0.6\\)), the three proposed approaches consistently perform well in identifying all the relevant predictors and excluding irrelevant predictors.\n\nRanking of Successful Approaches (by Success Rate)\nThe backward selection approach with two filters achieves the highest success rate (70.8%) across the various simulation scenarios tested. The two new permutation-based approaches followed closely in performance: 1. Backward selection with two filters: 70.8% success rate. 2. Permutation-based approach using BART MI: 66.7% success rate. 3. Permutation-based approach using BART Within-Type VIP: 62.5% success rate.\nIdentified Drawbacks\n\nA significant drawback of the three proposed approaches, similar to existing BART-based variable selection methods, is that they suffer from multicollinearity (correlated predictors). This challenge is particularly noticeable when the noise is high or when dealing with a binary response variable.\nThe computational cost of the backward selection approach is a shortcoming because it requires running BART multiple times, although this cost can be reduced by fitting the models in parallel on multiple cores.\nWhile the permutation-based approach using BART within-type VIP improves upon the existing BART VIP approach for small numbers of mixed-type predictors, it also suffers from multicollinearity."
  },
  {
    "objectID": "Summaries/et/paper3.html",
    "href": "Summaries/et/paper3.html",
    "title": "An introduction to using Bayesian linear regression with clinical data",
    "section": "",
    "text": "An introduction to using Bayesian linear regression with clinical data\nScott A. Baldwin, Michael J. Larson\nBehaviour Research and Therapy, Volume 98, 2017, Pages 58-75\nhttps://doi.org/10.1016/j.brat.2016.12.016\n\nSummary\nThis paper provides an introduction to Bayesian linear regression within the context of clinical data analysis. It contrasts Bayesian methodologies with traditional frequentist methods, highlighting the limitations of the latter, particularly in relation to p-values and confidence intervals. The article explains fundamental Bayesian concepts such as priors, likelihood, and posterior distributions. It presents an example using electroencephalogram (EEG) and anxiety study data to demonstrate the relationship between error-related negativity (ERN) and trait anxiety. It also covers practical aspects like Markov Chain Monte Carlo (MCMC) sampling, assessing model convergence, interpreting credible intervals, and evaluating model fit using WAIC and LOO-CV, extending the discussion to other models like logistic and Poisson regression.\n\n\nWhat is the goal of the paper?\nThe goal of the paper is to introduce Bayesian linear regression techniques to researchers in clinical psychology and related fields, demonstrating how these methods can provide more informative and nuanced insights compared to traditional frequentist approaches.\n\n\nWhat is the problem being addressed?\nThe problem being addressed is the limitations of frequentist statistical methods, particularly the reliance on p-values and confidence intervals, which can lead to misinterpretations and less informative results in clinical data analysis. ### Why is it important? It is important because clinical data often involve complexities such as small sample sizes, missing data, and hierarchical structures that traditional methods may not handle well. Bayesian methods offer a more flexible and robust framework for analyzing such data, leading to better-informed clinical decisions and research outcomes. ### What are the key results? Key results include a detailed explanation of Bayesian linear regression concepts, a practical example using EEG and anxiety data, and guidance on implementing Bayesian methods using software like R and Stan. The paper also discusses how to assess model convergence and fit, providing a comprehensive overview of Bayesian analysis in a clinical context. ### What are the limitations of the paper? Limitations of the paper include its focus on linear regression, which may not cover all types of clinical data analysis needs."
  },
  {
    "objectID": "Summaries/et/paper5.html",
    "href": "Summaries/et/paper5.html",
    "title": "A Dynamic Bayesian Model for Identifying High-Mortality Risk in Hospitalized COVID-19 Patients",
    "section": "",
    "text": "Amir Momeni-Boroujeni, Rachelle Mendoza, Isaac J. Stopard, Ben Lambert, and Alejandro Zuretti\nInfect. Dis. Rep. 2021, 13, 239–250. https://doi.org/10.3390/idr13010027\n\n\nTHis paper describes a study that used a Bayesian Markov model to better predict mortality risk for hospitalized COVID-19 patients by incorporating dynamic changes in laboratory values over time, rather than relying solely on admission data. The researchers collected demographic, comorbidity, and lab data for 553 PCR-positive patients and found that factors like age over 80 and certain comorbidities increased risk, but including dynamic changes in biomarkers significantly improved the predictive accuracy of the model. The study concludes by presenting a clinical decision tool that uses the most important factors for patient risk stratification based on available information at different stages of hospitalization.\n\n\n\nThe central problem this research aims to solve is how to accurately and quickly predict which hospitalized COVID-19 patients face the highest risk of death. Because hospitalization rates remained high and the disease caused millions of deaths globally, there was an urgent need to identify factors that predict severe disease and mortality to improve patient care and outcomes. The developed model is intended to allow for prioritization at the systems level and the individualization of care for each patient.\nA major shortcoming of previous prognostic tools was that they were static. Existing models generally looked at patient data and biomarkers only at the time of hospital admission to predict a single outcome, like death, at a single future time point. However, the sources note that a patient’s risk changes constantly during hospitalization, as biomarkers show outcome-specific dynamic changes. Existing models struggled to incorporate these dynamic changes and had difficulty handling the competing risks a patient faces, like remaining in the hospital, being discharged, or dying. Failing to account for these ongoing changes meant that risk predictions were often inaccurate in real-time clinical settings.\nTherefore, the authors developed a “Dynamic Bayesian Model” to overcome these limitations. This new model combines a patient’s initial information (demographics and comorbidities) with daily dynamic changes in laboratory test values throughout their hospital stay. By incorporating these time-dependent measurements, the model achieved dramatic improvements in predictive accuracy compared to models that relied only on admission data. This allows the model to provide daily adjustments to the patient’s in-hospital mortality risk, making it a more effective tool for clinicians deciding on appropriate care and resource allocation.\n\n\n\n\nCase selection\n\nPatients admitted to SUNY Downstate Medical Center, with COVID-19 related symptoms, between February 2020 and March 2020.\n\nSample\n\n553 PCR-positive patients included in the study.\nStratified into two groups: 200 patients who were discharged and 200 patients who died.\nData collected: demographic information, comorbidities, and laboratory test values.\n\nStatistical modeling approaches\n\nThe researchers conducted two main sets of analyses, both estimated using a Bayesian framework:\n\nDynamic Bayesian Markov Model\n\nThe primary goal was to develop a prognostic Markiv model that incorporates dynamic laboratory values with patients’ admission profiles.\nMarkov model accounts for competing risks (discharge vs. death) and allows for daily updates to mortality risk based on changing lab values.\nPredictor sets included:\n\nDemographics, comorbidities, admission, and lab values.\n\nThe model structure included:\n\nSecondary methodology: Logistic Regression Analysis\n\nThis analysis was conducted to determine factors most predictive of patient mortality but was specifically not intended to assess dynamic changes in mortality risk. This approach considered only patients’ outcomes, without factoring in the time taken for the outcome to occur.\n\nUnivariate Analysis (Baseline Comparison)\n\nBefore the multivariate analyses, univariate Cox survival analyses were used to illustrate the baseline patient characteristics and lab values upon admission that, individually, were the strongest determinants of risk.\n\n\n\nModel Validation\n\nInternal Validation: To assess the internal validity of the Markov model, the researchers performed k-fold cross-validation for each of the four regressions. The predictive accuracy increased significantly when dynamic test values were included, boosting accuracy from around 64–67% (for static models) to 83% (for the dynamic model).\nParameter Interpretation: Since the models were estimated in a Bayesian framework, the results are presented as probabilities representing the posterior probability that a given variable had an odds ratio exceeding one, removing the need for an arbitrary significance cutoff.\n\n\n\n\n\nThe performance results demonstrate that relying solely on static admission data (demographics, comorbidities, and initial lab values) yielded a mean predictive accuracy of only 64% to 67% across the initial regression sets. In contrast, incorporating dynamic changes in laboratory values (percentage changes relative to admission values) throughout hospitalization significantly boosted the model’s predictive power to an accuracy of 83%.\n\n\n\nThe article details a Dynamic Bayesian Markov Model that uses time-dependent changes in biomarkers to achieve 83% accuracy in identifying high-mortality risk in hospitalized COVID-19 patients, significantly outperforming static admission-only models (64–67% accuracy)."
  },
  {
    "objectID": "Summaries/et/paper5.html#summary",
    "href": "Summaries/et/paper5.html#summary",
    "title": "A Dynamic Bayesian Model for Identifying High-Mortality Risk in Hospitalized COVID-19 Patients",
    "section": "",
    "text": "THis paper describes a study that used a Bayesian Markov model to better predict mortality risk for hospitalized COVID-19 patients by incorporating dynamic changes in laboratory values over time, rather than relying solely on admission data. The researchers collected demographic, comorbidity, and lab data for 553 PCR-positive patients and found that factors like age over 80 and certain comorbidities increased risk, but including dynamic changes in biomarkers significantly improved the predictive accuracy of the model. The study concludes by presenting a clinical decision tool that uses the most important factors for patient risk stratification based on available information at different stages of hospitalization."
  },
  {
    "objectID": "Summaries/et/paper5.html#problem",
    "href": "Summaries/et/paper5.html#problem",
    "title": "A Dynamic Bayesian Model for Identifying High-Mortality Risk in Hospitalized COVID-19 Patients",
    "section": "",
    "text": "The central problem this research aims to solve is how to accurately and quickly predict which hospitalized COVID-19 patients face the highest risk of death. Because hospitalization rates remained high and the disease caused millions of deaths globally, there was an urgent need to identify factors that predict severe disease and mortality to improve patient care and outcomes. The developed model is intended to allow for prioritization at the systems level and the individualization of care for each patient.\nA major shortcoming of previous prognostic tools was that they were static. Existing models generally looked at patient data and biomarkers only at the time of hospital admission to predict a single outcome, like death, at a single future time point. However, the sources note that a patient’s risk changes constantly during hospitalization, as biomarkers show outcome-specific dynamic changes. Existing models struggled to incorporate these dynamic changes and had difficulty handling the competing risks a patient faces, like remaining in the hospital, being discharged, or dying. Failing to account for these ongoing changes meant that risk predictions were often inaccurate in real-time clinical settings.\nTherefore, the authors developed a “Dynamic Bayesian Model” to overcome these limitations. This new model combines a patient’s initial information (demographics and comorbidities) with daily dynamic changes in laboratory test values throughout their hospital stay. By incorporating these time-dependent measurements, the model achieved dramatic improvements in predictive accuracy compared to models that relied only on admission data. This allows the model to provide daily adjustments to the patient’s in-hospital mortality risk, making it a more effective tool for clinicians deciding on appropriate care and resource allocation."
  },
  {
    "objectID": "Summaries/et/paper5.html#methodology",
    "href": "Summaries/et/paper5.html#methodology",
    "title": "A Dynamic Bayesian Model for Identifying High-Mortality Risk in Hospitalized COVID-19 Patients",
    "section": "",
    "text": "Case selection\n\nPatients admitted to SUNY Downstate Medical Center, with COVID-19 related symptoms, between February 2020 and March 2020.\n\nSample\n\n553 PCR-positive patients included in the study.\nStratified into two groups: 200 patients who were discharged and 200 patients who died.\nData collected: demographic information, comorbidities, and laboratory test values.\n\nStatistical modeling approaches\n\nThe researchers conducted two main sets of analyses, both estimated using a Bayesian framework:\n\nDynamic Bayesian Markov Model\n\nThe primary goal was to develop a prognostic Markiv model that incorporates dynamic laboratory values with patients’ admission profiles.\nMarkov model accounts for competing risks (discharge vs. death) and allows for daily updates to mortality risk based on changing lab values.\nPredictor sets included:\n\nDemographics, comorbidities, admission, and lab values.\n\nThe model structure included:\n\nSecondary methodology: Logistic Regression Analysis\n\nThis analysis was conducted to determine factors most predictive of patient mortality but was specifically not intended to assess dynamic changes in mortality risk. This approach considered only patients’ outcomes, without factoring in the time taken for the outcome to occur.\n\nUnivariate Analysis (Baseline Comparison)\n\nBefore the multivariate analyses, univariate Cox survival analyses were used to illustrate the baseline patient characteristics and lab values upon admission that, individually, were the strongest determinants of risk.\n\n\n\nModel Validation\n\nInternal Validation: To assess the internal validity of the Markov model, the researchers performed k-fold cross-validation for each of the four regressions. The predictive accuracy increased significantly when dynamic test values were included, boosting accuracy from around 64–67% (for static models) to 83% (for the dynamic model).\nParameter Interpretation: Since the models were estimated in a Bayesian framework, the results are presented as probabilities representing the posterior probability that a given variable had an odds ratio exceeding one, removing the need for an arbitrary significance cutoff."
  },
  {
    "objectID": "Summaries/et/paper5.html#results-and-performance",
    "href": "Summaries/et/paper5.html#results-and-performance",
    "title": "A Dynamic Bayesian Model for Identifying High-Mortality Risk in Hospitalized COVID-19 Patients",
    "section": "",
    "text": "The performance results demonstrate that relying solely on static admission data (demographics, comorbidities, and initial lab values) yielded a mean predictive accuracy of only 64% to 67% across the initial regression sets. In contrast, incorporating dynamic changes in laboratory values (percentage changes relative to admission values) throughout hospitalization significantly boosted the model’s predictive power to an accuracy of 83%."
  },
  {
    "objectID": "Summaries/et/paper5.html#conclusion",
    "href": "Summaries/et/paper5.html#conclusion",
    "title": "A Dynamic Bayesian Model for Identifying High-Mortality Risk in Hospitalized COVID-19 Patients",
    "section": "",
    "text": "The article details a Dynamic Bayesian Markov Model that uses time-dependent changes in biomarkers to achieve 83% accuracy in identifying high-mortality risk in hospitalized COVID-19 patients, significantly outperforming static admission-only models (64–67% accuracy)."
  },
  {
    "objectID": "Summaries/nm/My_references.html",
    "href": "Summaries/nm/My_references.html",
    "title": "Bayesian Logistic Regression Capstone",
    "section": "",
    "text": "@article{Zeger2020, abstract = {The PCORI mission is to address questions about health care from the patients’ perspective, such as “What is my health status and its trajectory?” and “What are my treatment options and the expected benefits and harms of each?” The purpose of this PCORI-funded project is to make it easier for clinicians and patients to find valid answers to these and other clinical questions by using modern digital tools that support (1) learning from the experience of prior patients, and (2) translating what is learned to inform the decision at hand, taking into account each patient’s unique circumstances. For this project, we developed and implemented statistical methods called bayesian hierarchical models that combine existing data on past clinical experience from a reference population with new measurements for the individual. Clinicians currently use such methods when screening patients for disease. Modern technologies make it possible for this proven approach to extend far beyond its current use. The recent revolution in information technology has unleashed new types of health data, from DNA sequences to functional images of the brain to patient-reported outcomes. Furthermore, the electronic health record captures every patient’s sequence of health measurements, diagnoses, and treatments. The bayesian methods developed and reported on here combine even complex data to produce predictions about an individual patient’s health status, trajectory, and likely benefits and harms of interventions. In addition to developing novel methods, we facilitated their use by creating and locally disseminating a software package, OSLER inHealth, that will allow other researchers to apply this methodology. The software repository is open-source and includes the methodology developed as part of this research as well as other existing methods that facilitate individualized health prediction. We have tested the proposed methods and software on 3 case studies to (1) estimate the frequency with which various pathogens cause children’s pneumonia and predict which pathogen is likely to be causing a particular child’s pneumonia given her or his clinical data, potentially reducing unnecessary use of antibiotics; (2) infer whether a prostate cancer is indolent or aggressive for a patient under active surveillance; and (3) characterize the variation in multiple, time-varying symptoms of major mental disorders, including schizophrenia and depression, and then use this knowledge to provide patient-specific estimates of past and, likely, future trajectories. With this project, we have developed and demonstrated the value of combining even complex measurements on a population of patients, then translating this experience into more valid assessments of a new patient’s health status and trajectory. The model also supports inferences about the likely benefits and harms associated with available interventions. Copyright {} 2020. Johns Hopkins Bloomberg School of Public Health. All Rights Reserved.}, author = {Zeger, Scott L and Wu, Zhenke and Coley, Yates and Fojo, Anthony Todd and Carter, Bal and O’Brien, Katherine and Zandi, Peter and Cooke, Mary and Carey, Vince and Crainiceanu, Ciprian and Muscelli, John and Gherman, Adrian and Mekosh, Jason}, mendeley-groups = {CapStone_2025/CapStone_DS_2025}, number = {2020}, title = {{Using a Bayesian Approach to Predict Patients’ Health and Response to Treatment}}, url = {http://ovidsp.ovid.com/ovidweb.cgi?T=JS&PAGE=reference&D=medp&NEWS=N&AN=37708307}, year = {2020} }\n@article{Chatzimichail2023, abstract = {Medical diagnosis is the basis for treatment and management decisions in healthcare. Conventional methods for medical diagnosis commonly use established clinical criteria and fixed numerical thresholds. The limitations of such an approach may result in a failure to capture the intricate relations between diagnostic tests and the varying prevalence of diseases. To explore this further, we have developed a freely available specialized computational tool that employs Bayesian inference to calculate the posterior probability of disease diagnosis. This novel software comprises of three distinct modules, each designed to allow users to define and compare parametric and nonparametric distributions effectively. The tool is equipped to analyze datasets generated from two separate diagnostic tests, each performed on both diseased and nondiseased populations. We demonstrate the utility of this software by analyzing fasting plasma glucose, and glycated hemoglobin A1c data from the National Health and Nutrition Examination Survey. Our results are validated using the oral glucose tolerance test as a reference standard, and we explore both parametric and nonparametric distribution models for the Bayesian diagnosis of diabetes mellitus.}, author = {Chatzimichail, Theodora and Hatjimihail, Aristides T.}, doi = {10.3390/DIAGNOSTICS13193135,}, file = {:C:/Users/Namita/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chatzimichail, Hatjimihail - 2023 - A Bayesian Inference Based Computational Tool for Parametric and Nonparametric Medical Diagnosis.pdf:pdf}, issn = {20754418}, journal = {Diagnostics}, keywords = {Bayesian diagnosis,Bayesian inference,copula distribution,diabetes mellitus,kernel density estimator,likelihood,nonparametric distribution,parametric distribution,posterior probability,prior probability,probability density function}, mendeley-groups = {CapStone_2025}, month = {oct}, number = {19}, publisher = {Multidisciplinary Digital Publishing Institute (MDPI)}, title = {{A Bayesian Inference Based Computational Tool for Parametric and Nonparametric Medical Diagnosis}}, url = {https://pubmed.ncbi.nlm.nih.gov/37835877/}, volume = {13}, year = {2023} }\n@article{VandeSchoot2021, abstract = {Bayesian statistics is an approach to data analysis based on Bayes’ theorem, where available knowledge about parameters in a statistical model is updated with the information in observed data. The background knowledge is expressed as a prior distribution and combined with observational data in the form of a likelihood function to determine the posterior distribution. The posterior can also be used for making predictions about future events. This Primer describes the stages involved in Bayesian analysis, from specifying the prior and data models to deriving inference, model checking and refinement. We discuss the importance of prior and posterior predictive checking, selecting a proper technique for sampling from a posterior distribution, variational inference and variable selection. Examples of successful applications of Bayesian analysis across various research fields are provided, including in social sciences, ecology, genetics, medicine and more. We propose strategies for reproducibility and reporting standards, outlining an updated WAMBS (when to Worry and how to Avoid the Misuse of Bayesian Statistics) checklist. Finally, we outline the impact of Bayesian analysis on artificial intelligence, a major goal in the next decade.}, author = {van de Schoot, Rens and Depaoli, Sarah and King, Ruth and Kramer, Bianca and M{\"{a}}rtens, Kaspar and Tadesse, Mahlet G. and Vannucci, Marina and Gelman, Andrew and Veen, Duco and Willemsen, Joukje and Yau, Christopher}, doi = {10.1038/s43586-020-00001-2}, file = {:C:/Users/Namita/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/van de Schoot et al. - 2021 - Bayesian statistics and modelling.pdf:pdf}, issn = {2662-8449}, journal = {Nature Reviews Methods Primers}, keywords = {Scientific community,Statistics}, mendeley-groups = {CapStone_2025}, month = {jan}, number = {1}, pages = {1}, publisher = {Springer Nature}, title = {{Bayesian statistics and modelling}}, url = {https://www.nature.com/articles/s43586-020-00001-2}, volume = {1}, year = {2021} }\n@article{Klauenberg2015, abstract = {Regression is a common task in metrology and often applied to calibrate instruments, evaluate inter-laboratory comparisons or determine fundamental constants, for example. Yet, a regression model cannot be uniquely formulated as a measurement function, and consequently the Guide to the Expression of Uncertainty in Measurement (GUM) and its supplements are not applicable directly. Bayesian inference, however, is well suited to regression tasks, and has the advantage of accounting for additional a priori information, which typically robustifies analyses. Furthermore, it is anticipated that future revisions of the GUM shall also embrace the Bayesian view. Guidance on Bayesian inference for regression tasks is largely lacking in metrology. For linear regression models with Gaussian measurement errors this tutorial gives explicit guidance. Divided into three steps, the tutorial first illustrates how a priori knowledge, which is available from previous experiments, can be translated into prior distributions from a specific class. These prior distributions have the advantage of yielding analytical, closed form results, thus avoiding the need to apply numerical methods such as Markov Chain Monte Carlo. Secondly, formulas for the posterior results are given, explained and illustrated, and software implementations are provided. In the third step, Bayesian tools are used to assess the assumptions behind the suggested approach. These three steps (prior elicitation, posterior calculation, and robustness to prior uncertainty and model adequacy) are critical to Bayesian inference. The general guidance given here for Normal linear regression tasks is accompanied by a simple, but real-world, metrological example. The calibration of a flow device serves as a running example and illustrates the three steps. It is shown that prior knowledge from previous calibrations of the same sonic nozzle enables robust predictions even for extrapolations.}, author = {Klauenberg, Katy and W{\"{u}}bbeler, Gerd and Mickan, Bodo and Harris, Peter and Elster, Clemens}, doi = {10.1088/0026-1394/52/6/878}, file = {:C:/Users/Namita/Downloads/Klauenberg_2015_Metrologia_52_878.pdf:pdf;:C:/Users/Namita/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Klauenberg et al. - 2015 - A tutorial on Bayesian Normal linear regression.pdf:pdf}, issn = {16817575}, journal = {Metrologia}, keywords = {Bayesian inference,Gaussian measurement error,Normal inverse Gamma distribution,conjugate prior distribution,linear regression,prior knowledge,sonic nozzle calibration}, mendeley-groups = {CapStone_2025}, number = {6}, pages = {878–892}, publisher = {IOP Publishing}, title = {{A tutorial on Bayesian Normal linear regression}}, volume = {52}, year = {2015} }\n@article{DeLeeuw2012a, abstract = {In most research, linear regression analyses are performed without taking into account published results (i.e., reported summary statistics) of similar previous studies. Although the prior density in Bayesian linear regression could accommodate such prior knowledge, formal models for doing so are absent from the literature. The goal of this article is therefore to develop a Bayesian model in which a linear regression analysis on current data is augmented with the reported regression coefficients (and standard errors) of previous studies. Two versions of this model are presented. The first version incorporates previous studies through the prior density and is applicable when the current and all previous studies are exchangeable. The second version models all studies in a hierarchical structure and is applicable when studies are not exchangeable. Both versions of the model are assessed using simulation studies. Performance for each in estimating the regression coefficients is consistently superior to using current data alone and is close to that of an equivalent model that uses the data from previous studies rather than reported regression coefficients. Overall the results show that augmenting data with results from previous studies is viable and yields significant improvements in the parameter estimation. {} 2012 Copyright Taylor and Francis Group, LLC.}, author = {de Leeuw, Christiaan and Klugkist, Irene}, doi = {10.1080/00273171.2012.673957}, file = {:C:/Users/Namita/Downloads/Augmenting Data With Published Results in Bayesian Linear Regression.pdf:pdf}, issn = {00273171}, journal = {Multivariate Behavioral Research}, mendeley-groups = {CapStone_2025}, number = {3}, pages = {369–391}, title = {{Augmenting Data With Published Results in Bayesian Linear Regression}}, volume = {47}, year = {2012} }\n@article{Liu2013, abstract = {Background: A Bayesian clinical reasoning model was developed to predict an individual risk for cardiovascular disease (CVD) for desk-top reference. Methods: Three Bayesian models were constructed to estimate the CVD risk by sequentially incorporating demographic features (basic), six metabolic syndrome components (metabolic score) and conventional risk factors (enhanced model). By considering clinical weights (regression coefficients) of each model as normal distribution, individual risk can be predicted making allowance for uncertainty of clinical weights. A community-based cohort that enrolled 64,489 participants free of CVD at baseline and followed up over five years to ascertain newly diagnosed CVD cases during the period through 2000 to 2004 was used for the illustration of the three proposed models (full empirical data are available from website http://homepage.ntu.edu.tw/\\(\\sim\\)chenlin/CVD-prediction-data.rar). Results: The proposed models can be applied to predicting the CVD risk with any combination of risk factors. For a 47-year-old man, the five-year risk for CVD with the basic model was 11.2% (95% CI: 7.8%-15.6%). His metabolic syndrome score, leading to 1.488 of likelihood ratio, enhanced the risk for CVD up to 15.8% (95% CI: 11.0%-21.5%) and put him in highest deciles. As with the habit of smoking over 2 packs per-day and family history of CVD, yielding the likelihood ratios of 1.62 and 1.47, respectively, the risk was further raised to 30.9% (95% CI: 20.7%-39.8%). Conclusions: We demonstrate how to make individual risk prediction for CVD by incorporating routine information with a sequential Bayesian clinical reasoning approach. {} 2012 Elsevier Ireland Ltd.}, author = {Liu, Yi Ming and Chen, Sam Li Sheng and Yen, Amy Ming Fang and Chen, Hsiu Hsi}, doi = {10.1016/J.IJCARD.2012.05.016}, file = {:C:/Users/Namita/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu et al. - 2013 - Individual risk prediction model for incident cardiovascular disease A Bayesian clinical reasoning approach(9).pdf:pdf}, issn = {0167-5273}, journal = {International Journal of Cardiology}, keywords = {Bayes’ theorem,Bayesian,Cardiovascular disease,Likelihood ratio,Metabolic syndrome,Prediction model}, mendeley-groups = {CapStone_2025}, month = {sep}, number = {5}, pages = {2008–2012}, pmid = {22658349}, publisher = {Elsevier}, title = {{Individual risk prediction model for incident cardiovascular disease: A Bayesian clinical reasoning approach}}, url = {https://www.sciencedirect.com/science/article/pii/S0167527312006274}, volume = {167}, year = {2013} }"
  },
  {
    "objectID": "Summaries/nm/r code.html",
    "href": "Summaries/nm/r code.html",
    "title": "making subsets for each dataset",
    "section": "",
    "text": "NHANES DATASET -https://wwwn.cdc.gov/nchs/nhanes/continuousnhanes/default.aspx?BeginYear=2013 # loading packages\nlibrary(tidyverse) library(knitr) library(ggthemes) library(ggrepel) library(dslabs) library(Hmisc) library(dplyr) library(tidyr) library(forcats) library(ggplot2) library (“nhanesA”)\noptions(repos = c(CRAN = “https://cloud.r-project.org”))\ninstall.packages(“nhanesA”)\n\nmaking subsets for each dataset\n                   nhanesTables('EXAM', 2013)\n                   nhanesTables('QUESTIONNAIRE', 2013)\n                   nhanesTables('DEMOGRAPHICS', 2013)\n                   \n                   \nnhanesCodebook(“BMX_H”) nhanesCodebook(“SMQ_H”)\n# .xpt files read ( 2013–2014) bmx_h &lt;- nhanes(“BMX_H”) #Exam smq_h &lt;- nhanes(“SMQ_H”) #Quest demo_h &lt;- nhanes(“DEMO_H”) #Demo diq_h &lt;- nhanes(“DIQ_H”) #diabetes\n\n\nvariables of interest\nexam_sub &lt;- bmx_h %&gt;% select(SEQN, BMDBMIC) demo_sub &lt;- demo_h %&gt;% select(SEQN, RIDAGEYR, RIAGENDR, RIDRETH1, SDMVPSU, SDMVSTRA, WTMEC2YR) diq_sub &lt;- diq_h %&gt;% select (SEQN, DIQ240)\n\n\nNames of all variables selected for analysis\nnames(exam_sub) names(demo_sub) names(diq_sub)\n\n\nmerged dataframe\nmerged_data &lt;- exam_sub %&gt;% left_join(demo_sub, by = “SEQN”) %&gt;% left_join(diq_sub, by = “SEQN”) head(merged_data)\nnhanesCodebook(“DEMO_H”,‘RIDRETH1’) nhanesCodebook(“DEMO_H”,‘RIAGENDR’) nhanesCodebook(“DEMO_H”,‘RIDAGEYR’) nhanesCodebook(“DIQ_H”,“DIQ240”) nhanesCodebook(“BMX_H”,‘BMDBMIC’)"
  },
  {
    "objectID": "Summaries/nm/Summaries.html",
    "href": "Summaries/nm/Summaries.html",
    "title": "Bayesian Logistic Regression Capstone",
    "section": "",
    "text": "All 6 summaries and references & citation"
  },
  {
    "objectID": "Summaries/nm/Summaries.html#namitas-literature",
    "href": "Summaries/nm/Summaries.html#namitas-literature",
    "title": "Bayesian Logistic Regression Capstone",
    "section": "Namita’s Literature",
    "text": "Namita’s Literature"
  },
  {
    "objectID": "Summaries/nm/Summaries.html#introduction",
    "href": "Summaries/nm/Summaries.html#introduction",
    "title": "Bayesian Logistic Regression Capstone",
    "section": "Introduction",
    "text": "Introduction\n\nBayesian Hierarchical Model (Disease reclassification and prediction)\n\nWhat is the goal of the paper?\nThe authors develop a Bayesian hierarchical model for multivariate longitudinal data to predict health status, trajectories, and intervention effects at the individual level in the PCORI mission to address questions about health status from patients and clinicians.\nWhy is it important?\nHealthcare data (DNA sequences, functional images of the brain, patient-reported outcomes, and electronic health records with patients’ sequences of health measurements, diagnoses, and treatments) are complex, and the standard approaches are not adequate for clinical data analysis. Electronic health records (EHRs) could improve diagnostic accuracy and predict treatment effects. Visualizations of characteristics of posterior distributions can be immediately understood by clinicians and patients as relevant to their decision. Combining prior knowledge and patient data with evidence could predict the patient’s health status, trajectory, and/or likely benefits of interventions.\nHow is it solved?\nMethod: The authors applied Bayesian hierarchical regression for multivariate longitudinal patient data using open-source R-packages and developed 2 levels—time within person and persons within a population\nThe model combined exogenous (eg, age, clinical history) factors and endogenous (eg, current treatment) variables on the individual’s multivariate health measurements and the effects of health measurements at one time on subsequent interventions.\nThe model produced an estimate of the posterior distribution for each value of the predictor variable and an estimate of the marginal distribution of the regression coefficients for each coefficient that measures the outcome (health status) associated with its predictor variables. In a larger sample, the likelihood dominates the prior distribution for regression coefficients and Bayesian hierarchical model used a likelihood-based approach, used priors (prior laboratory and clinical trials data) that provided the assay sensitivities, which through the prior assumptions, made the model identifiable and the integration of Markov chain Monte Carlo (MCMC) estimates the posterior distributions, avoided missing data and complex outcome measurements.\nResults/limitations\nThree case studies: pneumonia etiology in children, prostate cancer, and mental disorders chosen for model development, identified low-risk patient population, reduced the risk of overtreatment, complications, adverse effects, and financial burden for patients (Disease Reclassification). Prostate cancer software was then implemented within the JHM HER.\nLimitation:\nModels were entirely parametric, and extensions to nonparametric or more flexible parametric models were recommended to improve approaches for neuroimage or genomic data.\nApplications:\n\nto scale a tool that addresses a particular unmet need across a larger, more diverse population of patients and clinicians\nuse in autoimmune diseases, sudden cardiac arrest, and diabetes.\nembed a collection of tools to acquire and use the most relevant information, agnostic to its level of measurement, to improve population and individual health decisions that cause better outcomes at more affordable costs. @Zeger2020\n\n\nBayesian Inference (parametric vs non-parametric)\n\nWhat is the goal of the paper?\nThe authors calculated the posterior probability of disease diagnosis and applied Bayesian inference to develop three modules comparing parametric (with a fixed set of parameters) and nonparametric distributions (which do not make a priori assumptions) by analyzing the National Health and Nutrition Examination Survey dataset from two separate diagnostic tests on both diseased and non-diseased populations.\nWhy is it important?\nMedical diagnosis, treatment, and management decisions are crucial, and conventional methods for diagnosis using clinical criteria and fixed numerical thresholds limit the capture of other information related to the intricate relationship between diagnostic tests and the varying prevalence of diseases. The probability distributions associated with quantitative diagnostic test outcomes often demonstrate some overlap between the diseased and nondiseased groups. The dichotomous method fails to capture the complexity and heterogeneity of disease presentations across diverse populations. The applicability of the normal distribution (conventional method) is critiqued, especially in dealing with clinical measurands having skewness, bimodality, or multimodality.\nHow is it solved?\nMethods: The Authors developed models employing Bayesian inference (Bayesian diagnostic approach) to calculate the posterior probability of @Liu2013"
  },
  {
    "objectID": "Summaries/nm/Comparing conventional and Bayesian.html",
    "href": "Summaries/nm/Comparing conventional and Bayesian.html",
    "title": "Bayesian Logistic Regression Capstone",
    "section": "",
    "text": "Comparing conventional and Bayesian workflows for clinical outcome prediction modelling with an exemplar cohort study of severe COVID-19 infection incorporating clinical biomarker test results\nRef: Sullivan, B., Barker, E., MacGregor, L. et al. Comparing conventional and Bayesian workflows for clinical outcome prediction modelling with an exemplar cohort study of severe COVID-19 infection incorporating clinical biomarker test results. BMC Med Inform Decis Mak 25, 123 (2025). https://doi.org/10.1186/s12911-025-02955-3\nProblem: Data curation is challenging as clinical data are heterogeneous in multiple ways. Biomarkers are recorded for different reasons. If missing data imputation is performed, it raises another decision point on whether to impute the continuous or the transformed categorical data.\nAim: Estimating predictive risk factors for disease outcomes with explainable statistical models is desirable for clinical use and decision making. The author provide a guide for modern Bayesian approaches for joint risk factor analysis and variable selection.\nStudy design: Retrospective observational cohort design, lab data linked to the patient data for laboratory markers and clinical outcomes from three hospitals in the Southwest region of England, UK on adults admitted between March to October 2020 and tested positive for SARS CoV-2 by PCR.\nMethod: Analyze range of laboratory blood marker values, Develop cross-validated logistic regression prediction models using the candidate biomarkers, highlighting biomarkers worthy of future research. Employed selection techniques, comparing LASSO frequentist method and Projective Prediction approach on Bayesian logistic regression models with horseshoe priors to illustrate the process of creating a reduced model. They considered models deliver good prediction performance with a small amount of biomarker data.\nVairables (1) Predictors: Includes a variety of clinical severity indices, lab biomarkers (microbiological, immunological, haematological and biochemistry) as parameters used as predictive variables in the regression models.\n\nOutcome: The primary prediction outcome was death or transfer to the ICU.\n\nData management: Continuous biomarkers were trannsfromed into categorical variables (reference ranges for clinical use).\nStaistical calcukation: Analytics carried out using the R statistical language using- Standard logistic regression analyses used the R Stats GLM package (v4.4.1); LASSO analyses, GLMnet (v4.1.8); and for Bayesian analyses, BRMS (v2.22.0) and ProjPred (v2.8.0).\nBefore running full regression models, the independent contribution of individual biomarkers were examined in the training dataset predicting ICU entry or death via standard logistic regressions and Bayesian logistic regressions with either a flat (aka uniform) or horseshoe prior and calculated p-values and odds ratios for each biomarker.\nPer biomarker, patients with and without the outcome were separated and then these groups were shuffled and split into 5 equal subgroups. These groups were then paired at random, to ensure training and test datasets have the same proportion of patients with a severe outcome as in full the sample for that biomarker. This was to improve the chance of convergence for biomarkers with high data missingness and only complete cases of training data available for each biomarker were considered for the study.\nEach individual biomarker model including age and gender (except univariate age and gender models) were compared against a standard model including only age and gender. Regressions were fit using all associated dummy variables for a given biomarker (e.g. ‘Mild’, ‘Moderate’, ‘Severe’) using ‘Normal’ as the reference.\nAnalysis using all valid biomarker data: After individual biomarker evaluation, logistic regression models considering all valid biomarkers (Prediction using individual variables section) and demographic variables were fit to the data and the predictions were tested via internal and external validation using the stratified cross-validation procedures.\nAnalysis using reduced variable models: Considering that eben though a model using all biomarker data may have strong predictive power, but clinically a strong prediction with the least amount of biomarkers might save on time, money and other resources, they used LASSO and Bayesian Projective Prediction methodologies to choose reduced variable models to predict COVID-19 severe outcomes.\nTo estimate variability in model performance and allow comparison between models, we compute inter-quantile AUC difference ranges using 5-fold 20-repeat cross-validation of models\nReduced variable models: LASSO and projective prediction performed for creation of reduced models with fewer biomarkers.\nModel performance evaluation and dissemination They chose to peform cross-validated estimates of AUC, sensitivity, and specificity and the Inter-quartile intervals over these measures.\nRecommendations: Categorization is worth critical consideration in model planning Reduced number of variables Imputation Bayesian approaches should include that coefficients estimated via Bayes should on average deliver better predictive performance than standard GLM"
  },
  {
    "objectID": "Summaries/aw/paper2_summary.html",
    "href": "Summaries/aw/paper2_summary.html",
    "title": "Summary: An Introduction to Using Bayesian Linear Regression with Clinical Data",
    "section": "",
    "text": "Scott A. Baldwin and Michael J. Larson (2017)\n\n\nTraditional statistics training in psychology emphasizes frequentist methods, and researchers often feel unsure about Bayesian alternatives. This paper aims to introduce Bayesian linear regression in a clear and practical way, using real clinical data to help researchers understand and apply Bayesian tools.\n\n\n\nThe authors walk readers through setting up practical Bayesian regression models. They explain how to choose and justify priors, run models using software (like R and Stan), check convergence, and interpret results—including posterior distributions, credible intervals, and predictive outputs. They also compare Bayesian and frequentist approaches and offer useful model-comparison tools. The article includes EEG-anxiety data, and it comes with the data and R code for easy replication.\n\n\n\nThe paper shows how Bayesian regression can be applied step-by-step and how outputs like posterior distributions and interval estimates offer more intuitive, flexible insights than traditional methods. Including data and code makes the method transparent and replicable, while model diagnostics (like convergence checks) ensure reliability.\n\n\n\nThe paper is introductory; it doesn’t dive into advanced modeling extensions or deeply mathematical derivations. It focuses on a single case study (EEG data and anxiety) so broader generalization requires additional application. Expertise is still needed to adapt methods to other complex datasets.\n\n\n\nThe authors illustrate methods using clinical EEG data related to error-related negativity (ERN) and trait anxiety. They provide both the actual dataset and R/Stan code for readers to replicate and learn directly from the example."
  },
  {
    "objectID": "Summaries/aw/paper2_summary.html#problem-the-article-is-addressing",
    "href": "Summaries/aw/paper2_summary.html#problem-the-article-is-addressing",
    "title": "Summary: An Introduction to Using Bayesian Linear Regression with Clinical Data",
    "section": "",
    "text": "Traditional statistics training in psychology emphasizes frequentist methods, and researchers often feel unsure about Bayesian alternatives. This paper aims to introduce Bayesian linear regression in a clear and practical way, using real clinical data to help researchers understand and apply Bayesian tools."
  },
  {
    "objectID": "Summaries/aw/paper2_summary.html#how-it-has-been-solved",
    "href": "Summaries/aw/paper2_summary.html#how-it-has-been-solved",
    "title": "Summary: An Introduction to Using Bayesian Linear Regression with Clinical Data",
    "section": "",
    "text": "The authors walk readers through setting up practical Bayesian regression models. They explain how to choose and justify priors, run models using software (like R and Stan), check convergence, and interpret results—including posterior distributions, credible intervals, and predictive outputs. They also compare Bayesian and frequentist approaches and offer useful model-comparison tools. The article includes EEG-anxiety data, and it comes with the data and R code for easy replication."
  },
  {
    "objectID": "Summaries/aw/paper2_summary.html#results",
    "href": "Summaries/aw/paper2_summary.html#results",
    "title": "Summary: An Introduction to Using Bayesian Linear Regression with Clinical Data",
    "section": "",
    "text": "The paper shows how Bayesian regression can be applied step-by-step and how outputs like posterior distributions and interval estimates offer more intuitive, flexible insights than traditional methods. Including data and code makes the method transparent and replicable, while model diagnostics (like convergence checks) ensure reliability."
  },
  {
    "objectID": "Summaries/aw/paper2_summary.html#limitations",
    "href": "Summaries/aw/paper2_summary.html#limitations",
    "title": "Summary: An Introduction to Using Bayesian Linear Regression with Clinical Data",
    "section": "",
    "text": "The paper is introductory; it doesn’t dive into advanced modeling extensions or deeply mathematical derivations. It focuses on a single case study (EEG data and anxiety) so broader generalization requires additional application. Expertise is still needed to adapt methods to other complex datasets."
  },
  {
    "objectID": "Summaries/aw/paper2_summary.html#datasets",
    "href": "Summaries/aw/paper2_summary.html#datasets",
    "title": "Summary: An Introduction to Using Bayesian Linear Regression with Clinical Data",
    "section": "",
    "text": "The authors illustrate methods using clinical EEG data related to error-related negativity (ERN) and trait anxiety. They provide both the actual dataset and R/Stan code for readers to replicate and learn directly from the example."
  },
  {
    "objectID": "Summaries/aw/paper4_summary.html",
    "href": "Summaries/aw/paper4_summary.html",
    "title": "Summary: Dropout as a Bayesian Approximation—Representing Model Uncertainty in Deep Learning",
    "section": "",
    "text": "Yarin Gal & Zoubin Ghahramani (2016)\n\n\nDeep neural networks achieve strong predictive accuracy but typically do not quantify uncertainty. This limitation is critical in applications like healthcare or autonomous systems, where overconfidence can lead to serious consequences. Traditional Bayesian neural networks can model uncertainty but are often computationally expensive and hard to train.\n\n\n\nThe authors demonstrate that standard dropout, a common regularization technique, can serve as an approximate Bayesian inference method. By keeping dropout active during both training and testing and performing multiple stochastic forward passes, the resulting distribution of predictions can be interpreted as samples from a posterior. This provides an efficient, scalable way to estimate model uncertainty without altering model architecture.\n\n\n\nThe paper shows that “Monte Carlo Dropout” (MC-Dropout) yields well-calibrated uncertainty estimates and competitive accuracy on tasks such as image classification (MNIST, CIFAR-10) and regression. The method improves out-of-distribution detection, active learning, and decision-making under uncertainty.\n\n\n\nThe uncertainty estimates depend on the dropout rate and can underestimate epistemic uncertainty for small or biased datasets. The approach is an approximation, not a full Bayesian posterior, so results may deviate from exact inference. Additional calibration steps may still be required.\n\n\n\nExperiments include standard image datasets (MNIST, CIFAR-10) and regression benchmarks from the UCI repository. Predictions are made using multiple forward passes with dropout enabled to approximate posterior mean and variance."
  },
  {
    "objectID": "Summaries/aw/paper4_summary.html#problem-the-article-is-addressing",
    "href": "Summaries/aw/paper4_summary.html#problem-the-article-is-addressing",
    "title": "Summary: Dropout as a Bayesian Approximation—Representing Model Uncertainty in Deep Learning",
    "section": "",
    "text": "Deep neural networks achieve strong predictive accuracy but typically do not quantify uncertainty. This limitation is critical in applications like healthcare or autonomous systems, where overconfidence can lead to serious consequences. Traditional Bayesian neural networks can model uncertainty but are often computationally expensive and hard to train."
  },
  {
    "objectID": "Summaries/aw/paper4_summary.html#how-it-has-been-solved",
    "href": "Summaries/aw/paper4_summary.html#how-it-has-been-solved",
    "title": "Summary: Dropout as a Bayesian Approximation—Representing Model Uncertainty in Deep Learning",
    "section": "",
    "text": "The authors demonstrate that standard dropout, a common regularization technique, can serve as an approximate Bayesian inference method. By keeping dropout active during both training and testing and performing multiple stochastic forward passes, the resulting distribution of predictions can be interpreted as samples from a posterior. This provides an efficient, scalable way to estimate model uncertainty without altering model architecture."
  },
  {
    "objectID": "Summaries/aw/paper4_summary.html#results",
    "href": "Summaries/aw/paper4_summary.html#results",
    "title": "Summary: Dropout as a Bayesian Approximation—Representing Model Uncertainty in Deep Learning",
    "section": "",
    "text": "The paper shows that “Monte Carlo Dropout” (MC-Dropout) yields well-calibrated uncertainty estimates and competitive accuracy on tasks such as image classification (MNIST, CIFAR-10) and regression. The method improves out-of-distribution detection, active learning, and decision-making under uncertainty."
  },
  {
    "objectID": "Summaries/aw/paper4_summary.html#limitations",
    "href": "Summaries/aw/paper4_summary.html#limitations",
    "title": "Summary: Dropout as a Bayesian Approximation—Representing Model Uncertainty in Deep Learning",
    "section": "",
    "text": "The uncertainty estimates depend on the dropout rate and can underestimate epistemic uncertainty for small or biased datasets. The approach is an approximation, not a full Bayesian posterior, so results may deviate from exact inference. Additional calibration steps may still be required."
  },
  {
    "objectID": "Summaries/aw/paper4_summary.html#datasets",
    "href": "Summaries/aw/paper4_summary.html#datasets",
    "title": "Summary: Dropout as a Bayesian Approximation—Representing Model Uncertainty in Deep Learning",
    "section": "",
    "text": "Experiments include standard image datasets (MNIST, CIFAR-10) and regression benchmarks from the UCI repository. Predictions are made using multiple forward passes with dropout enabled to approximate posterior mean and variance."
  },
  {
    "objectID": "Summaries/aw/paper6_summary.html",
    "href": "Summaries/aw/paper6_summary.html",
    "title": "Summary: Bayesian Model Averaging: A Practical Review",
    "section": "",
    "text": "Jennifer A. Hoeting, David Madigan, Adrian E. Raftery, & Chris T. Volinsky (1999)\n\n\nIn applied statistics and data science, analysts often face uncertainty about which model best represents the data. Standard practice is to select a single “best” model using criteria like AIC or BIC, but this ignores model uncertainty and leads to overconfident inferences and biased predictions. The paper addresses how to properly account for model uncertainty in statistical inference.\n\n\n\nThe authors propose Bayesian Model Averaging (BMA), a framework that combines predictions from multiple models weighted by their posterior probabilities. Instead of committing to a single model, BMA integrates over all plausible models using Bayes’ theorem. This approach accounts for uncertainty in both model selection and parameter estimation, yielding more robust predictions.\n\n\n\nBMA is shown to improve predictive performance and reduce overfitting across a variety of domains, including regression, classification, and time-series modeling. The method provides posterior model probabilities and model-averaged parameter estimates, offering a principled way to handle model uncertainty. The paper also presents examples and computational techniques for implementing BMA, such as Markov Chain Monte Carlo Model Composition (MC³).\n\n\n\nBMA can be computationally demanding when the number of candidate models is large, as it requires calculating and storing posterior probabilities for each. Its success also depends on reasonable prior specifications over both models and parameters. Simplified approximations (like Occam’s window) may introduce bias but are often necessary for feasibility.\n\n\n\nThe authors apply BMA to several real-world and simulated datasets, including linear regression examples and ecological modeling case studies. The paper focuses on demonstrating methodology rather than a specific dataset, emphasizing reproducible model-averaging workflows and sensitivity analysis."
  },
  {
    "objectID": "Summaries/aw/paper6_summary.html#problem-the-article-is-addressing",
    "href": "Summaries/aw/paper6_summary.html#problem-the-article-is-addressing",
    "title": "Summary: Bayesian Model Averaging: A Practical Review",
    "section": "",
    "text": "In applied statistics and data science, analysts often face uncertainty about which model best represents the data. Standard practice is to select a single “best” model using criteria like AIC or BIC, but this ignores model uncertainty and leads to overconfident inferences and biased predictions. The paper addresses how to properly account for model uncertainty in statistical inference."
  },
  {
    "objectID": "Summaries/aw/paper6_summary.html#how-it-has-been-solved",
    "href": "Summaries/aw/paper6_summary.html#how-it-has-been-solved",
    "title": "Summary: Bayesian Model Averaging: A Practical Review",
    "section": "",
    "text": "The authors propose Bayesian Model Averaging (BMA), a framework that combines predictions from multiple models weighted by their posterior probabilities. Instead of committing to a single model, BMA integrates over all plausible models using Bayes’ theorem. This approach accounts for uncertainty in both model selection and parameter estimation, yielding more robust predictions."
  },
  {
    "objectID": "Summaries/aw/paper6_summary.html#results",
    "href": "Summaries/aw/paper6_summary.html#results",
    "title": "Summary: Bayesian Model Averaging: A Practical Review",
    "section": "",
    "text": "BMA is shown to improve predictive performance and reduce overfitting across a variety of domains, including regression, classification, and time-series modeling. The method provides posterior model probabilities and model-averaged parameter estimates, offering a principled way to handle model uncertainty. The paper also presents examples and computational techniques for implementing BMA, such as Markov Chain Monte Carlo Model Composition (MC³)."
  },
  {
    "objectID": "Summaries/aw/paper6_summary.html#limitations",
    "href": "Summaries/aw/paper6_summary.html#limitations",
    "title": "Summary: Bayesian Model Averaging: A Practical Review",
    "section": "",
    "text": "BMA can be computationally demanding when the number of candidate models is large, as it requires calculating and storing posterior probabilities for each. Its success also depends on reasonable prior specifications over both models and parameters. Simplified approximations (like Occam’s window) may introduce bias but are often necessary for feasibility."
  },
  {
    "objectID": "Summaries/aw/paper6_summary.html#datasets",
    "href": "Summaries/aw/paper6_summary.html#datasets",
    "title": "Summary: Bayesian Model Averaging: A Practical Review",
    "section": "",
    "text": "The authors apply BMA to several real-world and simulated datasets, including linear regression examples and ecological modeling case studies. The paper focuses on demonstrating methodology rather than a specific dataset, emphasizing reproducible model-averaging workflows and sensitivity analysis."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk Using NHANES 2013–2014 Data",
    "section": "",
    "text": "Slides: slides.html (Edit slides.qmd.)"
  },
  {
    "objectID": "index.html#aims",
    "href": "index.html#aims",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk Using NHANES 2013–2014 Data",
    "section": "Aims",
    "text": "Aims\nThe present study employs Bayesian logistic regression to predict diabetes status and examine the relationships between diabetes and key predictors, including body mass index (BMI), age (≥20 years), sex, and race. Using retrospective data from the 2013–2014 NHANES survey, the analysis accounts for the study’s complex sampling design, which involves stratification, clustering, and the oversampling of specific subpopulations rather than simple random sampling. The Bayesian framework is applied to address common analytical challenges such as missing data, complete case bias, and data separation, thereby improving the robustness and reliability of inference compared to traditional logistic regression methods."
  },
  {
    "objectID": "index.html#bayesian-logistic-regression",
    "href": "index.html#bayesian-logistic-regression",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk Using NHANES 2013–2014 Data",
    "section": "Bayesian Logistic Regression",
    "text": "Bayesian Logistic Regression\nThe Bayesian framework integrates prior knowledge with observed data to generate posterior distributions, allowing parameters to be interpreted directly in probabilistic terms.\nUnlike traditional frequentist approaches that yield single-point estimates and p-values, Bayesian methods represent parameters as random variables with full probability distributions.\nThis provides greater flexibility, incorporates parameter uncertainty, and produces credible intervals that directly quantify the probability that a parameter lies within a given range."
  },
  {
    "objectID": "index.html#model-structure",
    "href": "index.html#model-structure",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk Using NHANES 2013–2014 Data",
    "section": "Model Structure",
    "text": "Model Structure\nBayesian logistic regression models the log-odds of a binary outcome as a linear combination of predictors:\n\\[\n\\text{logit}(P(Y = 1)) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_k X_k\n\\]\nwhere\n\n\\(P(Y = 1)\\) is the probability of the event of interest,\n\\(\\beta_0\\) is the intercept (log-odds when all predictors are zero), and\n\\(\\beta_j\\) represents the effect of predictor \\(X_j\\) on the log-odds of the outcome, holding other predictors constant.\n\nIn the Bayesian framework, model parameters (\\(\\boldsymbol{\\beta}\\)) are treated as random variables and assigned prior distributions that reflect existing knowledge or plausible ranges before observing the data. After incorporating the observed evidence, the priors are updated through Bayes’ theorem (Leeuw and Klugkist 2012; Klauenberg et al. 2015):\n\\[\n\\text{Posterior} \\propto \\text{Likelihood} \\times \\text{Prior}\n\\]\n\nLikelihood: represents the probability of the observed data given the model parameters—it captures how well different parameter values explain the data.\nPrior: expresses beliefs or existing information about the parameters before observing the data.\nPosterior: combines both, representing the updated distribution of parameter values after accounting for the data.\n\nThis formulation allows uncertainty to propagate naturally through the model, producing posterior distributions for each coefficient that can be directly interpreted as probabilities."
  },
  {
    "objectID": "index.html#prior-specification",
    "href": "index.html#prior-specification",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk Using NHANES 2013–2014 Data",
    "section": "Prior Specification",
    "text": "Prior Specification\nWeakly informative priors were used to regularize estimation without imposing strong assumptions:\n\nRegression coefficients: \\(N(0, 2.5)\\), providing gentle regularization while allowing substantial variation in plausible effects (Gelman et al. 2008; Vande Schoot et al. 2021).\nIntercept: Student’s t-distribution prior, \\(t(3, 0, 10)\\) (Schoot et al. 2013; Vande Schoot et al. 2021), which has\n\n3 degrees of freedom (heavy tails to allow occasional large effects),\nmean 0 (no bias toward positive or negative effects), and\nscale 10 (broad range of possible values).\n\n\nSuch priors help stabilize estimation in the presence of multicollinearity, limited sample size, or potential outliers."
  },
  {
    "objectID": "index.html#advantages-of-bayesian-logistic-regression",
    "href": "index.html#advantages-of-bayesian-logistic-regression",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk Using NHANES 2013–2014 Data",
    "section": "Advantages of Bayesian Logistic Regression",
    "text": "Advantages of Bayesian Logistic Regression\n\nUncertainty quantification: Produces full posterior distributions instead of single estimates.\nCredible intervals: Provide the range within which a parameter lies with a specified probability (e.g., 95%).\nFlexible priors: Allow integration of expert knowledge or findings from prior studies.\nProbabilistic predictions: Posterior predictive distributions yield direct probabilities for new or future observations.\nModel evaluation: Posterior predictive checks (PPCs) assess how well simulated outcomes reproduce observed data."
  },
  {
    "objectID": "index.html#posterior-predictions",
    "href": "index.html#posterior-predictions",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk Using NHANES 2013–2014 Data",
    "section": "Posterior Predictions",
    "text": "Posterior Predictions\nPosterior distributions of regression coefficients were used to estimate the probability of the outcome for given predictor values. This allows statements such as:\nGiven the predictors, the probability of the outcome lies between X% and Y%.\nPosterior predictions account for two key sources of uncertainty:\n\nParameter uncertainty: Variability in estimated model coefficients.\nPredictive uncertainty: Variability in possible future outcomes given those parameters.\n\nIn Bayesian analysis, all unknown quantities—coefficients, means, variances, or probabilities—are treated as random variables described by their posterior distributions."
  },
  {
    "objectID": "index.html#model-evaluation-and-diagnostics",
    "href": "index.html#model-evaluation-and-diagnostics",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk Using NHANES 2013–2014 Data",
    "section": "Model Evaluation and Diagnostics",
    "text": "Model Evaluation and Diagnostics\nModel quality and convergence were assessed using standard Bayesian diagnostics:\n\nPosterior sampling: Conducted via Markov Chain Monte Carlo (MCMC) using the No-U-Turn Sampler (NUTS), a variant of Hamiltonian Monte Carlo (HMC) (Austin et al. 2021). Four chains were run with sufficient warm-up iterations to ensure convergence.\nConvergence metrics: The potential scale reduction factor (\\(\\hat{R}\\)) and effective sample size (ESS) were used to verify stability and mixing across chains.\nAutocorrelation checks: Ensured independence between successive draws.\nPosterior predictive checks (PPCs): Compared simulated outcomes to observed data to evaluate fit.\nBayesian \\(R^2\\): Quantified the proportion of variance explained by predictors, incorporating posterior uncertainty."
  },
  {
    "objectID": "index.html#data-preparation",
    "href": "index.html#data-preparation",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk Using NHANES 2013–2014 Data",
    "section": "Data Preparation",
    "text": "Data Preparation\nThis study used publicly available 2013–2014 NHANES data published by the CDC’s National Center for Health Statistics (National Center for Health Statistics (NCHS) 2014). Three component files were utilized: DEMO_H (demographics), BMX_H (body measures), and DIQ_H (diabetes questionnaire). Each file was imported in .XPT format using the haven package in R, and merged using the unique participant identifier SEQN to create a single adult analytic dataset (age ≥ 20 years).\nAll variables were coerced to consistent numeric or factor types prior to merging to ensure atomic columns suitable for survey-weighted analysis and modeling. The use of SEQN preserved respondent integrity across datasets and ensured accurate record linkage. This preprocessing step standardized variable formats and minimized inconsistencies between files.\nData wrangling, cleaning, and merging were performed in R using a combination of base functions and tidyverse packages. Bayesian logistic regression modeling was later implemented using the brms interface to Stan, allowing probabilistic inference within a reproducible workflow that accommodated the NHANES complex survey design and missing data considerations.\n\nData Import and Merging\n\n\nCode\nmerged_data &lt;- readRDS(\"data/merged_2013_2014.rds\")\n\nmerged_n &lt;- nrow(merged_data)\n\n\nThe merged dataset contains 10,175 participants. It integrates demographic, examination, and diabetes questionnaire data. We then restrict the sample to adults (age ≥ 20) to define the analytic cohort used in subsequent analyses. A small proportion of records have missing values in BMI and diabetes status, which will be addressed later through multiple imputation.\n\n\n\nPreview of merged NHANES 2013–2014 dataset limited to analysis variables (source columns only).\n\n\nRIDAGEYR\nBMXBMI\nRIAGENDR\nRIDRETH1\nDIQ010\n\n\n\n\n69\n26.7\n1\n4\n1\n\n\n54\n28.6\n1\n3\n1\n\n\n72\n28.9\n1\n3\n1\n\n\n9\n17.1\n1\n3\n2\n\n\n73\n19.7\n2\n3\n2\n\n\n56\n41.7\n1\n1\n2\n\n\n0\nNA\n1\n3\nNA\n\n\n61\n35.7\n2\n3\n2\n\n\n42\nNA\n1\n2\n2\n\n\n56\n26.5\n2\n3\n2\n\n\n\n\n\n\n\nVariable Definitions\n\nResponse Variable:\ndiabetes_ind (binary) represents a Type 2 diabetes diagnosis, excluding gestational diabetes. It was derived from DIQ010 (“Doctor told you have diabetes”), while DIQ050 (insulin use) was excluded to prevent treatment-related confounding.\nPredictor Variables:\n\nBMXBMI – Body Mass Index (categorical; six levels as bmi_cat)\n\nRIDAGEYR – Age (continuous, 20–80 years)\n\nRIAGENDR – Sex (factor, two levels)\n\nRIDRETH1 – Ethnicity (factor, five levels)\n\n\n\n\nCode\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(dplyr)\nlibrary(tibble)\n\n# -----------------------------\n# Variable descriptions (with `code` formatting for names)\n# -----------------------------\nvar_tbl &lt;- tribble(\n  ~Variable,      ~Description,                                                                                                   ~Type,         ~Origin,\n  \"`diabetes_ind`\",\"Type 2 diabetes diagnosis (1 = Yes, 0 = No) derived from `DIQ010`; gestational diabetes excluded.\",           \"Categorical\", \"Derived from `DIQ010`\",\n  \"`age`\",        \"Age in years.\",                                                                                                \"Continuous\",  \"NHANES `RIDAGEYR`\",\n  \"`bmi`\",        \"Body Mass Index (kg/m^2) computed from measured height and weight.\",                                           \"Continuous\",  \"NHANES `BMXBMI`\",\n  \"`bmi_cat`\",    \"BMI categories: Underweight, Normal, Overweight, Obesity I–III (`Normal` is reference in models).\",            \"Categorical\", \"Derived from `bmi`\",\n  \"`sex`\",        \"Sex of participant (`Male`, `Female`).\",                                                                       \"Categorical\", \"NHANES `RIAGENDR`\",\n  \"`race`\",       \"Race/Ethnicity with five levels (e.g., Non-Hispanic White, Non-Hispanic Black, Mexican American).\",            \"Categorical\", \"NHANES `RIDRETH1`\",\n  \"`WTMEC2YR`\",   \"Examination sample weight for Mobile Examination Center participants.\",                                        \"Weight\",      \"NHANES design\",\n  \"`SDMVPSU`\",    \"Primary Sampling Unit used for variance estimation in the complex survey design.\",                             \"Design\",      \"NHANES design\",\n  \"`SDMVSTRA`\",   \"Stratum identifier used to define strata for the complex survey design.\",                                      \"Design\",      \"NHANES design\",\n  \"`age_c`\",      \"Centered and standardized age (z-score).\",                                                                     \"Continuous\",  \"Derived from `age`\",\n  \"`bmi_c`\",      \"Centered and standardized BMI (z-score).\",                                                                     \"Continuous\",  \"Derived from `bmi`\"\n)\n\nkbl(\n  var_tbl,\n  caption = \"Variable Descriptions: Adult Analytic Dataset\",\n  align = c(\"l\",\"l\",\"l\",\"l\"),\n  escape = FALSE\n) %&gt;%\n  kable_styling(full_width = FALSE, position = \"center\", bootstrap_options = c(\"striped\",\"hover\")) %&gt;%\n  group_rows(\"Analysis variables\", 1, 6) %&gt;%              # &lt;-- updated range (now 6 analysis rows)\n  group_rows(\"Survey design variables\", 7, 9) %&gt;%\n  group_rows(\"Derived variables\", 10, 11)\n\n\n\nVariable Descriptions: Adult Analytic Dataset\n\n\nVariable\nDescription\nType\nOrigin\n\n\n\n\nAnalysis variables\n\n\n`diabetes_ind`\nType 2 diabetes diagnosis (1 = Yes, 0 = No) derived from `DIQ010`; gestational diabetes excluded.\nCategorical\nDerived from `DIQ010`\n\n\n`age`\nAge in years.\nContinuous\nNHANES `RIDAGEYR`\n\n\n`bmi`\nBody Mass Index (kg/m^2) computed from measured height and weight.\nContinuous\nNHANES `BMXBMI`\n\n\n`bmi_cat`\nBMI categories: Underweight, Normal, Overweight, Obesity I–III (`Normal` is reference in models).\nCategorical\nDerived from `bmi`\n\n\n`sex`\nSex of participant (`Male`, `Female`).\nCategorical\nNHANES `RIAGENDR`\n\n\n`race`\nRace/Ethnicity with five levels (e.g., Non-Hispanic White, Non-Hispanic Black, Mexican American).\nCategorical\nNHANES `RIDRETH1`\n\n\nSurvey design variables\n\n\n`WTMEC2YR`\nExamination sample weight for Mobile Examination Center participants.\nWeight\nNHANES design\n\n\n`SDMVPSU`\nPrimary Sampling Unit used for variance estimation in the complex survey design.\nDesign\nNHANES design\n\n\n`SDMVSTRA`\nStratum identifier used to define strata for the complex survey design.\nDesign\nNHANES design\n\n\nDerived variables\n\n\n`age_c`\nCentered and standardized age (z-score).\nContinuous\nDerived from `age`\n\n\n`bmi_c`\nCentered and standardized BMI (z-score).\nContinuous\nDerived from `bmi`\n\n\n\n\n\n\n\nStudy Design and Survey-Weighted Analysis\nThe National Health and Nutrition Examination Survey (NHANES) employs a complex, multistage probability sampling design with stratification, clustering, and oversampling of specific demographic groups (for example, racial/ethnic minorities and older adults) to produce nationally representative estimates of the U.S. population.\nSurvey design variables — primary sampling units (SDMVPSU), strata (SDMVSTRA), and examination sample weights (WTMEC2YR) — were retained to account for this complex design. These variables were applied to adjust for unequal probabilities of selection, nonresponse, and oversampling, ensuring valid standard errors, unbiased prevalence estimates, and generalizable population-level inference.\nA survey-weighted logistic regression model was used to evaluate the association between diabetes status (diabetes_ind, binary outcome) and key predictors: body mass index (bmi), age (age), sex (sex), and race/ethnicity (race). Diabetes was defined using DIQ010 (“Doctor told you have diabetes”) and coded as 0/1, with DIQ050 (insulin use) excluded to avoid treatment-related confounding.\nCovariates included:\n- age (continuous; centered as age_c, categorized 20–80 years)\n- bmi (continuous; centered as bmi_c, and categorized by BMI class bmi_cat)\n- sex (male, female)\n- race (five ethnicity levels)\nThis approach accounts for NHANES’ complex sampling design, producing unbiased parameter estimates and valid inference for U.S. adults.\n\n\n\n\n\n\n\nStep\nDescription\n\n\n\n\nWeighting\nUsed the survey package to calculate weighted means and standard deviations for all variables.\n\n\nStandardization\nCentered and standardized BMI and age (bmi_c, age_c) for use in regression models.\n\n\nAge Categorization\nRecoded into intervals: 20–&lt;30, 30–&lt;40, 40–&lt;50, 50–&lt;60, 60–&lt;70, and 70–80 years.\n\n\nBMI Categorization\nRecoded as: &lt;18.5 (Underweight), 18.5–&lt;25 (Normal), 25–&lt;30 (Overweight), 30–&lt;35 (Obesity I), 35–&lt;40 (Obesity II), ≥40 (Obesity III).\n\n\nEthnicity Recoding\nRecoded as: 1 = Mexican American, 2 = Other Hispanic, 3 = Non-Hispanic White, 4 = Non-Hispanic Black, 5 = Other/Multi.\n\n\nSpecial Codes\nTransformed nonresponse codes (e.g., 3, 7) to NA. These missing codes were evaluated for potential nonrandom patterns (MAR/MNAR).\n\n\nMissing Data\nRetained and visualized missing values to assess their pattern and informativeness before multiple imputation.\n\n\nFinal Dataset\nCreated the cleaned analytic dataset (adult) using Non-Hispanic White and Male as reference groups for modeling.\n\n\n\n\n\nAdult Cohort Definition\n\n\nCode\nlibrary(survey)\n\n# NHANES survey design object for the adult analytic cohort\n\nnhanes_design_adult &lt;- survey::svydesign(\nid      = ~SDMVPSU,\nstrata  = ~SDMVSTRA,\nweights = ~WTMEC2YR,\nnest    = TRUE,\ndata    = adult\n)\n\n# Quick weighted checks\n\nsurvey::svymean(~age, nhanes_design_adult, na.rm = TRUE)\n\n\n      mean     SE\nage 47.496 0.3805\n\n\nCode\nsurvey::svymean(~diabetes_ind, nhanes_design_adult, na.rm = TRUE)\n\n\n                mean     SE\ndiabetes_ind 0.10391 0.0052\n\n\nCode\n# Design effect and effective sample size for `diabetes_ind`\n\nv_hat &lt;- as.numeric(survey::svyvar(~diabetes_ind, nhanes_design_adult, na.rm = TRUE))\np_hat &lt;- mean(adult$diabetes_ind, na.rm = TRUE)\nn_obs &lt;- nrow(adult)\nv_srs &lt;- p_hat * (1 - p_hat) / n_obs\ndeff  &lt;- v_hat / v_srs\n\nn_total &lt;- sum(weights(nhanes_design_adult), na.rm = TRUE)\ness     &lt;- as.numeric(n_total / deff)\n\ncat(\"Design effect for diabetes_ind:\", round(deff, 2), \"\\n\")\n\n\nDesign effect for diabetes_ind: 4777.96 \n\n\nCode\ncat(\"Effective sample size for diabetes_ind:\", round(ess), \"\\n\")\n\n\nEffective sample size for diabetes_ind: 47960 \n\n\nDescriptive statistics for continuous and categorical variables are presented below.\n\n\nCode\n# Keep only analytic variables for Table 1\ntbl1_dat &lt;- adult %&gt;%\n  transmute(\n    age,\n    bmi,\n    bmi_cat,\n    sex,\n    race4,\n    # CHANGE: make diabetes_ind a factor so it plays nice with pivot_longer()\n    diabetes_ind = factor(diabetes_ind, levels = c(0, 1), labels = c(\"No\", \"Yes\"))\n  )\n\n# Continuous summaries: n, missing, mean, sd, min, max\ncont_vars &lt;- c(\"age\", \"bmi\")\n\ncont_sum &lt;- tbl1_dat %&gt;%\n  select(all_of(cont_vars)) %&gt;%\n  pivot_longer(everything(), names_to = \"Variable\", values_to = \"value\") %&gt;%\n  group_by(Variable) %&gt;%\n  summarise(\n    N          = sum(!is.na(value)),\n    Missing    = sum(is.na(value)),\n    Mean       = mean(value, na.rm = TRUE),\n    SD         = sd(value, na.rm = TRUE),\n    Min        = min(value, na.rm = TRUE),\n    Max        = max(value, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(across(c(Mean, SD, Min, Max), ~round(.x, 2)))\n\n# Categorical summaries: counts and percents\ncat_vars &lt;- c(\"sex\", \"race4\", \"diabetes_ind\", \"bmi_cat\")\n\ncat_sum &lt;- tbl1_dat %&gt;%\n  # CHANGE: ensure all cat vars are factors and include NA as an explicit \"(Missing)\" level\n  mutate(across(all_of(cat_vars),\n                ~ forcats::fct_explicit_na(as.factor(.x), na_level = \"(Missing)\"))) %&gt;%\n  select(all_of(cat_vars)) %&gt;%\n  pivot_longer(everything(), names_to = \"Variable\", values_to = \"Level\") %&gt;%\n  count(Variable, Level, name = \"n\") %&gt;%\n  group_by(Variable) %&gt;%\n  mutate(pct = round(100 * n / sum(n), 1)) %&gt;%\n  ungroup() %&gt;%\n  arrange(Variable, desc(n))\n\nkable(cont_sum, caption = \"Table 1a. Continuous variables (age, BMI): N, missing, mean (SD), range.\")\n\n\n\nTable 1a. Continuous variables (age, BMI): N, missing, mean (SD), range.\n\n\nVariable\nN\nMissing\nMean\nSD\nMin\nMax\n\n\n\n\nage\n5769\n0\n49.11\n17.56\n20.0\n80.0\n\n\nbmi\n5520\n249\n29.10\n7.15\n14.1\n82.9\n\n\n\n\n\nCode\nkable(cat_sum,  caption = \"Table 1b. Categorical variables (sex, race4, diabetes_ind, bmi_cat): counts and percentages.\")\n\n\n\nTable 1b. Categorical variables (sex, race4, diabetes_ind, bmi_cat): counts and percentages.\n\n\nVariable\nLevel\nn\npct\n\n\n\n\nbmi_cat\nOverweight\n1768\n30.6\n\n\nbmi_cat\nNormal\n1579\n27.4\n\n\nbmi_cat\nObesity I\n1145\n19.8\n\n\nbmi_cat\nObesity II\n519\n9.0\n\n\nbmi_cat\nObesity III\n419\n7.3\n\n\nbmi_cat\n(Missing)\n249\n4.3\n\n\nbmi_cat\nUnderweight\n90\n1.6\n\n\ndiabetes_ind\nNo\n4870\n84.4\n\n\ndiabetes_ind\nYes\n722\n12.5\n\n\ndiabetes_ind\n(Missing)\n177\n3.1\n\n\nrace4\nWhite\n2472\n42.8\n\n\nrace4\nHispanic\n1275\n22.1\n\n\nrace4\nBlack\n1177\n20.4\n\n\nrace4\nOther\n845\n14.6\n\n\nsex\nFemale\n3011\n52.2\n\n\nsex\nMale\n2758\n47.8\n\n\n\n\n\nTable 1a and 1b summarize the analytic variables included in subsequent models. Mean age and BMI values indicate an adult cohort spanning a wide range of body composition, while categorical summaries confirm balanced sex representation and sufficient sample sizes across race/ethnicity categories. These variables were standardized and used as predictors in all modeling frameworks.\n\n\nCode\nadult_n &lt;- nrow(adult)\n\n\n\n\n\n\nTable 1: Excerpt of the NHANES 2013–2014 adult cohort (age ≥ 20; N = 5,769) with derived and standardized variables.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSDMVPSU\nSDMVSTRA\nWTMEC2YR\ndiabetes_ind\nbmi\nage\nsex\nrace\nage_c\nbmi_c\nbmi_cat\nrace4\n\n\n\n\n1\n112\n13481.04\n1\n26.7\n69\nMale\nNH Black\n1.1324183\n-0.3358861\nOverweight\nBlack\n\n\n1\n108\n24471.77\n1\n28.6\n54\nMale\nNH White\n0.2783598\n-0.0702810\nOverweight\nWhite\n\n\n1\n109\n57193.29\n1\n28.9\n72\nMale\nNH White\n1.3032300\n-0.0283434\nOverweight\nWhite\n\n\n2\n116\n65541.87\n0\n19.7\n73\nFemale\nNH White\n1.3601672\n-1.3144311\nNormal\nWhite\n\n\n1\n111\n25344.99\n0\n41.7\n56\nMale\nMexican American\n0.3922343\n1.7609961\nObesity III\nHispanic\n\n\n1\n114\n61758.65\n0\n35.7\n61\nFemale\nNH White\n0.6769204\n0.9222432\nObesity II\nWhite\n\n\n\n\n\n\n\n\nAs shown in Table 1, the analytic adult cohort (N = 5,769) includes standardized variables for age and BMI (age_c, bmi_c), categorical indicators for sex and race/ethnicity (race4), and a binary doctor-diagnosed diabetes variable (diabetes_ind).\n\n\nCode\n# Textual structure and preview\nstr(adult)\n\n\n'data.frame':   5769 obs. of  12 variables:\n $ SDMVPSU     : num  1 1 1 2 1 1 2 1 2 2 ...\n $ SDMVSTRA    : num  112 108 109 116 111 114 106 112 112 113 ...\n $ WTMEC2YR    : num  13481 24472 57193 65542 25345 ...\n $ diabetes_ind: num  1 1 1 0 0 0 0 0 0 0 ...\n $ bmi         : num  26.7 28.6 28.9 19.7 41.7 35.7 NA 26.5 22 20.3 ...\n $ age         : num  69 54 72 73 56 61 42 56 65 26 ...\n $ sex         : Factor w/ 2 levels \"Female\",\"Male\": 2 2 2 1 2 1 2 1 2 1 ...\n $ race        : Factor w/ 5 levels \"Mexican American\",..: 2 3 3 3 1 3 4 3 3 3 ...\n $ age_c       : num  1.132 0.278 1.303 1.36 0.392 ...\n $ bmi_c       : num  -0.3359 -0.0703 -0.0283 -1.3144 1.761 ...\n $ bmi_cat     : Factor w/ 7 levels \"Underweight\",..: 3 3 3 2 6 5 7 3 2 2 ...\n $ race4       : Factor w/ 4 levels \"White\",\"Hispanic\",..: 3 1 1 1 2 1 2 1 1 1 ...\n\n\nCode\n# Visual structure and type overview\nplot_intro(adult, title = \"Adult dataset: variable types and completeness\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(DataExplorer)\n\n# Visualize missing data pattern for key variables\n\nplot_missing(adult, title = \"Missing data pattern (Adult dataset)\")\n\n\n\n\n\nMissingness overview for the adult dataset.\n\n\n\n\n\n\nCode\nmiss_tbl &lt;- tibble::tibble(\nVariable = c(\"`bmi`\",\"`diabetes_ind`\"),\nMissing_n = c(sum(is.na(adult_eda$bmi)),\nsum(is.na(adult_eda$diabetes_ind))),\nMissing_pct = round(c(mean(is.na(adult_eda$bmi)),\nmean(is.na(adult_eda$diabetes_ind))) * 100, 1)\n)\n\nknitr::kable(\nmiss_tbl,\ncaption = \"Missingness for key analysis variables.\"\n)\n\n\n\nMissingness for key analysis variables.\n\n\nVariable\nMissing_n\nMissing_pct\n\n\n\n\nbmi\n249\n4.3\n\n\ndiabetes_ind\n177\n3.1\n\n\n\n\n\n\n\nMissing Data Summary\nMissingness was low overall (~7.3%). Gaps were concentrated in bmi and diabetes_ind; design variables and demographics were complete. Patterns were consistent with MAR.\n\n\nCode\n# Summarize missingness by variable\n\ncolSums(is.na(adult_eda))\n\n\n     SDMVPSU     SDMVSTRA     WTMEC2YR diabetes_ind          bmi          age \n           0            0            0          177          249            0 \n         sex         race        age_c        bmi_c      bmi_cat        race4 \n           0            0            0          249            0            0 \n\n\nCode\n# Focus on expected variables with NAs\n\nsapply(adult_eda[, c(\"diabetes_ind\", \"bmi\", \"bmi_c\", \"age\")], function(x) sum(is.na(x)))\n\n\ndiabetes_ind          bmi        bmi_c          age \n         177          249          249            0 \n\n\nCode\n# Check for missing or invalid source codes\n\ntable(merged_data$DIQ010, useNA = \"ifany\")\n\n\n\n   1    2    3    7    9 &lt;NA&gt; \n 737 8841  185    1    5  406 \n\n\nCode\nsum(is.na(merged_data$BMXBMI))\n\n\n[1] 1120\n\n\n\n\nCode\n# (Note: full missingness pattern already visualized above using plot_missing(adult).)\n\n\nResults indicated that missing values were limited primarily to BMI and diabetes indicators, while demographic and survey design variables were complete.\nThe overall missingness (~4%) was low and appears consistent with data missing at random (MAR). This pattern likely reflects participation differences in the physical examination component among older adults or individuals with health limitations.\n\n\nExploratory Data Summary\nFollowing the missing data assessment, exploratory analyses were conducted to describe the adult analytic cohort and visualize distributions across key demographic and health variables. The goal was to examine univariate patterns and bivariate relationships relevant to diabetes prevalence before modeling.\nThe adult analytic cohort was broadly representative of the U.S. population, with a majority identifying as Non-Hispanic White. Age and BMI distributions were right-skewed, with most participants classified as overweight or obese. Visual exploration revealed a clear positive relationship between age, BMI, and diabetes prevalence. Non-Hispanic Black and Hispanic participants exhibited higher proportions of diabetes compared to Non-Hispanic Whites.\nApproximately 25% of variables were categorical (for example, sex, race4, diabetes_ind) and 75% were continuous (age, bmi, age_c, bmi_c), indicating that the dataset primarily consisted of measured numeric values such as BMI and age. About 93% of rows contained complete information across all predictors and outcomes, reflecting high data quality.\nAge was relatively evenly distributed across adult age groups, while BMI was concentrated in the overweight and obese ranges. Female participants were slightly overrepresented relative to Male participants.\n\n\nCode\nstopifnot(all(c(\"diabetes_ind\",\"age\",\"bmi\") %in% names(adult_eda)))\n\nadult_eda_num &lt;- adult_eda |&gt;\n  dplyr::transmute(\n    diabetes_ind = as.numeric(diabetes_ind),\n    age  = as.numeric(age),\n    bmi  = as.numeric(bmi)\n  )\n\ncmat &lt;- cor(adult_eda_num, use = \"pairwise.complete.obs\", method = \"pearson\")\ncor_melt &lt;- reshape2::melt(cmat, varnames = c(\"Var1\",\"Var2\"), value.name = \"value\")\n\nggplot(cor_melt, aes(Var1, Var2, fill = value)) +\n  geom_tile(color = \"white\") +\n  scale_fill_gradient2(low = \"blue\", mid = \"white\", high = \"red\",\n                       midpoint = 0, limits = c(-1, 1), name = \"Correlation\") +\n  theme_minimal() +\n  labs(title = \"Correlation Heatmap (EDA: pairwise-complete)\", x = NULL, y = NULL)\n\n\n\n\n\nCorrelation heatmap (EDA, pairwise-complete) using adult_eda.\n\n\n\n\n\n\nCode\n# Age distribution (analytic adult)\nggplot(adult, aes(x = age)) +\n  geom_histogram(binwidth = 5, color = \"white\") +\n  labs(title = \"Distribution of Age (≥20 years)\", x = \"Age (years)\", y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# Diabetes outcome distribution\nggplot(adult, aes(x = factor(diabetes_ind, levels = c(0,1), labels = c(\"No\",\"Yes\")))) +\n  geom_bar() +\n  labs(title = \"Diabetes Outcome Distribution (≥20 years)\", x = \"Diabetes (No/Yes)\", y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# BMI category distribution\nggplot(adult, aes(x = bmi_cat)) +\n  geom_bar(color = \"white\", fill = \"skyblue\") +\n  labs(title = \"Distribution of BMI Categories (≥20 years)\", x = \"BMI Category\", y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# BMI by diabetes outcome (boxplot)\n# (You can’t use boxplot with categorical y, so revert to numeric BMI here)\nggplot(adult, aes(x = factor(diabetes_ind, levels = c(0,1), labels = c(\"No\",\"Yes\")), y = bmi)) +\n  geom_boxplot(fill = \"lightblue\") +\n  labs(title = \"BMI by Diabetes Diagnosis (≥20 years)\", x = \"Diabetes (No/Yes)\", y = \"BMI (numeric)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# Diabetes by race4 (dodged bars)\nggplot(adult, aes(x = race4, fill = factor(diabetes_ind, levels = c(0,1), labels = c(\"No\",\"Yes\")))) +\n  geom_bar(position = \"dodge\") +\n  labs(title = \"Diabetes Diagnosis by Race/Ethnicity (≥20 years)\",\n       x = \"Race/Ethnicity (race4)\", y = \"Count\", fill = \"Diabetes\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\nThe EDA missingness summary shows approximately 4.3% missing BMI and 3.1% missing diabetes status (diabetes_ind). All design variables (WTMEC2YR, SDMVPSU, SDMVSTRA), as well as age, sex, and race4, are complete—sex and race NAs are encoded as explicit “(Missing)” levels in the EDA view."
  },
  {
    "objectID": "index.html#modeling-frameworks",
    "href": "index.html#modeling-frameworks",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk Using NHANES 2013–2014 Data",
    "section": "Modeling Frameworks",
    "text": "Modeling Frameworks\nThree modeling frameworks were compared using identical predictors (standardized age, BMI, sex, and race4) and the binary outcome diabetes_ind: (1) survey-weighted logistic regression to incorporate the NHANES complex sampling design, (2) multiple imputation (MICE) to address missing BMI values, and (3) Bayesian logistic regression with weakly informative priors to quantify uncertainty.\n\nSurvey-Weighted Logistic Regression (Design-Based MLE)\n\n\nCode\nadult_clean &lt;- adult %&gt;%\n  dplyr::mutate(\n    sex   = forcats::fct_drop(sex),\n    race4 = forcats::fct_drop(race4),\n    age_c = as.numeric(age_c),\n    bmi_c = as.numeric(bmi_c)\n  ) %&gt;%\n  dplyr::filter(\n    !is.na(diabetes_ind),\n    !is.na(age_c),\n    !is.na(bmi_c),\n    !is.na(sex),\n    !is.na(race4)\n  )\n\n# Structure and sanity checks\nstr(adult_clean[, c(\"diabetes_ind\",\"sex\",\"race4\",\"age_c\",\"bmi_c\")])\n\n\n'data.frame':   5349 obs. of  5 variables:\n $ diabetes_ind: num  1 1 1 0 0 0 0 0 0 1 ...\n $ sex         : Factor w/ 2 levels \"Female\",\"Male\": 2 2 2 1 2 1 1 2 1 2 ...\n $ race4       : Factor w/ 4 levels \"White\",\"Hispanic\",..: 3 1 1 1 2 1 1 1 1 1 ...\n $ age_c       : num  1.132 0.278 1.303 1.36 0.392 ...\n $ bmi_c       : num  -0.3359 -0.0703 -0.0283 -1.3144 1.761 ...\n\n\nCode\ntable(adult_clean$sex)\n\n\n\nFemale   Male \n  2798   2551 \n\n\nCode\ntable(adult_clean$race4)\n\n\n\n   White Hispanic    Black    Other \n    2293     1183     1101      772 \n\n\nCode\ntable(adult_clean$diabetes_ind)\n\n\n\n   0    1 \n4657  692 \n\n\n\n\nCode\noptions(survey.lonely.psu = \"adjust\")\n\nnhanes_design_adult &lt;- survey::svydesign(\n  id      = ~SDMVPSU,\n  strata  = ~SDMVSTRA,\n  weights = ~WTMEC2YR,\n  nest    = TRUE,\n  data    = adult_clean\n)\n\nsvy_fit &lt;- survey::svyglm(\n  diabetes_ind ~ age_c + bmi_c + sex + race4,\n  design = nhanes_design_adult,\n  family = quasibinomial()\n)\n\nsvy_or &lt;- broom::tidy(svy_fit, conf.int = TRUE) %&gt;%\n  dplyr::mutate(\n    OR  = exp(estimate),\n    LCL = exp(conf.low),\n    UCL = exp(conf.high)\n  ) %&gt;%\n  dplyr::select(term, OR, LCL, UCL, p.value) %&gt;%\n  dplyr::filter(term != \"(Intercept)\")\n\n# View results table\nsvy_or\n\n\n# A tibble: 6 × 5\n  term             OR   LCL   UCL  p.value\n  &lt;chr&gt;         &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 age_c          2.98  2.70  3.28 9.95e-10\n2 bmi_c          1.93  1.68  2.22 2.07e- 6\n3 sexMale        1.29  1.02  1.63 3.73e- 2\n4 race4Hispanic  1.81  1.43  2.29 3.02e- 4\n5 race4Black     1.60  1.16  2.21 9.48e- 3\n6 race4Other     2.20  1.46  3.30 1.80e- 3\n\n\nDesign-based odds ratios are summarized in Table 2.\n\n\nCode\nknitr::kable(svy_or)\n\n\n\n\nTable 2: Survey-weighted logistic regression: odds ratios (OR) and 95% confidence intervals for diabetes diagnosis among adults (NHANES 2013–2014).\n\n\n\n\n\n\nterm\nOR\nLCL\nUCL\np.value\n\n\n\n\nage_c\n2.977668\n2.704677\n3.278212\n0.0000000\n\n\nbmi_c\n1.930284\n1.679190\n2.218924\n0.0000021\n\n\nsexMale\n1.287236\n1.018664\n1.626618\n0.0373081\n\n\nrace4Hispanic\n1.809943\n1.428957\n2.292507\n0.0003024\n\n\nrace4Black\n1.599844\n1.157393\n2.211434\n0.0094754\n\n\nrace4Other\n2.195356\n1.461219\n3.298334\n0.0017976\n\n\n\n\n\n\n\n\n\nResults from Survey-Weighted Logistic Regression\n\n\n\n\n\n\n\n\n\nPredictor\nOdds Ratio (OR)\np-value\nInterpretation\n\n\n\n\nage_c\n2.98\n&lt;0.001\nEach 1 SD increase in age nearly triples the odds of diabetes.\n\n\nbmi_c\n1.93\n&lt;0.001\nEach 1 SD increase in BMI raises the odds of diabetes by about 93%.\n\n\nsexMale\n1.29\n0.037\nMales have about 29% higher odds of diabetes compared with females.\n\n\nrace4Hispanic\n1.81\n&lt;0.001\nHispanic adults have ~81% higher odds of diabetes compared with Non-Hispanic Whites.\n\n\nrace4Black\n1.60\n0.009\nNon-Hispanic Black adults have ~60% higher odds of diabetes compared with Non-Hispanic Whites.\n\n\nrace4Other\n2.20\n0.002\nAdults identifying as Other/Multi-racial have more than 2× higher odds of diabetes compared with Non-Hispanic Whites.\n\n\n\nInterpretation:\nAge and BMI are the strongest predictors of diabetes in the NHANES 2013–2014 adult sample, with each 1 SD increase in age nearly tripling diabetes risk and higher BMI contributing substantially to elevated odds. Males show significantly higher odds of diabetes compared to females, consistent with known sex differences in metabolic risk. Racial and ethnic disparities are evident, with Hispanic, Black, and Other/Multi-racial adults all exhibiting significantly higher odds of diabetes than Non-Hispanic Whites. All predictors are statistically significant (p &lt; 0.05), indicating robust associations across demographic and health characteristics.\nThe NHANES 2013–2014 data use a complex, multistage probability design involving strata (SDMVSTRA), primary sampling units (PSUs; SDMVPSU), and examination weights (WTMEC2YR) to ensure nationally representative estimates (National Center for Health Statistics (NCHS) 2014).\nEstimates are population-weighted using NHANES survey design variables (WTMEC2YR, SDMVSTRA, SDMVPSU). Odds ratios are reported per one standard deviation (1 SD) increase in age and BMI, with reference groups Male and White.\n\n\n\nMultivariate Imputation by Chained Equations (Pooled Logistic Regression)\nMultiple Imputation by Chained Equations (MICE) was used as a principled approach for handling missing data (Stef van Buuren and Groothuis-Oudshoorn 2011; S. van Buuren 2012).\nMICE iteratively imputes each incomplete variable using regression models based on other variables in the dataset, producing multiple completed datasets that reflect uncertainty due to missingness. Estimates are then pooled across imputations using Rubin’s rules to generate final parameter estimates and confidence intervals.\nMICE, as an alternative to the Bayesian approach, effectively manages missing data through chained regression equations without requiring full joint modeling of all variables.\nFor large sample sizes (n ≥ 400), even in the presence of high percentages (up to 75%) of missing data in one variable, non-normal distributions such as flat densities, heavy tails, skewness, and multimodality do not materially affect mean structure estimation performance (S. van Buuren 2012).\nIn this study, continuous variables (age and BMI) were imputed using predictive mean matching (PMM) to preserve realistic distributions, while categorical variables (sex and race4) were imputed using logistic and polytomous regression models, respectively. Diabetes status (diabetes_ind) was treated as an outcome variable and was not imputed. Twenty imputations were generated to reduce Monte Carlo error and maintain robust variance estimation.\nConvergence and Data Stability\nThe chained equation process showed stable convergence across iterations, indicating reliable estimation of missing BMI and age values. After MICE, the final imputed dataset consisted of n = 5,592 adults with all key predictor variables completed.\n\n\nCode\nadult_imp1 &lt;- mice::complete(imp, 1) %&gt;%\ndplyr::mutate(\nsex     = factor(sex,   levels = levels(adult$sex)),\nrace4   = factor(race4, levels = levels(adult$race4)),\nage_c   = as.numeric(scale(age)),\nbmi_c   = as.numeric(scale(bmi)),\nwt_norm = WTMEC2YR / mean(WTMEC2YR, na.rm = TRUE)\n) %&gt;%\ndplyr::filter(!is.na(diabetes_ind), !is.na(age_c), !is.na(bmi_c)) %&gt;%\ndroplevels()\n\n# Glimpse imputed dataset\n\nglimpse(adult_imp1)\n\n\nRows: 5,592\nColumns: 11\n$ diabetes_ind &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ age          &lt;dbl&gt; 69, 54, 72, 73, 56, 61, 42, 56, 65, 26, 76, 33, 32, 38, 5…\n$ bmi          &lt;dbl&gt; 26.7, 28.6, 28.9, 19.7, 41.7, 35.7, 29.0, 26.5, 22.0, 20.…\n$ sex          &lt;fct&gt; Male, Male, Male, Female, Male, Female, Male, Female, Mal…\n$ race4        &lt;fct&gt; Black, White, White, White, Hispanic, White, Hispanic, Wh…\n$ WTMEC2YR     &lt;dbl&gt; 13481.04, 24471.77, 57193.29, 65541.87, 25344.99, 61758.6…\n$ SDMVPSU      &lt;dbl&gt; 1, 1, 1, 2, 1, 1, 2, 1, 2, 2, 1, 2, 2, 2, 2, 1, 1, 1, 2, …\n$ SDMVSTRA     &lt;dbl&gt; 112, 108, 109, 116, 111, 114, 106, 112, 112, 113, 116, 11…\n$ age_c        &lt;dbl&gt; 1.13241831, 0.27835981, 1.30323001, 1.36016725, 0.3922342…\n$ bmi_c        &lt;dbl&gt; -0.331790880, -0.065542632, -0.023503435, -1.312705481, 1…\n$ wt_norm      &lt;dbl&gt; 0.3393916, 0.6160884, 1.4398681, 1.6500477, 0.6380722, 1.…\n\n\nCode\n# Descriptive statistics for imputed dataset\n\nlibrary(tableone)\nvars &lt;- c(\"age\", \"bmi\", \"sex\", \"race4\", \"diabetes_ind\")\ntable1 &lt;- CreateTableOne(vars = vars, data = adult_imp1, factorVars = c(\"sex\", \"race4\", \"diabetes_ind\"))\nprint(table1, showAllLevels = TRUE)\n\n\n                  \n                   level    Overall      \n  n                          5592        \n  age (mean (SD))           48.84 (17.57)\n  bmi (mean (SD))           28.99 (7.09) \n  sex (%)          Female    2923 (52.3) \n                   Male      2669 (47.7) \n  race4 (%)        White     2398 (42.9) \n                   Hispanic  1231 (22.0) \n                   Black     1141 (20.4) \n                   Other      822 (14.7) \n  diabetes_ind (%) 0         4870 (87.1) \n                   1          722 (12.9) \n\n\nDescriptive Results (Imputed Dataset)\nAfter imputation, the analytic dataset contained approximately 5,500–5,600 adults. The mean age was around 49 years (SD ≈ 17), and the mean BMI was approximately 29 (SD ≈ 7). Females represented roughly 52% of participants, and the majority identified as Non-Hispanic White (~43%). Diabetes prevalence was about 11%, consistent with population-level NHANES estimates.\n\n\nCode\nlibrary(ggplot2)\nlibrary(reshape2)\n\n# Correlation heatmap\n\ncorrelation_matrix &lt;- cor(adult_imp1[, c(\"diabetes_ind\", \"age\", \"bmi\")], use = \"complete.obs\", method = \"pearson\")\ncorrelation_melted &lt;- melt(correlation_matrix)\n\nggplot(correlation_melted, aes(Var1, Var2, fill = value)) +\ngeom_tile(color = \"white\") +\nscale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\", midpoint = 0, name = \"Correlation\") +\ntheme_minimal() +\ntheme(axis.text.x = element_text(angle = 45, hjust = 1)) +\nlabs(title = \"Correlation Heatmap: Diabetes, Age, and BMI\")\n\n\n\n\n\n\n\n\n\nCode\n# Diabetes diagnosis distribution\n\nggplot(adult_imp1, aes(x = factor(diabetes_ind))) +\ngeom_bar(fill = \"steelblue\") +\nlabs(title = \"Diabetes Diagnosis Distribution\", x = \"Diabetes (0 = No, 1 = Yes)\", y = \"Count\") +\ntheme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# BMI distribution by diabetes status\n\nggplot(adult_imp1, aes(x = factor(diabetes_ind), y = bmi, fill = factor(diabetes_ind))) +\ngeom_boxplot(alpha = 0.7) +\nscale_x_discrete(labels = c(\"0\" = \"No Diabetes\", \"1\" = \"Diabetes\")) +\nlabs(x = \"Diabetes Diagnosis\", y = \"BMI\", title = \"BMI Distribution by Diabetes Status\") +\ntheme_minimal() +\ntheme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nCode\n# Predicted probability of diabetes vs BMI\n\nggplot(adult_imp1, aes(x = bmi, y = diabetes_ind)) +\ngeom_point(alpha = 0.2, position = position_jitter(height = 0.02)) +\ngeom_smooth(method = \"glm\", method.args = list(family = \"binomial\"), se = TRUE, color = \"blue\") +\nlabs(x = \"BMI\", y = \"Probability of Diabetes\", title = \"Predicted Probability of Diabetes vs BMI\") +\ntheme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nCode\nmiss_age  &lt;- sum(is.na(mi_dat$age))\nmiss_bmiN &lt;- sum(is.na(mi_dat$bmi))\n\nmi_caption &lt;- if (miss_age &gt; 0 && miss_bmiN &gt; 0) {\n\"Multiple Imputation (MICE): pooled odds ratios (OR) and 95% confidence intervals after imputing missing age (normal) and BMI (PMM) (m = 20); diabetes status was not imputed.\"\n} else if (miss_bmiN &gt; 0) {\n\"Multiple Imputation (MICE): pooled odds ratios (OR) and 95% confidence intervals after imputing missing BMI (PMM) (m = 20); diabetes status was not imputed.\"\n} else if (miss_age &gt; 0) {\n\"Multiple Imputation (MICE): pooled odds ratios (OR) and 95% confidence intervals after imputing missing age (normal) (m = 20); diabetes status was not imputed.\"\n} else {\n\"Multiple Imputation (MICE): pooled odds ratios (OR) and 95% confidence intervals (no variables required imputation); diabetes status was not imputed.\"\n}\nmi_caption &lt;- paste0(mi_caption, \" Odds ratios are per 1 SD for age and BMI.\")\n\nknitr::kable(mi_or, caption = mi_caption)\n\n\n\n\nTable 3: Multiple Imputation (MICE): pooled odds ratios (OR) and 95% confidence intervals after imputing missing BMI (PMM) (m = 20); diabetes status was not imputed. Odds ratios are per 1 SD for age and BMI.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nOR\nstd.error\nstatistic\ndf\np.value\nLCL\nUCL\nconf.low\nconf.high\n\n\n\n\n2\nscale(age)\n2.8956646\n0.0524433\n20.273592\n5499.574\n0.0000000\n2.6127544\n3.2092084\n2.6127544\n3.2092084\n\n\n3\nscale(bmi)\n1.8053391\n0.0430294\n13.728961\n3877.226\n0.0000000\n1.6592839\n1.9642506\n1.6592839\n1.9642506\n\n\n4\nrelevel(sex, “Male”)Female\n0.8056102\n0.0872427\n-2.477631\n5545.705\n0.0132553\n0.6789653\n0.9558776\n0.6789653\n0.9558776\n\n\n5\nrelevel(race4, “White”)Hispanic\n2.0741944\n0.1128115\n6.467183\n5562.036\n0.0000000\n1.6626591\n2.5875915\n1.6626591\n2.5875915\n\n\n6\nrelevel(race4, “White”)Black\n1.7931172\n0.1137153\n5.135240\n5508.172\n0.0000003\n1.4348045\n2.2409110\n1.4348045\n2.2409110\n\n\n7\nrelevel(race4, “White”)Other\n2.0011166\n0.1443011\n4.807344\n5464.535\n0.0000016\n1.5080503\n2.6553940\n1.5080503\n2.6553940\n\n\n\n\n\n\n\n\nInterpretation\n\nAge and BMI are strong positive predictors of diabetes; each 1 SD increase substantially raises the odds.\nSex: Females show significantly lower odds of diabetes compared to males.\nRace/Ethnicity: All non-White racial/ethnic groups have higher odds of diabetes compared to Non-Hispanic Whites, underscoring persistent disparities in diabetes risk.\nModel significance: All predictors are statistically significant (p &lt; 0.05).\nResults are consistent with those from the survey-weighted model, confirming robustness to imputation.\n\n\n\nBayesian Logistic Regression\nBayesian logistic regression was used to quantify parameter uncertainty and compare posterior estimates with the survey-weighted and MICE models. Weakly informative priors were used to regularize estimates while preserving flexibility in inference.\nModel Specifications: - Family: Bernoulli with logit link\n- Data: adult_imp1 (N = 5,592)\n- Chains: 4 (2,000 iterations each; 1,000 warmup)\n- Adaptation delta: 0.95\n- Weights: normalized NHANES exam weights (wt_norm, mean ≈ 1.00, SD ≈ 0.79)\n- Predictors: standardized age, BMI, sex, and race\n\nDefine Model and Priors\n\n\nCode\nfml_bayes &lt;- diabetes_ind | weights(wt_norm) ~ age_c + bmi_c + sex + race4\n\npriors &lt;- c(\n  brms::set_prior(\"normal(0, 2.5)\", class = \"b\"),\n  brms::set_prior(\"student_t(3, 0, 10)\", class = \"Intercept\")\n)\n\n\n\n\nCode\nadult_long &lt;- adult_imp1 %&gt;%\ndplyr::select(bmi_c, age_c) %&gt;%\ntidyr::pivot_longer(\ncols = dplyr::everything(),\nnames_to = \"Coefficient\",\nvalues_to = \"Value\"\n)\n\nggplot2::ggplot(adult_long, ggplot2::aes(x = Value, fill = Coefficient)) +\nggplot2::geom_density(alpha = 0.5) +\nggplot2::theme_minimal() +\nggplot2::labs(\ntitle = \"Distributions for Standardized Age and BMI (adult_imp1)\",\nx = \"Standardized value (z-score)\",\ny = \"Density\",\nfill = \"Coefficient\"\n)\n\n\n\n\n\nDistributions for standardized BMI and Age (adult_imp1).\n\n\n\n\n\n\nCode\nprior_draws &lt;- tibble::tibble(\nterm = rep(c(\"Age (per 1 SD)\", \"BMI (per 1 SD)\"), each = 4000),\nvalue = c(\nstats::rnorm(4000, mean = 0, sd = 2.5),\nstats::rnorm(4000, mean = 0, sd = 2.5)\n)\n)\n\nggplot2::ggplot(prior_draws, ggplot2::aes(x = value, fill = term)) +\nggplot2::geom_density(alpha = 0.5) +\nggplot2::theme_minimal() +\nggplot2::labs(\ntitle = \"Prior Distributions for Age and BMI Coefficients\",\nx = \"Coefficient value\",\ny = \"Density\",\nfill = NULL\n)\n\n\n\n\n\nPrior distributions for Age and BMI coefficients (Normal(0, 2.5)).\n\n\n\n\n\n\nFit the Model\n\n\nCode\ninvisible(capture.output({\nbayes_fit &lt;- brms::brm(\nformula = fml_bayes,\ndata    = adult_imp1,\nfamily  = bernoulli(link = \"logit\"),\nprior   = priors,\nchains  = 4, iter = 2000, seed = 123,\ncontrol = list(adapt_delta = 0.95),\nrefresh = 0\n)\n}))\n\nbayes_or &lt;- brms::posterior_summary(bayes_fit, pars = \"^b_\") %&gt;%\nas.data.frame() %&gt;%\ntibble::rownames_to_column(\"raw\") %&gt;%\ndplyr::mutate(\nterm = gsub(\"^b_\", \"\", raw),\nOR   = exp(Estimate),\nLCL  = exp(Q2.5),\nUCL  = exp(Q97.5)\n) %&gt;%\ndplyr::select(term, OR, LCL, UCL)\n\n\n\n\nPosterior Odd Ratios (Main Results)\n\n\nCode\nknitr::kable(\ndplyr::mutate(bayes_or, dplyr::across(c(OR, LCL, UCL), ~ round(.x, 2)))\n)\n\n\n\n\nTable 4: Bayesian logistic regression: posterior odds ratios (OR) with 95% credible intervals.\n\n\n\n\n\n\nterm\nOR\nLCL\nUCL\n\n\n\n\nIntercept\n0.06\n0.05\n0.07\n\n\nage_c\n2.93\n2.62\n3.30\n\n\nbmi_c\n1.92\n1.76\n2.10\n\n\nsexMale\n1.28\n1.06\n1.55\n\n\nrace4Hispanic\n1.82\n1.40\n2.38\n\n\nrace4Black\n1.62\n1.23\n2.12\n\n\nrace4Other\n2.11\n1.46\n2.99\n\n\n\n\n\n\n\n\nAge and BMI show strong positive associations with diabetes (credible intervals exclude 1).\nFemale sex shows lower odds than male (protective factor).\nNon-White racial groups have higher odds compared with Whites, consistent with known disparities.\nAll model parameters exhibit well-defined, unimodal posteriors with narrow credible intervals.\n\n\nDiagnostics and Model Fit\n\n\nCode\nknitr::kable(as.data.frame(brms::bayes_R2(bayes_fit)))\n\n\n\n\nTable 5: Bayesian R² summary.\n\n\n\n\n\n\n\nEstimate\nEst.Error\nQ2.5\nQ97.5\n\n\n\n\nR2\n0.1380082\n0.0118579\n0.115269\n0.1616909\n\n\n\n\n\n\n\n\n\n\nCode\ndiag &lt;- posterior::summarise_draws(bayes_fit, \"rhat\", \"ess_bulk\", \"ess_tail\")\n\ndiag_b &lt;- diag |&gt;\ndplyr::as_tibble() |&gt;\ndplyr::filter(grepl(\"^b_\", .data$variable)) |&gt;\ndplyr::transmute(\nParameter = .data$variable,\nRhat      = .data$rhat,\nBulk_ESS  = .data$ess_bulk,\nTail_ESS  = .data$ess_tail\n)\n\nknitr::kable(diag_b, digits = 1)\n\n\n\n\nTable 6: MCMC diagnostics (R-hat and Effective Sample Sizes) for model parameters.\n\n\n\n\n\n\nParameter\nRhat\nBulk_ESS\nTail_ESS\n\n\n\n\nb_Intercept\n1\n2721.2\n2817.8\n\n\nb_age_c\n1\n2605.7\n2704.7\n\n\nb_bmi_c\n1\n3229.5\n2783.0\n\n\nb_sexMale\n1\n3613.7\n3128.7\n\n\nb_race4Hispanic\n1\n3805.4\n3109.7\n\n\nb_race4Black\n1\n3685.6\n3133.0\n\n\nb_race4Other\n1\n3513.0\n2637.5\n\n\n\n\n\n\n\n\nAll parameters achieved R̂ ≈ 1.00 and effective sample sizes &gt;2,000, indicating excellent convergence. The Bayesian R² ≈ 0.13, showing that age, BMI, sex, and race explain about 13% of diabetes variability.\n\n\nModel Comparison\n\n\nCode\ninvisible(capture.output({\nfit_no_race &lt;- update(bayes_fit, formula = update(fml_bayes, . ~ . - race4))\nfit_no_sex  &lt;- update(bayes_fit, formula = update(fml_bayes, . ~ . - sex))\n}))\n\nloo_base    &lt;- loo::loo(bayes_fit)\nloo_no_race &lt;- loo::loo(fit_no_race)\nloo_no_sex  &lt;- loo::loo(fit_no_sex)\n\ncmp_df &lt;- as.data.frame(loo::loo_compare(loo_base, loo_no_race, loo_no_sex))\ncmp_df$Model &lt;- rownames(cmp_df)\ncmp_df &lt;- cmp_df[, c(\"Model\", setdiff(names(cmp_df), \"Model\"))]\n\nknitr::kable(\ncmp_df,\ncaption = \"LOO comparison (higher elpd_loo indicates better predictive performance).\"\n)\n\n\n\nBayesian model comparison (LOO): base model vs. reduced models without race or sex.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nelpd_diff\nse_diff\nelpd_loo\nse_elpd_loo\np_loo\nse_p_loo\nlooic\nse_looic\n\n\n\n\nbayes_fit\nbayes_fit\n0.000000\n0.000000\n-1574.573\n56.98453\n8.291612\n0.5425672\n3149.146\n113.9691\n\n\nfit_no_sex\nfit_no_sex\n-2.184925\n3.291896\n-1576.758\n57.12682\n6.622225\n0.4449483\n3153.516\n114.2536\n\n\nfit_no_race\nfit_no_race\n-14.758799\n6.315403\n-1589.332\n54.49691\n5.468956\n0.3648936\n3178.664\n108.9938\n\n\n\n\n\nModels excluding race or sex had lower expected log predictive density (elpd), confirming that both variables contribute meaningfully to model fit.\n\n\nPosterior Predictive Checks\n\n\nCode\nyobs &lt;- adult_imp1$diabetes_ind\n\n\n\n\nCode\n# Use a modest number of replicated datasets\n\nyrep_small &lt;- brms::posterior_predict(bayes_fit, ndraws = 50)\n\nbayesplot::ppc_dens_overlay(\ny    = yobs,\nyrep = yrep_small\n)\n\n\n\n\n\nPosterior predictive check: density overlay for diabetes outcome.\n\n\n\n\n\n\nCode\nbayesplot::pp_check(bayes_fit, type = \"bars\", nsamples = 100)\n\n\n\n\n\n\n\n\nFigure 1: Posterior predictive check: observed vs. replicated outcome distribution (bars).\n\n\n\n\n\n\n\nCode\nyrep &lt;- brms::posterior_predict(bayes_fit, ndraws = 400)\nbayesplot::ppc_stat(y = yobs, yrep = yrep, stat = \"mean\")\nbayesplot::ppc_stat(y = yobs, yrep = yrep, stat = \"sd\")\n\n\n\n\n\n\n\n\nFigure 2: Posterior predictive checks for mean and standard deviation of the binary outcome.\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Posterior predictive checks for mean and standard deviation of the binary outcome.\n\n\n\n\n\nPosterior predictive checks show excellent model calibration: simulated means and standard deviations closely match observed data. The model accurately reproduces diabetes prevalence (~10–11%) seen in the imputed dataset.\n\n\nIndividual Level Predictions\n\n\nCode\nadult_means &lt;- adult_imp1 %&gt;% summarise(\nage_mean = mean(age, na.rm = TRUE),\nage_sd   = sd(age, na.rm = TRUE),\nbmi_mean = mean(bmi, na.rm = TRUE),\nbmi_sd   = sd(bmi, na.rm = TRUE)\n)\n\nto_model_row &lt;- function(age_raw, bmi_raw, sex_lab, race4_lab) {\ntibble(\nage_c  = (age_raw - adult_means$age_mean)/adult_means$age_sd,\nbmi_c  = (bmi_raw - adult_means$bmi_mean)/adult_means$bmi_sd,\nsex    = factor(sex_lab,   levels = levels(adult_imp1$sex)),\nrace4  = factor(race4_lab, levels = levels(adult_imp1$race4)),\nwt_norm = 1\n)\n}\n\nplot_post_density &lt;- function(df_row, title_txt) {\nphat &lt;- posterior_linpred(bayes_fit, newdata = df_row, transform = TRUE)\nci95 &lt;- quantile(phat, c(0.025, 0.975))\nggplot(data.frame(pred = as.numeric(phat)), aes(x = pred)) +\ngeom_density(fill = \"skyblue\", alpha = 0.4) +\ngeom_vline(xintercept = ci95[1], linetype = \"dashed\", color = \"red\") +\ngeom_vline(xintercept = ci95[2], linetype = \"dashed\", color = \"red\") +\nlabs(x = \"P(Diabetes = 1)\", y = \"Density\", title = title_txt) +\ntheme_minimal()\n}\n\np1 &lt;- to_model_row(adult$age[1], adult$bmi[1],\nas.character(adult$sex[1]), as.character(adult$race4[1]))\nplot_post_density(p1, \"Participant 1: Posterior Predictive Distribution (95% CrI)\")\n\n\n\n\n\nPosterior predictive distributions for example participants.\n\n\n\n\nPosterior predictive densities for individual participants illustrate uncertainty in diabetes risk estimates. Credible intervals quantify plausible risk ranges for each profile.\n\n\nMCMC Diagnostics and Posterior Distributions\n\n\nCode\nbayesplot::mcmc_areas(as.array(bayes_fit), regex_pars = \"^b_\", prob = 0.95)\n\n\n\n\n\n\n\n\nFigure 4: Posterior distributions (95% credible mass) for slope parameters.\n\n\n\n\n\n\n\nCode\nbayesplot::mcmc_trace(as.array(bayes_fit), regex_pars = \"^b_\")\n\n\n\n\n\n\n\n\nFigure 5: Trace plots for slope parameters (chain mixing and stationarity).\n\n\n\n\n\n\n\nCode\npost_array &lt;- posterior::as_draws_array(bayes_fit)\nbayesplot::mcmc_acf(post_array, pars = c(\"b_age_c\", \"b_bmi_c\"))\n\n\n\n\n\n\n\n\nFigure 6: Autocorrelation plots for posterior samples of age and BMI coefficients (MCMC diagnostics).\n\n\n\n\n\nTrace, density, and autocorrelation plots confirm smooth mixing, unimodal posteriors, and minimal autocorrelation across chains.\nAll four chains showed strong convergence with no signs of divergence or non-stationarity.\nTrace plots revealed stable, overlapping chains with consistent mixing across iterations.\nAutocorrelation decayed rapidly toward zero, confirming efficient sampling and low dependency between successive draws.\nTogether with R̂ ≈ 1.00 and high effective sample sizes, these diagnostics indicate a well-behaved posterior and reliable inference.\n\n\nCorrelation Structure (Imputed Data)\n\n\nCode\nstopifnot(all(c(\"diabetes_ind\",\"age\",\"bmi\") %in% names(adult_imp1)))\n\ncorr_matrix_imputed &lt;- cor(\nadult_imp1[, c(\"diabetes_ind\", \"age\", \"bmi\")],\nuse = \"complete.obs\",\nmethod = \"pearson\"\n)\n\ncorr_melted_imputed &lt;- reshape2::melt(\ncorr_matrix_imputed,\nvarnames = c(\"Var1\",\"Var2\"),\nvalue.name = \"value\"\n)\n\nggplot(corr_melted_imputed, aes(Var1, Var2, fill = value)) +\ngeom_tile(color = \"white\") +\nscale_fill_gradient2(\nlow = \"blue\", mid = \"white\", high = \"red\",\nmidpoint = 0, limits = c(-1, 1), name = \"Correlation\"\n) +\ntheme_minimal() +\nlabs(\ntitle = \"Correlation Heatmap (Imputed Data: adult_imp1)\",\nx = \"Features\",\ny = \"Features\"\n)\n\n\n\n\n\nCorrelation heatmap (imputed dataset: adult_imp1).\n\n\n\n\n\n\nPrior vs. Posterior\n\n\nCode\nsrc_obj &lt;- if (exists(\"bayes_fit_prior\") && !is.null(bayes_fit_prior)) bayes_fit_prior else bayes_fit\ndraws_names &lt;- names(posterior::as_draws_df(src_obj))\nsort(grep(\"^(b_|prior_b_)\", draws_names, value = TRUE))\n\n\n[1] \"b_age_c\"         \"b_bmi_c\"         \"b_Intercept\"     \"b_race4Black\"   \n[5] \"b_race4Hispanic\" \"b_race4Other\"    \"b_sexMale\"      \n\n\n\n\nCode\nsrc_obj &lt;- if (exists(\"bayes_fit_prior\") && !is.null(bayes_fit_prior)) bayes_fit_prior else bayes_fit\ndraws_df &lt;- posterior::as_draws_df(src_obj)\nall_cols &lt;- names(draws_df)\n\nwant_post &lt;- c(\"b_age_c\",\"b_bmi_c\",\"b_sexFemale\",\"b_race4Black\",\"b_race4Hispanic\",\"b_race4Other\")\n\nhave_post  &lt;- intersect(want_post, all_cols)\nhave_prior &lt;- intersect(paste0(\"prior_\", have_post), all_cols)\n\npairs &lt;- data.frame(post = have_post, prior = paste0(\"prior_\", have_post),\nstringsAsFactors = FALSE)\npairs &lt;- pairs[pairs$prior %in% have_prior, , drop = FALSE]\n\nif (nrow(pairs) == 0) {\nknitr::asis_output(\"**No matching prior/posterior parameters found to overlay.**\")\n} else {\npost_long &lt;- tidyr::pivot_longer(\ndraws_df[, pairs$post, drop = FALSE],\ncols = tidyselect::everything(), names_to = \"term\", values_to = \"estimate\"\n)\npost_long$type &lt;- \"Posterior\"\n\nprior_tmp &lt;- draws_df[, pairs$prior, drop = FALSE]\ncolnames(prior_tmp) &lt;- pairs$post\nprior_long &lt;- tidyr::pivot_longer(\nprior_tmp,\ncols = tidyselect::everything(), names_to = \"term\", values_to = \"estimate\"\n)\nprior_long$type &lt;- \"Prior\"\n\ncombined_draws &lt;&lt;- dplyr::bind_rows(prior_long, post_long)\n\nlbl &lt;- c(\nb_age_c = \"Age (1 SD)\", b_bmi_c = \"BMI (1 SD)\",\nb_sexFemale = \"Female vs Male\",\nb_race4Black = \"Black vs White\",\nb_race4Hispanic = \"Hispanic vs White\",\nb_race4Other = \"Other vs White\"\n)\ncombined_draws$term &lt;- factor(\ncombined_draws$term,\nlevels = intersect(names(lbl), unique(combined_draws$term)),\nlabels = lbl[intersect(names(lbl), unique(combined_draws$term))]\n)\n\nggplot2::ggplot(combined_draws, ggplot2::aes(x = estimate, linetype = type)) +\nggplot2::geom_density() +\nggplot2::facet_wrap(~ term, scales = \"free\", ncol = 2) +\nggplot2::labs(x = \"Coefficient (log-odds)\", y = \"Density\", linetype = NULL) +\nggplot2::theme_minimal()\n}\n\n\n\n\nNo matching prior/posterior parameters found to overlay.\n\n\nFigure 7: Prior (dashed) vs posterior (solid) densities for selected coefficients.\n\n\n\n\n\n\nCode\nif (exists(\"combined_draws\") && is.data.frame(combined_draws) && nrow(combined_draws) &gt; 0) {\nggplot(combined_draws, aes(x = estimate, fill = type)) +\ngeom_density(alpha = 0.4) +\nfacet_wrap(~ term, scales = \"free\", ncol = 2) +\ntheme_minimal(base_size = 13) +\nlabs(\ntitle = \"Prior vs Posterior Distributions\",\nx = \"Coefficient estimate\",\ny = \"Density\",\nfill = \"\"\n)\n} else {\nknitr::asis_output(\"**Skipped: no matching prior/posterior draws to plot.**\")\n}\n\n\n\n\nSkipped: no matching prior/posterior draws to plot.\n\n\nFigure 8: Prior vs Posterior Distributions (ggplot2 version).\n\n\n\n\n\n\nCode\npar_names &lt;- names(posterior::as_draws_df(bayes_fit))\nwant &lt;- c(\"b_age_c\",\"b_bmi_c\",\"b_sexFemale\",\"b_race4Black\",\"b_race4Hispanic\",\"b_race4Other\")\nhave &lt;- intersect(want, par_names)\n\nif (length(have)) {\nbayesplot::mcmc_areas(as.array(bayes_fit), pars = have, prob = 0.95)\n} else {\nknitr::asis_output(\"**No matching slope parameters available to plot.**\")\n}\n\n\n\n\n\n\n\n\nFigure 9: Posterior density areas (95% credible mass) for age, BMI, sex, and race coefficients.\n\n\n\n\n\nFor age and BMI, the posterior densities shift notably away from the N(0, 2.5) prior toward positive values and are narrower, indicating strong information from the data; for sex, the posterior remains closer to the prior with more overlap, indicating weaker evidence.\nThe overlay of prior and posterior densities illustrates that informative updates occurred primarily for BMI, age, and race coefficients, which showed distinct posterior shifts relative to the priors. In contrast, weaker predictors such as sex displayed overlapping distributions, indicating that inference for those parameters was more influenced by prior uncertainty than by the observed data. This balance confirms appropriate regularization rather than overfitting.\n\n\nModel Fit and Calibration\n\n\nCode\nor_plot &lt;- bayes_or %&gt;%\ndplyr::filter(term != \"Intercept\") %&gt;%\ndplyr::mutate(\nterm_clean = dplyr::case_when(\nterm == \"age_c\"            ~ \"Age (per 1 SD)\",\nterm == \"bmi_c\"            ~ \"BMI (per 1 SD)\",\nterm == \"sexFemale\"        ~ \"Female (vs. Male)\",\nterm == \"sexMale\"          ~ \"Male (vs. Female)\",\nterm == \"race4Black\"       ~ \"Black (vs. White)\",\nterm == \"race4Hispanic\"    ~ \"Hispanic (vs. White)\",\nterm == \"race4Other\"       ~ \"Other (vs. White)\",\nTRUE                       ~ term\n)\n)\n\nggplot(or_plot, aes(x = OR, y = reorder(term_clean, OR))) +\ngeom_vline(xintercept = 1, linetype = 2) +\ngeom_point() +\ngeom_errorbarh(aes(xmin = LCL, xmax = UCL), height = 0.15) +\nlabs(x = \"Odds ratio (logit model, posterior)\", y = NULL)\n\n\n\n\n\n\n\n\nFigure 10: Posterior odds ratios (points) with 95% credible intervals (lines).\n\n\n\n\n\n\n\nCode\npred_mean &lt;- colMeans(brms::posterior_epred(bayes_fit))\nggplot(data.frame(pred = pred_mean, obs = yobs),\naes(x = pred, y = obs)) +\ngeom_point(alpha = 0.15, position = position_jitter(height = 0.03)) +\ngeom_smooth(method = \"loess\", se = TRUE) +\nlabs(x = \"Mean predicted probability\", y = \"Observed diabetes (0/1)\")\n\n\n\n\n\n\n\n\nFigure 11: Observed outcome vs. mean predicted probability (calibration scatter with smoother).\n\n\n\n\n\n\n\nCode\npost_pred &lt;- brms::posterior_epred(bayes_fit, summary = FALSE)\npost_prev &lt;- rowMeans(post_pred)\nobs_prev  &lt;- mean(adult_imp1$diabetes_ind, na.rm = TRUE)\npost_prev_mat &lt;- matrix(post_prev, ncol = 1, dimnames = list(NULL, \"Prevalence\"))\n\np &lt;- bayesplot::mcmc_dens(post_prev_mat)\np + ggplot2::labs(title = \"Posterior diabetes prevalence\",\nx = \"Predicted prevalence\", y = NULL) +\nggplot2::geom_vline(xintercept = obs_prev, linetype = 2)\n\np\n\n\n\n\n\n\n\n\nFigure 12: Posterior predictive distribution of diabetes prevalence compared to observed NHANES prevalence.\n\n\n\n\n\n\n\n\n\n\n\nFigure 13: Posterior predictive distribution of diabetes prevalence compared to observed NHANES prevalence.\n\n\n\n\n\nThe posterior predictive distribution of diabetes prevalence closely mirrored the survey-estimated prevalence, with the posterior mean aligning within about 1% of the observed rate.\n\n\nCode\n# Posterior predictive prevalence (replicated datasets)\n\nyrep &lt;- brms::posterior_predict(bayes_fit, ndraws = 2000)   # draws x observations (0/1)\npost_prev &lt;- rowMeans(yrep)                                 # prevalence each posterior draw\n\n# Survey-weighted observed prevalence (population estimate)\n\ndes_obs &lt;- survey::svydesign(\nid = ~SDMVPSU, strata = ~SDMVSTRA, weights = ~WTMEC2YR,\nnest = TRUE, data = adult_imp1\n)\nobs &lt;- survey::svymean(~diabetes_ind, des_obs, na.rm = TRUE)\nobs_prev  &lt;- as.numeric(obs[\"diabetes_ind\"])\nobs_se    &lt;- as.numeric(SE(obs)[\"diabetes_ind\"])\nobs_lcl   &lt;- max(0, obs_prev - 1.96 * obs_se)\nobs_ucl   &lt;- min(1, obs_prev + 1.96 * obs_se)\n\n# Plot: posterior density with weighted point estimate and 95% CI band\n\nggplot(data.frame(prev = post_prev), aes(x = prev)) +\ngeom_density(alpha = 0.6) +\nannotate(\"rect\", xmin = obs_lcl, xmax = obs_ucl, ymin = 0, ymax = Inf, alpha = 0.15) +\ngeom_vline(xintercept = obs_prev, linetype = 2) +\ncoord_cartesian(xlim = c(0, 1)) +\nlabs(x = \"Diabetes prevalence\", y = \"Posterior density\",\nsubtitle = sprintf(\"Survey-weighted NHANES prevalence = %.1f%% (95%% CI %.1f–%.1f%%)\",\n100*obs_prev, 100*obs_lcl, 100*obs_ucl)) +\ntheme_minimal()\n\n\n\n\n\n\n\n\nFigure 14: Population (NHANES survey-weighted) vs posterior predictive diabetes prevalence.\n\n\n\n\n\n\n\nCode\n# Posterior predictive draws for the outcome\n\npp_samples &lt;- brms::posterior_predict(bayes_fit, ndraws = 1000)  # draws x individuals\npp_proportion &lt;- rowMeans(pp_samples)                            # prevalence per draw\n\npp_proportion_df &lt;- tibble::tibble(proportion = pp_proportion)\n\nggplot2::ggplot(pp_proportion_df, ggplot2::aes(x = proportion)) +\nggplot2::geom_histogram(binwidth = 0.01, color = \"black\") +\nggplot2::theme_minimal() +\nggplot2::labs(\ntitle = \"Posterior Distribution of Proportion of Diabetes = 1\",\nx = \"Proportion with diabetes\",\ny = \"Frequency\"\n)\n\n\n\n\n\nPosterior distribution of proportion of Diabetes = 1.\n\n\n\n\n\n\nCode\n# Survey-weighted prevalence (already computed earlier as `obs`)\n\nobs_prev &lt;- as.numeric(obs[\"diabetes_ind\"])\nobs_se   &lt;- as.numeric(survey::SE(obs)[\"diabetes_ind\"])\n\nsummary_table &lt;- tibble::tibble(\nMethod = c(\n\"Survey-weighted mean (NHANES)\",\n\"Imputed dataset mean (adult_imp1)\",\n\"Posterior predictive mean (Bayesian)\"\n),\ndiabetes_mean = c(\nobs_prev,\nmean(adult_imp1$diabetes_ind, na.rm = TRUE),\nmean(pp_proportion)\n),\nSE = c(\nobs_se,\nNA_real_,\nNA_real_\n)\n)\n\nknitr::kable(\nsummary_table,\ndigits = 4,\ncaption = \"Comparison of Diabetes Prevalence Across Methods\"\n)\n\n\n\nComparison of Diabetes Prevalence Across Methods.\n\n\nMethod\ndiabetes_mean\nSE\n\n\n\n\nSurvey-weighted mean (NHANES)\n0.1039\nNA\n\n\nImputed dataset mean (adult_imp1)\n0.1291\nNA\n\n\nPosterior predictive mean (Bayesian)\n0.1260\nNA\n\n\n\n\n\n\n\nPosterior Predictions and Inverse Inference\n\n\nCode\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tibble)\n\n# 1. Get the mean / SD used for standardization in adult_imp1\nadult_means &lt;- adult_imp1 %&gt;%\n  summarise(\n    age_mean = mean(age, na.rm = TRUE),\n    age_sd   = sd(age, na.rm = TRUE),\n    bmi_mean = mean(bmi, na.rm = TRUE),\n    bmi_sd   = sd(bmi, na.rm = TRUE)\n  )\n\n# 2. Choose raw BMI range (to plot) and a raw age for the profile\nbmi_raw &lt;- seq(18, 40, by = 0.5)   # this is what goes on the x-axis\nage_raw &lt;- 40                      # e.g., a 40-year-old\n\n# 3. Convert raw age/BMI to the standardized predictors the model uses\nnewdata_grid &lt;- tibble(\n  age_c  = (age_raw - adult_means$age_mean) / adult_means$age_sd,\n  bmi_c  = (bmi_raw - adult_means$bmi_mean) / adult_means$bmi_sd,\n  sex    = factor(\"Female\",  levels = levels(adult_imp1$sex)),\n  race4  = factor(\"Hispanic\", levels = levels(adult_imp1$race4)),\n  wt_norm = 1\n)\n\n# 4. Posterior predicted probabilities for each BMI value\npred_probs &lt;- posterior_linpred(\n  bayes_fit,\n  newdata   = newdata_grid,\n  transform = TRUE\n)\n\nprob_mean &lt;- colMeans(pred_probs)\n\npred_df &lt;- tibble(\n  bmi_raw   = bmi_raw,\n  prob_mean = prob_mean\n)\n\n# 5. Target probability and closest BMI\ntarget_prob &lt;- 0.30\nclosest &lt;- pred_df[which.min(abs(pred_df$prob_mean - target_prob)), , drop = FALSE]\n\n# 6. Plot (now should look like Namita's: an increasing curve, target line, and vertical BMI line)\nggplot(pred_df, aes(x = bmi_raw, y = prob_mean)) +\n  geom_line(color = \"darkblue\", linewidth = 1.2) +\n  geom_hline(yintercept = target_prob, color = \"red\", linetype = \"dashed\") +\n  geom_vline(xintercept = closest$bmi_raw, color = \"red\", linetype = \"dotted\") +\n  annotate(\n    \"text\",\n    x = closest$bmi_raw,\n    y = target_prob + 0.05,\n    label = paste0(\"Target BMI ≈ \", round(closest$bmi_raw, 1)),\n    color = \"red\",\n    hjust = -0.1\n  ) +\n  labs(\n    x = \"BMI (kg/m^2)\",\n    y = \"Predicted probability of diabetes\",\n    title = \"Inverse Prediction: BMI Needed for Target Diabetes Risk\\n(Female, Hispanic, age 40)\"\n  ) +\n  coord_cartesian(ylim = c(0, 1)) +\n  theme_bw()\n\n\n\n\n\nInverse Prediction: BMI needed for target diabetes risk."
  },
  {
    "objectID": "index.html#results",
    "href": "index.html#results",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk Using NHANES 2013–2014 Data",
    "section": "Results",
    "text": "Results\nA concise summary of posterior estimates is provided below.\n\nCode\ncat(paste(bullets, collapse = \"\\n\"))\n\n\nPopulation-level interpretation (posterior, odds ratios)\n\nConvergence. All R-hat ≈ 1.00; large ESS → excellent mixing.\nBaseline risk. Male, White, mean age/BMI: ~5.2% predicted diabetes prevalence.\nAge. +1 SD → 2.93× (95% CrI 2.62–3.30; CrI excludes 1).\nBMI. +1 SD → 1.92× (95% CrI 1.76–2.10; CrI excludes 1).\nFemale vs. Male. NA× (95% CrI NA–NA; CrI overlaps 1).\nBlack vs. White. 1.62× (95% CrI 1.23–2.12; CrI excludes 1).\nHispanic vs. White. 1.82× (95% CrI 1.40–2.38; CrI excludes 1).\nOther/Multi vs. White. 2.11× (95% CrI 1.46–2.99; CrI excludes 1).\n\n\n\nCode\n# Combine results from all three models\n\nsvy_tbl   &lt;- if (exists(\"svy_or\") && nrow(svy_or) &gt; 0)\ndplyr::mutate(svy_or,   Model = \"Survey-weighted (MLE)\") else NULL\nmi_tbl    &lt;- if (exists(\"mi_or\") && nrow(mi_or) &gt; 0)\ndplyr::mutate(mi_or,    Model = \"MICE Pooled\") else NULL\nbayes_tbl &lt;- if (exists(\"bayes_or\") && nrow(bayes_or) &gt; 0)\ndplyr::mutate(bayes_or, Model = \"Bayesian\") %&gt;%\ndplyr::filter(term != \"Intercept\") else NULL\n\nall_tbl &lt;- dplyr::bind_rows(Filter(Negate(is.null), list(svy_tbl, mi_tbl, bayes_tbl))) %&gt;%\ndplyr::mutate(\nterm = dplyr::case_when(\ngrepl(\"^bmi\", term,  ignore.case = TRUE) ~ \"BMI (per 1 SD)\",\ngrepl(\"^age\", term,  ignore.case = TRUE) ~ \"Age (per 1 SD)\",\ngrepl(\"^sexFemale$\", term)               ~ \"Female (vs. Male)\",\ngrepl(\"^sexMale$\", term)                 ~ \"Male (vs. Female)\",\ngrepl(\"^race4Hispanic$\", term)           ~ \"Hispanic (vs. White)\",\ngrepl(\"^race4Black$\", term)              ~ \"Black (vs. White)\",\ngrepl(\"^race4Other$\", term)              ~ \"Other (vs. White)\",\nTRUE ~ term\n),\nOR_CI = sprintf(\"%.2f (%.2f – %.2f)\", OR, LCL, UCL)\n) %&gt;%\ndplyr::select(Model, term, OR_CI)\n\n\n\n\nCode\nknitr::kable(all_tbl, align = c(\"l\",\"l\",\"c\"))\n\n\n\n\nTable 7: Comparison of odds ratios (per 1 SD for age and BMI) and 95% intervals across survey-weighted, MICE, and Bayesian frameworks.\n\n\n\n\n\n\n\n\n\n\n\nModel\nterm\nOR_CI\n\n\n\n\nSurvey-weighted (MLE)\nAge (per 1 SD)\n2.98 (2.70 – 3.28)\n\n\nSurvey-weighted (MLE)\nBMI (per 1 SD)\n1.93 (1.68 – 2.22)\n\n\nSurvey-weighted (MLE)\nMale (vs. Female)\n1.29 (1.02 – 1.63)\n\n\nSurvey-weighted (MLE)\nHispanic (vs. White)\n1.81 (1.43 – 2.29)\n\n\nSurvey-weighted (MLE)\nBlack (vs. White)\n1.60 (1.16 – 2.21)\n\n\nSurvey-weighted (MLE)\nOther (vs. White)\n2.20 (1.46 – 3.30)\n\n\nMICE Pooled\nscale(age)\n2.90 (2.61 – 3.21)\n\n\nMICE Pooled\nscale(bmi)\n1.81 (1.66 – 1.96)\n\n\nMICE Pooled\nrelevel(sex, “Male”)Female\n0.81 (0.68 – 0.96)\n\n\nMICE Pooled\nrelevel(race4, “White”)Hispanic\n2.07 (1.66 – 2.59)\n\n\nMICE Pooled\nrelevel(race4, “White”)Black\n1.79 (1.43 – 2.24)\n\n\nMICE Pooled\nrelevel(race4, “White”)Other\n2.00 (1.51 – 2.66)\n\n\nBayesian\nAge (per 1 SD)\n2.93 (2.62 – 3.30)\n\n\nBayesian\nBMI (per 1 SD)\n1.92 (1.76 – 2.10)\n\n\nBayesian\nMale (vs. Female)\n1.28 (1.06 – 1.55)\n\n\nBayesian\nHispanic (vs. White)\n1.82 (1.40 – 2.38)\n\n\nBayesian\nBlack (vs. White)\n1.62 (1.23 – 2.12)\n\n\nBayesian\nOther (vs. White)\n2.11 (1.46 – 2.99)\n\n\n\n\n\n\n\n\nAcross all three frameworks—survey-weighted (MLE), multiple imputation, and Bayesian—age and BMI were consistently associated with higher odds of doctor-diagnosed diabetes. Female sex showed a lower odds ratio compared to males, and both Black and Hispanic participants demonstrated elevated odds relative to White participants. The similarity of effect sizes across frameworks underscores the robustness of these predictors to different modeling assumptions and missing-data treatments."
  },
  {
    "objectID": "index.html#discussion-and-limitations",
    "href": "index.html#discussion-and-limitations",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk Using NHANES 2013–2014 Data",
    "section": "Discussion and Limitations",
    "text": "Discussion and Limitations\n\nInterpretation\nThe Bayesian logistic regression framework produced results that were highly consistent with both the survey-weighted and MICE-pooled frequentist models. Age and BMI remained the most influential predictors of doctor-diagnosed diabetes, each showing a strong and positive association with diabetes risk.\nUnlike classical maximum likelihood estimation, the Bayesian approach directly quantified uncertainty through posterior distributions, offering richer interpretability and more transparent probability statements. The alignment between Bayesian and design-based estimates supports the robustness of these associations and highlights the practicality of Bayesian modeling for complex, weighted population data.\nPosterior predictive checks confirmed that simulated diabetes prevalence closely matched the observed NHANES estimate, supporting good model calibration. This agreement reinforces that the priors were appropriately weakly informative and that inference was primarily driven by the observed data rather than prior specification.\nOverall, this study demonstrates that Bayesian inference complements traditional epidemiologic methods by maintaining interpretability while enhancing stability and explicitly quantifying uncertainty in complex survey data.\n\n\nLimitations\nWhile this analysis demonstrates the value of Bayesian logistic regression for epidemiologic modeling, several limitations should be acknowledged.\nFirst, the use of a single imputed dataset for the Bayesian model—rather than full joint modeling of imputation uncertainty—may understate total variance.\nSecond, NHANES exam weights were normalized and treated as importance weights, which approximate but do not fully reproduce design-based inference.\nThird, the weakly informative priors \\(N(0, 2.5)\\) for slopes and Student-t(3, 0, 10) for the intercept were not empirically tuned; alternative prior specifications could slightly alter posterior intervals.\nFinally, although convergence diagnostics (R̂ ≈ 1, sufficient effective sample sizes, and stable posterior predictive checks) indicated good model performance, results are conditional on the 2013–2014 NHANES cycle and may not generalize to later datasets or longitudinal analyses."
  },
  {
    "objectID": "index.html#conclusion",
    "href": "index.html#conclusion",
    "title": "Bayesian Logistic Regression for Predicting Diabetes Risk Using NHANES 2013–2014 Data",
    "section": "Conclusion",
    "text": "Conclusion\nThe Bayesian, survey-weighted, and imputed logistic regression frameworks all identified consistent predictors of diabetes risk in U.S. adults: advancing age, higher BMI, sex (lower odds for females), and non-White race/ethnicity.\nThe Bayesian model produced estimates nearly identical in direction and magnitude to the frequentist results while providing a more comprehensive assessment of uncertainty through posterior distributions and credible intervals.\nThese consistent findings across modeling frameworks underscore the robustness of core risk factors and support the use of Bayesian inference for epidemiologic research involving complex survey data.\nBy incorporating prior information and using MCMC to sample from the full posterior distribution, Bayesian inference enhances model transparency and interpretability. Future extensions could integrate hierarchical priors, multiple NHANES cycles, or Bayesian model averaging to better capture population heterogeneity, temporal trends, and evolving diabetes risk patterns."
  },
  {
    "objectID": "slides.html#slide-1-introduction",
    "href": "slides.html#slide-1-introduction",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013–2014)",
    "section": "Slide 1: Introduction",
    "text": "Slide 1: Introduction\n\nDiabetes is a chronic disease with rising global prevalence.\nEarly identification of risk factors is key to prevention and control.\nBayesian methods allow flexible modeling with uncertainty quantification.\nAim: Predict diabetes diagnosis using Bayesian logistic regression with imputed data ."
  },
  {
    "objectID": "slides.html#slide-2-data-source",
    "href": "slides.html#slide-2-data-source",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013–2014)",
    "section": "Slide 2: Data Source",
    "text": "Slide 2: Data Source\n\nData: National Health and Nutrition Examination Survey (NHANES)\nAdult dataset (&gt;20 years).\nVariables included:\n\nDemographics: age, sex, race\nClinical: BMI\nOutcome: diabetes_dx (diagnosis: 0 = No, 1 = Yes)\nhead(adult)"
  },
  {
    "objectID": "slides.html#slide-3-missing-data-assessment",
    "href": "slides.html#slide-3-missing-data-assessment",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013–2014)",
    "section": "Slide 3: Missing Data Assessment",
    "text": "Slide 3: Missing Data Assessment\n\nOverall missingness: ~4%, No variable completely missing, Missingness is not uniform\nMissingness pattern: likely MAR (Missing At Random).\nClustered mainly in bmi and diabetes_dx.\nDecision: Apply Multiple Imputation by Chained Equations (MICE)."
  },
  {
    "objectID": "slides.html#slide-4-mice-imputation",
    "href": "slides.html#slide-4-mice-imputation",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013–2014)",
    "section": "Slide 4: MICE Imputation",
    "text": "Slide 4: MICE Imputation\n\nMethod: Predictive mean matching (PMM) for continuous vars; logistic regression for binary.\nIterations: 5 imputations, 10 iterations each, combined imputed datasets using Rubin’s rules.\nDistribution plots confirmed consistency with the original data."
  },
  {
    "objectID": "slides.html#slide-5-multivariate-imputation-by-chained-equations-pooled-logistic-regression",
    "href": "slides.html#slide-5-multivariate-imputation-by-chained-equations-pooled-logistic-regression",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013–2014)",
    "section": "Slide 5: Multivariate Imputation by Chained Equations (Pooled Logistic Regression)",
    "text": "Slide 5: Multivariate Imputation by Chained Equations (Pooled Logistic Regression)\n  glm(diabetes_dx ~ age_c + bmi_c + sex + race, family = binomial()) }) pool_mi &lt;- pool(fit_mi) \n\nn=5,769 participants\nModel significance: All predictors are statistically significant (p &lt; 0.01), suggesting a robust association across demographic and health variables."
  },
  {
    "objectID": "slides.html#slide-5-bayesian-logistic-regression",
    "href": "slides.html#slide-5-bayesian-logistic-regression",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013–2014)",
    "section": "Slide 5: Bayesian Logistic Regression",
    "text": "Slide 5: Bayesian Logistic Regression\nOutcome: diabetes_dx (0 = non-diabetic, 1 = diabetic)\nPredictors: age_c, bmi_c, sex, race.\n\nIntercept prior: student_t(3, 0, 10) — allows heavy tails for flexibility in the intercept estimate. (VanDeSchoot2013?)\nRegression coefficients prior: normal(0, 2.5) — providing weakly informative regularization, constraining extreme values without overpowering the data. (VandeSchoot2021?)\nImplemented in brms (Stan backend), Posterior draws = 4000 (4 chains × 1000 iterations). Logistic link function"
  },
  {
    "objectID": "slides.html#slide-6-bayesian-model-equation",
    "href": "slides.html#slide-6-bayesian-model-equation",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013–2014)",
    "section": "Slide 6: Bayesian Model Equation",
    "text": "Slide 6: Bayesian Model Equation\nlibrary(gt)\npriors &lt;- c(\n  set_prior(\"normal(0, 2.5)\", class = \"b\"),\n  set_prior(\"student_t(3, 0, 10)\", class = \"Intercept\") \n)\nbayes_fit &lt;- brm(\n  formula = diabetes_dx | weights(wt_norm) ~ age_c + bmi_c + sex + race,\n  data    = adult_imp1,\n  family  = bernoulli(link = \"logit\"),\n  prior   = priors,\n  chains  = 4, iter = 2000, seed = 123,\n  control = list(adapt_delta = 0.95),\n  refresh = 0   # quiet Stan output\n)\n\\[\n\\text{logit}(P(Y=1)) = \\beta_0 + \\beta_1 \\text{age}_c + \\beta_2 \\text{bmi}_c + \\beta_3 \\text{sex} + \\beta_4 \\text{race}\n\\]\n\n( P(Y=1) ): Probability of being diabetic\nCoefficients estimated with posterior means and 95% credible intervals."
  },
  {
    "objectID": "slides.html#slide-7-bayesian-model-diagnostics",
    "href": "slides.html#slide-7-bayesian-model-diagnostics",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013–2014)",
    "section": "Slide 7: Bayesian Model Diagnostics",
    "text": "Slide 7: Bayesian Model Diagnostics\n\nRhat ≈ 1.00 → convergence achieved.\nBulk ESS &gt; 3000 for all parameters → good mixing.\nTrace plots showed stable sampling across chains.\nPosterior distributions were unimodal and well-centered."
  },
  {
    "objectID": "slides.html#slide-8-posterior-estimates",
    "href": "slides.html#slide-8-posterior-estimates",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013–2014)",
    "section": "Slide 8: Posterior Estimates",
    "text": "Slide 8: Posterior Estimates\n\n\n\n\n\n\n\n\n\nPredictor\nEstimate\n95% CI\nInterpretation\n\n\n\n\nIntercept\n-2.66\n[-2.84, -2.50]\nBaseline log-odds\n\n\nAge_c\n1.09\n[0.97, 1.22]\n↑ age increases diabetes risk\n\n\nBMI_c\n0.88\n[0.76, 1.01]\nHigher BMI linked with higher risk\n\n\nHTN\n0.65\n[0.50, 0.81]\nHypertension predicts diabetes"
  },
  {
    "objectID": "slides.html#slide-9-posterior-predictive-distribution",
    "href": "slides.html#slide-9-posterior-predictive-distribution",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013–2014)",
    "section": "Slide 9: Posterior Predictive Distribution",
    "text": "Slide 9: Posterior Predictive Distribution\nlibrary(ggplot2)\n\nggplot(combined_draws, aes(x = estimate, fill = type)) +\n  geom_density(alpha = 0.4) +\n  facet_wrap(~ term, scales = \"free\", ncol = 2) +\n  theme_minimal(base_size = 13) +\n  labs(\n    title = \"Prior vs Posterior Distributions\",\n    x = \"Coefficient estimate\",\n    y = \"Density\",\n    fill = \"\"\n  )"
  },
  {
    "objectID": "slides.html#slide-10-model-interpretation",
    "href": "slides.html#slide-10-model-interpretation",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013–2014)",
    "section": "Slide 10: Model Interpretation",
    "text": "Slide 10: Model Interpretation\n\nStrong positive relationship between age, BMI, and diabetes probability.\nPosterior predictive checks confirm the model captures data patterns well.\nImputation reduced bias and improved model robustness."
  },
  {
    "objectID": "slides.html#slide-11-limitations",
    "href": "slides.html#slide-11-limitations",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013–2014)",
    "section": "Slide 11: Limitations",
    "text": "Slide 11: Limitations\n\nNHANES data are cross-sectional → no causal inference.\nPotential unmeasured confounding (diet, physical activity).\nLimited predictors → simplified model structure.\nImputation assumes MAR; violations may introduce bias."
  },
  {
    "objectID": "slides.html#slide-12-conclusion",
    "href": "slides.html#slide-12-conclusion",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013–2014)",
    "section": "Slide 12: Conclusion",
    "text": "Slide 12: Conclusion\n\nBayesian logistic regression effectively models uncertainty.\nMICE improved data completeness and reliability.\nPosterior predictions provide interpretable probabilities for diabetes risk.\nFramework adaptable to other health outcomes (e.g., hypertension, obesity)."
  },
  {
    "objectID": "slides.html#slide-13-references",
    "href": "slides.html#slide-13-references",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013–2014)",
    "section": "Slide 13: References",
    "text": "Slide 13: References\n\nvan Buuren S., Groothuis-Oudshoorn K. (2011). MICE: Multivariate Imputation by Chained Equations in R.\nGelman A., Hill J. (2007). Data Analysis Using Regression and Multilevel/Hierarchical Models.\nNHANES Data Documentation (CDC).\nMcElreath R. (2020). Statistical Rethinking: A Bayesian Course with Examples in R and Stan."
  },
  {
    "objectID": "slides.html#slide-14-acknowledgements",
    "href": "slides.html#slide-14-acknowledgements",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013–2014)",
    "section": "Slide 14: Acknowledgements",
    "text": "Slide 14: Acknowledgements\n\nFaculty: Dr. Ashraf Cohen, PhD, MS\nUniversity of West Florida, Department: Mathematics and Statistics\n        Thanks for the guidance"
  },
  {
    "objectID": "slides.html#references",
    "href": "slides.html#references",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013–2014)",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Us",
    "section": "",
    "text": "Dr. Namita Mishra is a physician, Head and Neck surgeon, and public health researcher with a strong foundation in medicine, epidemiology, and data science. She is currently a graduate student in Data Science (Health Analytics).\nHer work focuses on the early detection and prevention of non-communicable diseases (cancer, obesity) and on health disparities at the community level. She has conducted research on salivary gland tumors, cardiac implants, and community-based healthy food access. Leveraging her skills in Data Science, she integrates statistical modeling and Bayesian methods into her analyses. Her bioinformatics expertise includes using geodata visualization tools (3D maps and GIS) for presentations. Passionate about bridging clinical insight with data-driven approaches, she is dedicated to advancing sustainable, evidence-based solutions in epidemiology and community health.\nOutside of work, she enjoys gardening, cooking, singing, and sewing.\nDr. Mishra developed the project plan, content draft, analytic coding, and coordinated commits and collaboration with the group on GitHub.\n📧 Contact: nmishra@uwf.edu\n\nAutumn S. Wilcox is a U.S. Navy veteran and Data Science graduate student at the University of West Florida, specializing in Analytics and Modeling. She has over nine years of experience in Network Operations and Technical Writing, including her current role at Navy Federal Credit Union, where she supports enterprise technology and process documentation initiatives. Autumn also holds certification in Clinical Research Quality Management (CRQM) and has contributed to quality oversight and compliance efforts in clinical research settings.\nHer background bridges technology, analytics, and healthcare, with a focus on applying data-driven approaches to improve communication and systems reliability. Outside of work, Autumn enjoys traveling, photography, and finding creative inspiration through music.\nAutumn contributed to analytic coding, content draft, structured the project workflow, and collaborated actively via GitHub.\n📧 Contact: awr12@students.uwf.edu"
  },
  {
    "objectID": "Summaries/aw/paper3_summary.html",
    "href": "Summaries/aw/paper3_summary.html",
    "title": "Bayesian Logistic Regression Capstone",
    "section": "",
    "text": "Paper 3: Abdullah, Hassan, & Mustafa (2022). “A Review on Bayesian Deep Learning in Healthcare: Applications and Challenges”\nGoal\nThe paper systematically reviews how Bayesian deep learning (BDL) is being applied in healthcare: its use cases, methodological approaches, and the challenges and future directions.\nImportance\nHealthcare data is often uncertain (noisy measurements, missing data, variability) and involves high stakes where mistakes can cost lives. While deep learning is powerful, it typically lacks mechanisms for representing uncertainty or handling limited data in a principled way. Bayesian techniques address these gaps by incorporating uncertainty, prior knowledge, and probabilistic reasoning—making models potentially safer, more trustworthy, and interpretable in clinical settings.\nMethods\n- Conduct a literature survey of recent work combining Bayesian methods with deep learning in healthcare.\n- Categorize approaches such as variational inference, Monte Carlo dropout, ensemble methods, Gaussian processes, and Bayesian neural networks.\n- Map these methods to healthcare tasks like diagnosis, prognosis, and treatment planning.\n- Evaluate strengths/weaknesses in terms of data availability, computational costs, interpretability, uncertainty calibration, and privacy.\nResults & Limitations\n- Results: BDL has shown success in disease classification, survival analysis, medical image segmentation, and predictive modeling. It often improves uncertainty quantification, may improve generalization, and provides clinicians with added information (e.g., confidence in predictions).\n- Limitations: High computational demands, scalability issues, difficulty specifying priors, and interpretability challenges remain. Many studies are proof-of-concept and lack validation in real-world clinical environments. Regulatory, privacy, and workflow integration concerns also limit deployment."
  },
  {
    "objectID": "Summaries/aw/paper1_summary.html",
    "href": "Summaries/aw/paper1_summary.html",
    "title": "Summary: Bayesian Data Analysis for Newcomers",
    "section": "",
    "text": "John K. Kruschke and Torrin M. Liddell (2017)\n\n\nThe paper tackles the issue that Bayesian data analysis is often viewed as too complicated or intimidating for newcomers. Traditional explanations rely heavily on mathematical formulas, which discourages students and researchers who want to understand the ideas without advanced math. The authors aim to show that Bayesian reasoning is actually natural and aligns with how people already think in everyday life.\n\n\n\nThe authors present Bayesian concepts in plain language with minimal technical notation. They explain the main idea of Bayesian analysis as reallocating credibility among possible explanations as new data comes in. Simple examples are used to show how priors, likelihoods, and posteriors work together. They also clarify common misunderstandings, such as the role of prior distributions and how Bayesian inference differs from frequentist methods like null hypothesis significance testing.\n\n\n\nThe article provides readers with a clear conceptual framework for Bayesian analysis. Instead of being overwhelmed by formulas, the reader can see how Bayesian methods produce intuitive and interpretable results. Examples highlight how Bayesian outputs, such as posterior distributions and credible intervals, can be directly applied to research questions.\n\n\n\nBecause the article is written as an introduction, it does not go into technical detail or advanced modeling. It is not a substitute for mathematical training or for learning to implement Bayesian models in practice. The focus is on clarity rather than rigor.\n\n\n\nThe paper does not use any real-world datasets. Instead, it relies on simple examples meant to illustrate the principles of Bayesian reasoning. These examples are conceptual rather than empirical, making the ideas easier to grasp for newcomers like myself."
  },
  {
    "objectID": "Summaries/aw/paper1_summary.html#problem-the-article-is-addressing",
    "href": "Summaries/aw/paper1_summary.html#problem-the-article-is-addressing",
    "title": "Summary: Bayesian Data Analysis for Newcomers",
    "section": "",
    "text": "The paper tackles the issue that Bayesian data analysis is often viewed as too complicated or intimidating for newcomers. Traditional explanations rely heavily on mathematical formulas, which discourages students and researchers who want to understand the ideas without advanced math. The authors aim to show that Bayesian reasoning is actually natural and aligns with how people already think in everyday life."
  },
  {
    "objectID": "Summaries/aw/paper1_summary.html#how-it-has-been-solved",
    "href": "Summaries/aw/paper1_summary.html#how-it-has-been-solved",
    "title": "Summary: Bayesian Data Analysis for Newcomers",
    "section": "",
    "text": "The authors present Bayesian concepts in plain language with minimal technical notation. They explain the main idea of Bayesian analysis as reallocating credibility among possible explanations as new data comes in. Simple examples are used to show how priors, likelihoods, and posteriors work together. They also clarify common misunderstandings, such as the role of prior distributions and how Bayesian inference differs from frequentist methods like null hypothesis significance testing."
  },
  {
    "objectID": "Summaries/aw/paper1_summary.html#results",
    "href": "Summaries/aw/paper1_summary.html#results",
    "title": "Summary: Bayesian Data Analysis for Newcomers",
    "section": "",
    "text": "The article provides readers with a clear conceptual framework for Bayesian analysis. Instead of being overwhelmed by formulas, the reader can see how Bayesian methods produce intuitive and interpretable results. Examples highlight how Bayesian outputs, such as posterior distributions and credible intervals, can be directly applied to research questions."
  },
  {
    "objectID": "Summaries/aw/paper1_summary.html#limitations",
    "href": "Summaries/aw/paper1_summary.html#limitations",
    "title": "Summary: Bayesian Data Analysis for Newcomers",
    "section": "",
    "text": "Because the article is written as an introduction, it does not go into technical detail or advanced modeling. It is not a substitute for mathematical training or for learning to implement Bayesian models in practice. The focus is on clarity rather than rigor."
  },
  {
    "objectID": "Summaries/aw/paper1_summary.html#datasets",
    "href": "Summaries/aw/paper1_summary.html#datasets",
    "title": "Summary: Bayesian Data Analysis for Newcomers",
    "section": "",
    "text": "The paper does not use any real-world datasets. Instead, it relies on simple examples meant to illustrate the principles of Bayesian reasoning. These examples are conceptual rather than empirical, making the ideas easier to grasp for newcomers like myself."
  },
  {
    "objectID": "Summaries/aw/paper5_summary.html",
    "href": "Summaries/aw/paper5_summary.html",
    "title": "Summary: The No-U-Turn Sampler (NUTS)—Adaptively Setting Path Lengths in Hamiltonian Monte Carlo",
    "section": "",
    "text": "Matthew D. Hoffman & Andrew Gelman (2014)\n\n\nHamiltonian Monte Carlo (HMC) offers efficient sampling from complex posteriors but requires manual tuning of the trajectory length (number of leapfrog steps). Poor tuning can cause slow convergence or biased sampling, making HMC difficult for non-experts to use effectively.\n\n\n\nThe authors introduce the No-U-Turn Sampler (NUTS), an adaptive extension of HMC that automatically determines the optimal trajectory length. NUTS expands the trajectory until it detects a “U-turn” in parameter space—meaning further movement would retrace steps—and then stops. Combined with dual-averaging step-size adaptation, this removes nearly all manual tuning while preserving the sampling efficiency of HMC.\n\n\n\nAcross hierarchical models and logistic regression tasks, NUTS produces higher effective sample sizes per second and improved convergence diagnostics compared to traditional HMC or Gibbs sampling. It is robust across diverse posterior geometries and scales well with model complexity, forming the default MCMC algorithm in Stan.\n\n\n\nWhile adaptive, NUTS still relies on differentiable log-posteriors and can be computationally intensive for extremely complex or multimodal distributions. Performance depends on the choice of priors and model reparameterization, and it remains unsuitable for discrete or non-differentiable models.\n\n\n\nThe paper evaluates NUTS on simulated hierarchical models and real logistic regression datasets, reporting metrics like effective sample size (ESS) and Gelman–Rubin convergence diagnostics (()) to assess efficiency and accuracy."
  },
  {
    "objectID": "Summaries/aw/paper5_summary.html#problem-the-article-is-addressing",
    "href": "Summaries/aw/paper5_summary.html#problem-the-article-is-addressing",
    "title": "Summary: The No-U-Turn Sampler (NUTS)—Adaptively Setting Path Lengths in Hamiltonian Monte Carlo",
    "section": "",
    "text": "Hamiltonian Monte Carlo (HMC) offers efficient sampling from complex posteriors but requires manual tuning of the trajectory length (number of leapfrog steps). Poor tuning can cause slow convergence or biased sampling, making HMC difficult for non-experts to use effectively."
  },
  {
    "objectID": "Summaries/aw/paper5_summary.html#how-it-has-been-solved",
    "href": "Summaries/aw/paper5_summary.html#how-it-has-been-solved",
    "title": "Summary: The No-U-Turn Sampler (NUTS)—Adaptively Setting Path Lengths in Hamiltonian Monte Carlo",
    "section": "",
    "text": "The authors introduce the No-U-Turn Sampler (NUTS), an adaptive extension of HMC that automatically determines the optimal trajectory length. NUTS expands the trajectory until it detects a “U-turn” in parameter space—meaning further movement would retrace steps—and then stops. Combined with dual-averaging step-size adaptation, this removes nearly all manual tuning while preserving the sampling efficiency of HMC."
  },
  {
    "objectID": "Summaries/aw/paper5_summary.html#results",
    "href": "Summaries/aw/paper5_summary.html#results",
    "title": "Summary: The No-U-Turn Sampler (NUTS)—Adaptively Setting Path Lengths in Hamiltonian Monte Carlo",
    "section": "",
    "text": "Across hierarchical models and logistic regression tasks, NUTS produces higher effective sample sizes per second and improved convergence diagnostics compared to traditional HMC or Gibbs sampling. It is robust across diverse posterior geometries and scales well with model complexity, forming the default MCMC algorithm in Stan."
  },
  {
    "objectID": "Summaries/aw/paper5_summary.html#limitations",
    "href": "Summaries/aw/paper5_summary.html#limitations",
    "title": "Summary: The No-U-Turn Sampler (NUTS)—Adaptively Setting Path Lengths in Hamiltonian Monte Carlo",
    "section": "",
    "text": "While adaptive, NUTS still relies on differentiable log-posteriors and can be computationally intensive for extremely complex or multimodal distributions. Performance depends on the choice of priors and model reparameterization, and it remains unsuitable for discrete or non-differentiable models."
  },
  {
    "objectID": "Summaries/aw/paper5_summary.html#datasets",
    "href": "Summaries/aw/paper5_summary.html#datasets",
    "title": "Summary: The No-U-Turn Sampler (NUTS)—Adaptively Setting Path Lengths in Hamiltonian Monte Carlo",
    "section": "",
    "text": "The paper evaluates NUTS on simulated hierarchical models and real logistic regression datasets, reporting metrics like effective sample size (ESS) and Gelman–Rubin convergence diagnostics (()) to assess efficiency and accuracy."
  },
  {
    "objectID": "Summaries/nm/My_articles.html",
    "href": "Summaries/nm/My_articles.html",
    "title": "Bayesian Logistic Regression Capstone",
    "section": "",
    "text": "Namita’s Literature"
  },
  {
    "objectID": "Summaries/nm/My_articles.html#introduction",
    "href": "Summaries/nm/My_articles.html#introduction",
    "title": "Bayesian Logistic Regression Capstone",
    "section": "Introduction",
    "text": "Introduction\n\nBayesian Hierarchical Model (Disease reclassification and prediction)\n\nWhat is the goal of the paper?\nThe authors develop a Bayesian hierarchical model for multivariate longitudinal data to predict health status, trajectories, and intervention effects at the individual level in the PCORI mission to address questions about health status from patients and clinicians.\nWhy is it important?\nHealthcare data (DNA sequences, functional images of the brain, patient-reported outcomes, and electronic health records with patients’ sequences of health measurements, diagnoses, and treatments) are complex, and the standard approaches are not adequate for clinical data analysis. Electronic health records (EHRs) could improve diagnostic accuracy and predict treatment effects. Visualizations of characteristics of posterior distributions can be immediately understood by clinicians and patients as relevant to their decision. Combining prior knowledge and patient data with evidence could predict the patient’s health status, trajectory, and/or likely benefits of interventions.\nHow is it solved?\nMethod: The authors applied Bayesian hierarchical regression for multivariate longitudinal patient data using open-source R-packages and developed 2 levels—time within person and persons within a population\nThe model combined exogenous (eg, age, clinical history) factors and endogenous (eg, current treatment) variables on the individual’s multivariate health measurements and the effects of health measurements at one time on subsequent interventions.\nThe model produced an estimate of the posterior distribution for each value of the predictor variable and an estimate of the marginal distribution of the regression coefficients for each coefficient that measures the outcome (health status) associated with its predictor variables. In a larger sample, the likelihood dominates the prior distribution for regression coefficients and Bayesian hierarchical model used a likelihood-based approach, used priors (prior laboratory and clinical trials data) that provided the assay sensitivities, which through the prior assumptions, made the model identifiable and the integration of Markov chain Monte Carlo (MCMC) estimates the posterior distributions, avoided missing data and complex outcome measurements.\nResults\nThree case studies: pneumonia etiology in children, prostate cancer, and mental disorders chosen for model development, identified low-risk patient population, reduced the risk of overtreatment, complications, adverse effects, and financial burden for patients (Disease Reclassification). Prostate cancer software was then implemented within the JHM HER.\nLimitation:\nModels were entirely parametric, and extensions to nonparametric or more flexible parametric models were recommended to improve approaches for neuroimage or genomic data.\nneuroimage or genomic data.\nApplications:\n\nto scale a tool that addresses a particular unmet need across a larger, more diverse population of patients and clinicians\nuse in autoimmune diseases, sudden cardiac arrest, and diabetes.\nembed a collection of tools to acquire and use the most relevant information, agnostic to its level of measurement, to improve population and individual health decisions that cause better outcomes at more affordable costs. @Zeger2020\n\n\nBayesian Inference (parametric vs non-parametric)\n\nWhat is the goal of the paper?\nThe authors calculated the posterior probability of disease diagnosis and applied Bayesian inference to develop three modules comparing parametric (with a fixed set of parameters) and nonparametric distributions (which do not make a priori assumptions) by analyzing the National Health and Nutrition Examination Survey dataset from two separate diagnostic tests on both diseased and non-diseased populations.\nWhy is it important?\nMedical diagnosis, treatment, and management decisions are crucial, and conventional methods for diagnosis using clinical criteria and fixed numerical thresholds limit the capture of other information related to the intricate relationship between diagnostic tests and the varying prevalence of diseases. The probability distributions associated with quantitative diagnostic test outcomes often demonstrate some overlap between the diseased and nondiseased groups. The dichotomous method fails to capture the complexity and heterogeneity of disease presentations across diverse populations. The applicability of the normal distribution (conventional method) is critiqued, especially in dealing with clinical measurands having skewness, bimodality, or multimodality.\nHow is it solved?\nMethods: The Authors developed models employing Bayesian inference (Bayesian diagnostic approach) to calculate the posterior probability of disease diagnosis in the Wolfram Language and integrated prior probabilities of disease with distributions of diagnostic measurands in both diseased and nondiseased populations. The approach enabled the evaluation of combined data from multiple diagnostic tests and improved the diagnostic accuracy, precision and adaptability, The model showed flexibility, adaptability, and versatility in the diagnostic.\nResults\nNonparametric Bayesian models tend to fit data distributions better, especially given limited existing literature, and are more robust in capturing complex data patterns.\nThese models produce multimodal probability patterns for disease, unlike the bimodal, double-sigmoidal curves seen with parametric models.\nLimitations\n\nReliance on parametric models: A need to extend to nonparametric or more flexible parametric models for medical data.\nLimited scholarly publications and over-dependence on prior probabilities increase uncertainties, resulting in broader confidence intervals for posterior probabilities. Systemic bias (unrepresentative datasets) compromises the accuracy of Bayesian calculations. For Incomplete datasets, Bayesian methods combined with other statistical and computational techniques could enhance diagnostic capabilities.\nThe foundational data is crucial to compare new diagnostic measurements. Absence of normative data compromises the reliability and validity of Bayesian diagnostic methods. @Chatzimichail2023\n\n\nBayesian model stages, development and advantages, and temporal models\nWhat is the goal of the paper?\n\nThe study describes the stages of Bayesian analysis, specifying the importance of the priors, data modeling, inferences, model checking and refinement, selecting a proper sampling technique from a posterior distribution, variational inferences, variable selection and its application across various research fields. The study proposes strategies for reproducibility and reporting standards, outlining an updated WAMBS (when to Worry and how to Avoid the Misuse of Bayesian Statistics) checklist and outlining the impact of Bayesian analysis on artificial intelligence in the future.\nWhy is it important?\nBayesian statistics is suitable for quantitative researchers accross different fields who have at least some knowledge of regression modelling.\nHow is it solved?\nExamples of successful applications of Bayesian analysis across various research fields (social sciences, ecology, genetics, medicine) and the advantages and disadvantages of the Bayesian model are provided here, and overview of the current and future use of Bayesian statistics.\nThe study mention priors into three categories (informative, weakly informative and diffuse) based on the degree of (un)certainty (hyperparameters) surrounding the population parameter. The prior distribution is as - N( μ0 , σ^ 20) where a larger variance represents a greater amount of uncertainty surrounding.\nPrior elicitation (experts, generic expert, data-based, sample data using maximum likelihood or sample statistics, etc) construct a prior distribution.\nPrior sensitivity analysis of the likelihood helps examine different forms of the model, assesses how the priors and the likelihood align and have an impact on posterior estimates, reflecting variations not captured by the prior or the likelihood alone.\nPrior estimation allows data-informed shrinkage, enacts regularization or influence algorithms towards a likely high-density region, and improves estimation efficiency.\nKnowing the exact probabilistic specification of the priors for a complex model with smaller sample sizes is important. A small sample conveys less information compared to the priors that quantify the strength of support the observed data lends to possible value(s) for the unknown parameter(s).\nFrequentists do not consider the probability of the unknown parameters as useful, and they are considered to be fixed; the likelihood is the conditional probability distribution p(y|θ) of the data (y), given fixed parameters (θ). In Bayesian inference, unknown parameters (random variables) have varied values, while the (observed) data have fixed values, and the likelihood is a function of θ for the fixed data y.\nTherefore, the likelihood function summarizes a statistical model that stochastically generates a range of possible values for θ and the observed data y. With priors and the likelihood of the observed data, the resulting posterior distribution provides an estimate of the unknown parameters, capturing the primary factors and improving our understanding. Monte Carlo technique provides integrals of sampled values from a given distribution through computer simulations. The packages BRMS and Blavaan in R are used for the probabilistic programming language Stan.\nVariable selection after checking correlations among the variables in the model (Eg: gene-to-gene interaction) aids in the prediction of genes in biomedical research (genome-wide association studies).\nSpatial and temporal variability are factored in Bayesian general linear models. A posterior distribution can simulate new data conditional on this distribution, assess, and provide valid predictions to be used for extrapolating to future events.\nResults\nThe Bayesian approach analyzes large-scale cancer genomic data, identifies novel molecular changes in cancer initiation and progression, the interactions between mutated genes and captured mutational signatures, highlighting key genetic interactions components, allowing genomic-based patient stratification both in clinical trials, in the personalized use of therapeutics, and in understanding cancer and its evolutionary processes.\nLimitations:\n\nIn temporal models, posterior inference challenges are inherent in the spatial and/or temporal dependencies (autocorrelation of parameters over time). @VandeSchoot2021\n\n\nBayesian Normal linear regression\n\nWhat is the goal of the paper?\nThe author provides guidance on Bayesian inference by performing Bayesian Normal linear regression in metrology to calibrate instruments and to evaluate inter-laboratory comparisons in determining fundamental constants.\nWhy is it important?\nThe measurement errors are assumed to be additive, independent, and identically distributed according to a Gaussian distribution with mean zero and variance σ2, which is usually unknown.\nRegression is used to calibrate instruments, evaluate inter-laboratory comparisons, or determine fundamental constants, but the regression model cannot be uniquely formulated as a measurement function. Guide to the Expression of Uncertainty in Measurement (GUM) and its supplements are not applicable directly.\nHow is it solved?\nMethods: Bayesian inference has the advantage of accounting for additional a priori information, which robustifies the analyses.\nThree steps (prior elicitation, posterior calculation, and robustness to prior uncertainty and model adequacy) and model assumptions are critical to Bayesian inference.\nIn Bayesian inference, all unknowns—observables (data) as well as unobservables (parameters and auxiliary variables) are considered to be random, are assigned probability distributions to summarize the available information, and to update prior knowledge about the unobservables with information about them contained in the data. The prior distribution and likelihood function provided by simple graphical displays, sensitivity analyses, or model checking enhance the elicitation and interpretation process.\nFor Normal linear regression problems\n(1) a family of prior distributions for θ and σ2 is (Normal inverse Gamma (NIG) distribution to a posterior from the same family of (NIG) distributions or\n(2) alternative families of prior distributions (hierarchical priors) assign an additional layer of distributions to uncertain prior parameters or non-para- metric priors.\nThe NIG prior with known variance σ2 of observations is a conjugate prior distribution. Vague or non-informative prior distributions can be derived from the NIG prior.\nBayesian inference is influenced by\n\nthe uncertainty in the transformation of prior knowledge to prior distributions\nthe assumptions of the statistical model\nthe mistakes in data acquisition\n\nResults\nThe knowledge from related previous experiments (Normal inverse Gamma distributions) allow for analytic posterior calculations of many quantities of interest. @Klauenberg2015\n\nBayesian linear regression and priors (exchangeable and unexchangeable)\n\nWhat is the goal of the paper?\nThe study developed a test of a formal method for augmenting data in linear regression analyses, by incorporating both exchangeable and unexchangeable information on regression coefficients (and standard errors) of previous studies.\nWhy is it important?\nThe frequent combination of multiple testing has relatively low statistical power, which is problematic in null-hypothesis significance testing. Linear regression analyses do not account for the published results and summary statistics from similar previous studies. Ignoring information on parameters from previous studies (relevant and readily available), affects the stability and precision of the parameter estimates and results in lower values than they could have been, resulting in conclusions that are less certain and are affected by sampling variation.\nMultiple linear regression with separate significance tests for all regression coefficients, and with the modest sample sizes, different studies find different sets of statistically significant predictors, and addressing the issue on larger samples is practically unrealistic.\nHow is it solved?\nMethods: Bayesian linear regression accommodates prior knowledge, overcoming the absence of formal studies, handles the issues of increasing the sample size, and augments the data of a new study with previous results (regression coefficients and standard errors) from similar studies.\nThe authors used Bayesian linear regression to solve the issue of the univariate case analysis by combining evidence of specific predictors from different linear regression analyses (as in meta-analysis) and found it a better method to resolve the issue of simultaneously combining multiple regression parameters per study, which ignored the relationship between the regression coefficients. Includes summary statistics from previous studies, Bayesian linear regression provided a more acceptable solution when data from previous studies were not (realistically) obtainable.\nModels in the study as categorized the model into - (1) Exchangable - when the current data and previous studies have the same set of predictors. (2) Unexchangable – when the predictors were different in the two.\nTo yield the posterior density that reflects the updated knowledge about the model parameters after having observed the data, the steps to Bayesian linear regression steps are mentioned -\n(1) To calculate the probability density function for the data given the unknown model parameters;\n(2) Taken as a function of those model parameters, the likelihood function is the second part of the prior density function of the model parameters. It quantifies what is assumed to be known about the model parameters before observing the data. The study applied the Standard multiple linear regression model, and with the integration of the prior, provided the joint posterior density using the Gibbs sampler. An Ordinary Least Squares linear regression was then applied to each of these samples to obtain the estimated regression coefficients B and the corresponding standard errors.\n(3) A hierarchical model version was developed in analyzing parameters where studies are not exchangeable.\nResults\nIncorporating priors from previous studies in a linear regression on new data yielded a significantly better parameter estimate with an adequate approximation.\nThe gains in comparison to using just the new data, and the large effects were obtained when the data from previous studies were available, resulting in encouraging performance.\nPerformance of the two versions (exchangeable and unexchangeable) of the replication model was consistently superior to using current data alone.\nThe model developed in the article offers the possibility of obtaining significantly better parameter estimates in a linear regression setting without needing to expend a prohibitive amount of time and energy to obtain data from the previous studies.\nHierarchical version (unexchangeable) of the model offers the advantage of being able to address questions about differences between studies and thus allows for explicit testing of the exchangeability assumption.\nLimitations:\n\nAll studies need to have the same set of predictors.\nThe issue of predictor variables that are correlated. @DeLeeuw2012a\n\n\nBayesian logistic regression (Sequential clinical reasoning approach)\n\nWhat is the goal of the paper?\nThe study aimed to develop models using a longitudinal prospective cohort to predict the risk of incident cardiovascular disease by incorporating demographic features (basic), six metabolic syndrome components (metabolic score), and conventional risk factors (enhanced model). The study participants free of CVD at baseline were followed up over five years, and a Bayesian clinical reasoning model was applied to diagnose new CVD cases.\nWhy is it important?\nEarly diagnosis, prevention, by identifying subjects under the high-risk category for cardiovascular disease (CVD), impacts health interventions.\nLimited availability of molecular information in clinical practice due to being costly and unavailability affects efficient disease diagnosis. To efficiently identify a high-risk population based on the routinely checked biological markers before doing these expensive molecular testsThe requires an alternative approach to analyze data.\nThe Tailored Framingham Risk Score method, for the purpose, is not sufficient because of the differences present in ethnic groups, location, and socio-economic status, as they require the construction of their own models. Heterogeneity (geographic, ethnic group, variations, and different characteristics of social contextual network) often is unobservable and unmeasurable.\nHow is it solved?\nMethods: The study evaluated subjects enrolled in a Keelung Community-based Integrated Screening (KCIS) Program, for mass screening (20–79 years) in the Keelung city of Taiwan, who were followed for 5 years to identify incident cancers and chronic diseases (cardiovascular disease).\nThe study classified the risk of having incident CVD cases or death from CVD by dint of available calculated standardized risk score of the MetS components (fasting glucose, blood pressure, HDL-C, triglyceride and waist circumference) together with conventional risk factors (gender, heredity, smoking, alcohol drinking, family history of parent’s CVD and betel quid and other relevant factors).\nEmulating a clinician’s evaluation process, the Bayesian clinical reasoning approach in a sequential manner was applied and three models were developed. The approach considered the normal distribution of regression coefficients of all predictors, allowing for uncertainty of clinical weights. The credible intervals of predicted risk estimates were obtained by averaging out. In the model, the individual risk is elicited by prior speculation (first impression) that is updated by objective observed data (patient’s history and laboratory findings), the regression coefficients for computing risk score were treated as random variable with a certain statistical distribution (e.g. normal distribution) rather than a fixed value (traditional risk prediction model by frequentist). The updated prior distribution with the likelihood of the current data provided a posterior distribution to predict the risk for a specific disease. The sequential approach included -\n\nThe (basic) basic model developed via logistic regression used prior information constructed on gender, age, age2, and time period.\nThe second (classical) model (metabolic score model: MS model) was based on six MetS components.\nThe third (enhanced model) incorporated information on smoking, drinking, betel-quid, and family history of CVD.\n\nResults\nCompared to the basic model and classical model, the enhanced model had better performance. The proposed models predicted CVD incidence at the individual level by incorporating routine information with a sequential Bayesian clinical reasoning approach. Patients’ background significantly contributes to baseline risk. Even with ecological heterogeneity, the regression model adopts individual characteristics and makes individual risk prediction for the CVD incidence.\nLimitations:\n\nWhether the interactions between age, gender, metabolic score, and other risk factors should be included.\nThe use of an enhanced model should be validated through external validation by applying the proposed models to new subjects not included in the training of the model parameters. @Liu2013"
  },
  {
    "objectID": "Summaries/nm/DATA.html",
    "href": "Summaries/nm/DATA.html",
    "title": "Bayesian Logistic Regression Capstone",
    "section": "",
    "text": "Data Source: NHANES merging the below files make 10175 sample size but have to clean it so maybe we will reduce the sample size 1. DEMO_H.xpt 2. DSQTOT_H.xpt 3. BMX_H.xpt 4. TCHOL_H.xpt 5. CDQ_H.xpt 6. SMQ_H.xpt 7. MCQ_H.xpt"
  },
  {
    "objectID": "Summaries/nm/My articles.html",
    "href": "Summaries/nm/My articles.html",
    "title": "Bayesian Logistic Regression Capstone",
    "section": "",
    "text": "Namita’s Literature"
  },
  {
    "objectID": "Summaries/nm/My articles.html#introduction",
    "href": "Summaries/nm/My articles.html#introduction",
    "title": "Bayesian Logistic Regression Capstone",
    "section": "Introduction",
    "text": "Introduction\n\nBayesian Hierarchical Model (Disease reclassification and prediction)\n\nWhat is the goal of the paper?\nThe authors develop a Bayesian hierarchical model for multivariate longitudinal data to predict health status, trajectories, and intervention effects at the individual level in the PCORI mission to address questions about health status from patients and clinicians.\nWhy is it important?\nHealthcare data (DNA sequences, functional images of the brain, patient-reported outcomes, and electronic health records with patients’ sequences of health measurements, diagnoses, and treatments) are complex, and the standard approaches are not adequate for clinical data analysis. Electronic health records (EHRs) could improve diagnostic accuracy and predict treatment effects. Visualizations of characteristics of posterior distributions can be immediately understood by clinicians and patients as relevant to their decision. Combining prior knowledge and patient data with evidence could predict the patient’s health status, trajectory, and/or likely benefits of interventions.\nHow is it solved?\nMethod: The authors applied Bayesian hierarchical regression for multivariate longitudinal patient data using open-source R-packages and developed 2 levels—time within person and persons within a population\nThe model combined exogenous (eg, age, clinical history) factors and endogenous (eg, current treatment) variables on the individual’s multivariate health measurements and the effects of health measurements at one time on subsequent interventions.\nThe model produced an estimate of the posterior distribution for each value of the predictor variable and an estimate of the marginal distribution of the regression coefficients for each coefficient that measures the outcome (health status) associated with its predictor variables. In a larger sample, the likelihood dominates the prior distribution for regression coefficients and Bayesian hierarchical model used a likelihood-based approach, used priors (prior laboratory and clinical trials data) that provided the assay sensitivities, which through the prior assumptions, made the model identifiable and the integration of Markov chain Monte Carlo (MCMC) estimates the posterior distributions, avoided missing data and complex outcome measurements.\nResults\nThree case studies: pneumonia etiology in children, prostate cancer, and mental disorders chosen for model development, identified low-risk patient population, reduced the risk of overtreatment, complications, adverse effects, and financial burden for patients (Disease Reclassification). Prostate cancer software was then implemented within the JHM HER.\nLimitation:\nModels were entirely parametric, and extensions to nonparametric or more flexible parametric models were recommended to improve approaches for neuroimage or genomic data.\nneuroimage or genomic data.\nApplications:\n\nto scale a tool that addresses a particular unmet need across a larger, more diverse population of patients and clinicians\nuse in autoimmune diseases, sudden cardiac arrest, and diabetes.\nembed a collection of tools to acquire and use the most relevant information, agnostic to its level of measurement, to improve population and individual health decisions that cause better outcomes at more affordable costs. @Zeger2020\n\n\nBayesian Inference (parametric vs non-parametric)\n\nWhat is the goal of the paper?\nThe authors calculated the posterior probability of disease diagnosis and applied Bayesian inference to develop three modules comparing parametric (with a fixed set of parameters) and nonparametric distributions (which do not make a priori assumptions) by analyzing the National Health and Nutrition Examination Survey dataset from two separate diagnostic tests on both diseased and non-diseased populations.\nWhy is it important?\nMedical diagnosis, treatment, and management decisions are crucial, and conventional methods for diagnosis using clinical criteria and fixed numerical thresholds limit the capture of other information related to the intricate relationship between diagnostic tests and the varying prevalence of diseases. The probability distributions associated with quantitative diagnostic test outcomes often demonstrate some overlap between the diseased and nondiseased groups. The dichotomous method fails to capture the complexity and heterogeneity of disease presentations across diverse populations. The applicability of the normal distribution (conventional method) is critiqued, especially in dealing with clinical measurands having skewness, bimodality, or multimodality.\nHow is it solved?\nMethods: The Authors developed models employing Bayesian inference (Bayesian diagnostic approach) to calculate the posterior probability of disease diagnosis in the Wolfram Language and integrated prior probabilities of disease with distributions of diagnostic measurands in both diseased and nondiseased populations. The approach enabled the evaluation of combined data from multiple diagnostic tests and improved the diagnostic accuracy, precision and adaptability, The model showed flexibility, adaptability, and versatility in the diagnostic.\nResults\nNonparametric Bayesian models tend to fit data distributions better, especially given limited existing literature, and are more robust in capturing complex data patterns.\nThese models produce multimodal probability patterns for disease, unlike the bimodal, double-sigmoidal curves seen with parametric models.\nLimitations\n\nReliance on parametric models: A need to extend to nonparametric or more flexible parametric models for medical data.\nLimited scholarly publications and over-dependence on prior probabilities increase uncertainties, resulting in broader confidence intervals for posterior probabilities. Systemic bias (unrepresentative datasets) compromises the accuracy of Bayesian calculations. For Incomplete datasets, Bayesian methods combined with other statistical and computational techniques could enhance diagnostic capabilities.\nThe foundational data is crucial to compare new diagnostic measurements. Absence of normative data compromises the reliability and validity of Bayesian diagnostic methods. @Chatzimichail2023\n\n\nBayesian model stages, development and advantages, and temporal models\nWhat is the goal of the paper?\n\nThe study describes the stages of Bayesian analysis, specifying the importance of the priors, data modeling, inferences, model checking and refinement, selecting a proper sampling technique from a posterior distribution, variational inferences, variable selection and its application across various research fields. The study proposes strategies for reproducibility and reporting standards, outlining an updated WAMBS (when to Worry and how to Avoid the Misuse of Bayesian Statistics) checklist and outlining the impact of Bayesian analysis on artificial intelligence in the future.\nWhy is it important?\nBayesian statistics is suitable for quantitative researchers accross different fields who have at least some knowledge of regression modelling.\nHow is it solved?\nExamples of successful applications of Bayesian analysis across various research fields (social sciences, ecology, genetics, medicine) and the advantages and disadvantages of the Bayesian model are provided here, and overview of the current and future use of Bayesian statistics.\nThe study mention priors into three categories (informative, weakly informative and diffuse) based on the degree of (un)certainty (hyperparameters) surrounding the population parameter. The prior distribution is as - N( μ0 , σ^ 20) where a larger variance represents a greater amount of uncertainty surrounding.\nPrior elicitation (experts, generic expert, data-based, sample data using maximum likelihood or sample statistics, etc) construct a prior distribution.\nPrior sensitivity analysis of the likelihood helps examine different forms of the model, assesses how the priors and the likelihood align and have an impact on posterior estimates, reflecting variations not captured by the prior or the likelihood alone.\nPrior estimation allows data-informed shrinkage, enacts regularization or influence algorithms towards a likely high-density region, and improves estimation efficiency.\nKnowing the exact probabilistic specification of the priors for a complex model with smaller sample sizes is important. A small sample conveys less information compared to the priors that quantify the strength of support the observed data lends to possible value(s) for the unknown parameter(s).\nFrequentists do not consider the probability of the unknown parameters as useful, and they are considered to be fixed; the likelihood is the conditional probability distribution p(y|θ) of the data (y), given fixed parameters (θ). In Bayesian inference, unknown parameters (random variables) have varied values, while the (observed) data have fixed values, and the likelihood is a function of θ for the fixed data y.\nTherefore, the likelihood function summarizes a statistical model that stochastically generates a range of possible values for θ and the observed data y. With priors and the likelihood of the observed data, the resulting posterior distribution provides an estimate of the unknown parameters, capturing the primary factors and improving our understanding. Monte Carlo technique provides integrals of sampled values from a given distribution through computer simulations. The packages BRMS and Blavaan in R are used for the probabilistic programming language Stan.\nVariable selection after checking correlations among the variables in the model (Eg: gene-to-gene interaction) aids in the prediction of genes in biomedical research (genome-wide association studies).\nSpatial and temporal variability are factored in Bayesian general linear models. A posterior distribution can simulate new data conditional on this distribution, assess, and provide valid predictions to be used for extrapolating to future events.\nResults\nThe Bayesian approach analyzes large-scale cancer genomic data, identifies novel molecular changes in cancer initiation and progression, the interactions between mutated genes and captured mutational signatures, highlighting key genetic interactions components, allowing genomic-based patient stratification both in clinical trials, in the personalized use of therapeutics, and in understanding cancer and its evolutionary processes.\nLimitations:\n\nIn temporal models, posterior inference challenges are inherent in the spatial and/or temporal dependencies (autocorrelation of parameters over time). @VandeSchoot2021\n\n\nBayesian Normal linear regression\n\nWhat is the goal of the paper?\nThe author provides guidance on Bayesian inference by performing Bayesian Normal linear regression in metrology to calibrate instruments and to evaluate inter-laboratory comparisons in determining fundamental constants.\nWhy is it important?\nThe measurement errors are assumed to be additive, independent, and identically distributed according to a Gaussian distribution with mean zero and variance σ2, which is usually unknown.\nRegression is used to calibrate instruments, evaluate inter-laboratory comparisons, or determine fundamental constants, but the regression model cannot be uniquely formulated as a measurement function. Guide to the Expression of Uncertainty in Measurement (GUM) and its supplements are not applicable directly.\nHow is it solved?\nMethods: Bayesian inference has the advantage of accounting for additional a priori information, which robustifies the analyses.\nThree steps (prior elicitation, posterior calculation, and robustness to prior uncertainty and model adequacy) and model assumptions are critical to Bayesian inference.\nIn Bayesian inference, all unknowns—observables (data) as well as unobservables (parameters and auxiliary variables) are considered to be random, are assigned probability distributions to summarize the available information, and to update prior knowledge about the unobservables with information about them contained in the data. The prior distribution and likelihood function provided by simple graphical displays, sensitivity analyses, or model checking enhance the elicitation and interpretation process.\nFor Normal linear regression problems\n(1) a family of prior distributions for θ and σ2 is (Normal inverse Gamma (NIG) distribution to a posterior from the same family of (NIG) distributions or\n(2) alternative families of prior distributions (hierarchical priors) assign an additional layer of distributions to uncertain prior parameters or non-para- metric priors.\nThe NIG prior with known variance σ2 of observations is a conjugate prior distribution. Vague or non-informative prior distributions can be derived from the NIG prior.\nBayesian inference is influenced by\n\nthe uncertainty in the transformation of prior knowledge to prior distributions\nthe assumptions of the statistical model\nthe mistakes in data acquisition\n\nResults\nThe knowledge from related previous experiments (Normal inverse Gamma distributions) allow for analytic posterior calculations of many quantities of interest. @Klauenberg2015\n\nBayesian linear regression and priors (exchangeable and unexchangeable)\n\nWhat is the goal of the paper?\nThe study developed a test of a formal method for augmenting data in linear regression analyses, by incorporating both exchangeable and unexchangeable information on regression coefficients (and standard errors) of previous studies.\nWhy is it important?\nThe frequent combination of multiple testing has relatively low statistical power, which is problematic in null-hypothesis significance testing. Linear regression analyses do not account for the published results and summary statistics from similar previous studies. Ignoring information on parameters from previous studies (relevant and readily available), affects the stability and precision of the parameter estimates and results in lower values than they could have been, resulting in conclusions that are less certain and are affected by sampling variation.\nMultiple linear regression with separate significance tests for all regression coefficients, and with the modest sample sizes, different studies find different sets of statistically significant predictors, and addressing the issue on larger samples is practically unrealistic.\nHow is it solved?\nMethods: Bayesian linear regression accommodates prior knowledge, overcoming the absence of formal studies, handles the issues of increasing the sample size, and augments the data of a new study with previous results (regression coefficients and standard errors) from similar studies.\nThe authors used Bayesian linear regression to solve the issue of the univariate case analysis by combining evidence of specific predictors from different linear regression analyses (as in meta-analysis) and found it a better method to resolve the issue of simultaneously combining multiple regression parameters per study, which ignored the relationship between the regression coefficients. Includes summary statistics from previous studies, Bayesian linear regression provided a more acceptable solution when data from previous studies were not (realistically) obtainable.\nModels in the study as categorized the model into - (1) Exchangable - when the current data and previous studies have the same set of predictors. (2) Unexchangable – when the predictors were different in the two.\nTo yield the posterior density that reflects the updated knowledge about the model parameters after having observed the data, the steps to Bayesian linear regression steps are mentioned -\n(1) To calculate the probability density function for the data given the unknown model parameters;\n(2) Taken as a function of those model parameters, the likelihood function is the second part of the prior density function of the model parameters. It quantifies what is assumed to be known about the model parameters before observing the data. The study applied the Standard multiple linear regression model, and with the integration of the prior, provided the joint posterior density using the Gibbs sampler. An Ordinary Least Squares linear regression was then applied to each of these samples to obtain the estimated regression coefficients B and the corresponding standard errors.\n(3) A hierarchical model version was developed in analyzing parameters where studies are not exchangeable.\nResults\nIncorporating priors from previous studies in a linear regression on new data yielded a significantly better parameter estimate with an adequate approximation.\nThe gains in comparison to using just the new data, and the large effects were obtained when the data from previous studies were available, resulting in encouraging performance.\nPerformance of the two versions (exchangeable and unexchangeable) of the replication model was consistently superior to using current data alone.\nThe model developed in the article offers the possibility of obtaining significantly better parameter estimates in a linear regression setting without needing to expend a prohibitive amount of time and energy to obtain data from the previous studies.\nHierarchical version (unexchangeable) of the model offers the advantage of being able to address questions about differences between studies and thus allows for explicit testing of the exchangeability assumption.\nLimitations:\n\nAll studies need to have the same set of predictors.\nThe issue of predictor variables that are correlated. @DeLeeuw2012a\n\n\nBayesian logistic regression (Sequential clinical reasoning approach)\n\nWhat is the goal of the paper?\nThe study aimed to develop models using a longitudinal prospective cohort to predict the risk of incident cardiovascular disease by incorporating demographic features (basic), six metabolic syndrome components (metabolic score), and conventional risk factors (enhanced model). The study participants free of CVD at baseline were followed up over five years, and a Bayesian clinical reasoning model was applied to diagnose new CVD cases.\nWhy is it important?\nEarly diagnosis, prevention, by identifying subjects under the high-risk category for cardiovascular disease (CVD), impacts health interventions.\nLimited availability of molecular information in clinical practice due to being costly and unavailability affects efficient disease diagnosis. To efficiently identify a high-risk population based on the routinely checked biological markers before doing these expensive molecular testsThe requires an alternative approach to analyze data.\nThe Tailored Framingham Risk Score method, for the purpose, is not sufficient because of the differences present in ethnic groups, location, and socio-economic status, as they require the construction of their own models. Heterogeneity (geographic, ethnic group, variations, and different characteristics of social contextual network) often is unobservable and unmeasurable.\nHow is it solved?\nMethods: The study evaluated subjects enrolled in a Keelung Community-based Integrated Screening (KCIS) Program, for mass screening (20–79 years) in the Keelung city of Taiwan, who were followed for 5 years to identify incident cancers and chronic diseases (cardiovascular disease).\nThe study classified the risk of having incident CVD cases or death from CVD by dint of available calculated standardized risk score of the MetS components (fasting glucose, blood pressure, HDL-C, triglyceride and waist circumference) together with conventional risk factors (gender, heredity, smoking, alcohol drinking, family history of parent’s CVD and betel quid and other relevant factors).\nEmulating a clinician’s evaluation process, the Bayesian clinical reasoning approach in a sequential manner was applied and three models were developed. The approach considered the normal distribution of regression coefficients of all predictors, allowing for uncertainty of clinical weights. The credible intervals of predicted risk estimates were obtained by averaging out. In the model, the individual risk is elicited by prior speculation (first impression) that is updated by objective observed data (patient’s history and laboratory findings), the regression coefficients for computing risk score were treated as random variable with a certain statistical distribution (e.g. normal distribution) rather than a fixed value (traditional risk prediction model by frequentist). The updated prior distribution with the likelihood of the current data provided a posterior distribution to predict the risk for a specific disease. The sequential approach included -\n\nThe (basic) basic model developed via logistic regression used prior information constructed on gender, age, age2, and time period.\nThe second (classical) model (metabolic score model: MS model) was based on six MetS components.\nThe third (enhanced model) incorporated information on smoking, drinking, betel-quid, and family history of CVD.\n\nResults\nCompared to the basic model and classical model, the enhanced model had better performance. The proposed models predicted CVD incidence at the individual level by incorporating routine information with a sequential Bayesian clinical reasoning approach. Patients’ background significantly contributes to baseline risk. Even with ecological heterogeneity, the regression model adopts individual characteristics and makes individual risk prediction for the CVD incidence.\nLimitations:\n\nWhether the interactions between age, gender, metabolic score, and other risk factors should be included.\nThe use of an enhanced model should be validated through external validation by applying the proposed models to new subjects not included in the training of the model parameters. @Liu2013"
  },
  {
    "objectID": "Summaries/et/paper1.html",
    "href": "Summaries/et/paper1.html",
    "title": "Bayesian Nonparametric Regression for Healthcare Claims Modeling",
    "section": "",
    "text": "Bayesian Nonparametric Regression for Healthcare Claims Modeling\nRichardson, R., & Hartman, B. (2018). Bayesian nonparametric regression models for modeling and predicting healthcare claims. Insurance, Mathematics & Economics, 79, 1-13.\n\n\nSummary\nThis paper introduces Bayesian nonparametric regression models to effectively model and predict healthcare claims data, which often exhibit complex characteristics such as skewness, heavy tails, and excess zeros. The authors propose using Dirichlet process mixtures to flexibly capture the underlying distribution of healthcare claims without assuming a specific parametric form. The models are designed to handle the unique features of healthcare claims data, including the presence of many zero claims (non-claimants) and the variability in claim amounts among claimants.\nThe authors apply their models to a real-world dataset of healthcare claims, demonstrating the models’ ability to accurately predict claim amounts and identify high-risk individuals. The Bayesian framework allows for the incorporation of prior information and provides a coherent way to quantify uncertainty in predictions. The results show that the proposed nonparametric regression models outperform traditional parametric models in terms of predictive accuracy and flexibility.\n\n\nProblem\n\nStandard regression limitations\n\nInsufficient for complex relationships in healthcare claims data\nAssumes Gaussianity, independence, linearity\nAssumptions often not met in insurance data\nDifficulty in capturing skewness, heavy tails, outliers, and bimodality present in claims data\n\n\n\n\nSolution\n\nBayesian nonparametric regression models\n\nFlexible regression model\nRelaxes Normality and linearity assumptions\nPoweful tool for non-Gaussian densities\nIncreased predictive accuracy\nHandles complex error distribution characteristics\nApplicable to all insurance regression problems\n\n\n\n\nMethodology\n\nDependent Dirichlet Process (DDP) ANOVA Model\n\nDirichlet Process (DP) (3.1)\n\nStandard building block for Bayesian nonparametric models\nConstructed via stick-breaking process\nDiscrete distribution with countably infinite atoms\nDP mixture of normals for continuous settings\nAllows for infinite mixture models\n\nDDP (3.2)\n\nBasis for fully nonparametric regression model\nPrior on space of random probability measures\nPoint masses are realizations of stochastic processes\nInfinite mixture of Gaussian processes\n\nDDP ANOVA model (3.3)\n\nExtends DDP to include covariate information\nAtoms are regression coefficients and covariance\nFlexible relationships and error structure\nUses Gibbs sampling for posterior inference\n\n\n\n\n\nApplication\n\nHealthcare claims by ETG\n\nDataset: Episode Treatment Groups (ETG) from a large health insurer\nETG\n\nClassification system for medical conditions\nPredicts healthcare costs\nUncertainty in cost predictions is important\n\nCovariates\n\nage, gender, healthcare charges\n\nFocus on prediction of new observations\n\n\n\n\nResults and performance\n\nOutperforms standard linear model\nOutperforms Generalized Beta 2 (GB2) regression (all but 11 of 347 ETGs)\nBetter predictive accuracy\nDIC (Deviance Information Criterion) consistently lower for BNP\nLower averarge CRPS for out-of-sample predictions\nAccurately captures tail behavior (useful for reserving/risk assessment)\n\n\n\nCase studies\n\nConjunctivitis\n\nLarge dataset (160,000+ observations)\nDistribution is highly non-Gaussian, bimodal\nBNP captures gender effect, standard models fail\nLowest DIC (10,600 vs 12,600 for BLM and 12,300 for GB2)\nAccurately predicts extra mode in left tail\n\nLung transplant\n\nSmaller dataset\nData skewed, Gaussian assumption inappropriate\nBNP/GB2 predict thicker tail for outliers\nGB2 slightly better DIC (1002 vs 1005 for BNP, and 1080 for BLM)\nLess advantage from flexible models in smaller datasets\n\n\n\n\nLimitations\n\nBNP more computationally intensive than standard linear regression\nBut BNP regression scales at the same reate as linear regression\n\n\n\nConclusion\n\nPowerful tool for healthcare claims modeling\nVastly outperforms linear and GB2 regression\nUseful for reserving due to improved distributional fit\nDDP ANOVA useful for continuous independent variables\nCan be extended(e.g. covariate-dependent weights)\nAvailable in R package DPpackage"
  },
  {
    "objectID": "Summaries/et/paper4.html",
    "href": "Summaries/et/paper4.html",
    "title": "Bayesian Logistic Regression Capstone",
    "section": "",
    "text": "What is BART-Survival?\n\nIt’s a Python software package developed by the CDC.\nThe package is for survival analysis: studying time-to-event data (how long until something happens, or doesn’t happen).\nIt uses a machine learning method called Bayesian Additive Regression Trees (BART).\n\nHow does it work (in simple terms)? - “Discrete-time” means it splits up time into chunks (for example, weeks, months, etc.). The model checks at each time‐point whether the event has happened yet or not. - BART is “non-parametric”, which means the model doesn’t assume a fixed mathematical form (like linear or exponential) for how risk changes over time. It lets the data itself shape the risk patterns more flexibly. - Because it’s Bayesian, it gives not just a prediction but also measures of uncertainty (how confident the model is) about those predictions.\nWhat makes it useful / better in some cases - Traditional survival analysis (like the Cox model) often assumes certain things (e.g. that hazards are proportional over time). If those assumptions are wrong, the models can be misleading. BART-Survival is more flexible and can perform better when assumptions of older methods fail. - It provides a friendly API, and tools to dive deeper into the model (e.g. inspecting results, uncertainty) when needed.\nWhen / where we might use it - When you have data about when events happen (or don’t happen) and want to model that. For example, time until recovery, time until equipment failure, time until some event in public health etc. - When you suspect that the standard assumptions of simpler survival models may not hold (e.g. that risk doesn’t change proportionally over time). - When you want flexible models that can give you both predictions and uncertainty.\n\nReference\n\nSparapani, R. A., Logan, B. R., McCulloch, R., & Laud, P. W. (2020). Nonparametric survival analysis using Bayesian additive regression trees (BART). Statistics in Medicine, 39(20), 2526-2546. https://doi.org/10.1002/sim.8523\nBART-Survival GitHub Repository"
  },
  {
    "objectID": "Summaries/et/paper2.html",
    "href": "Summaries/et/paper2.html",
    "title": "Bayesian parametric models for survival prediction in medical applications",
    "section": "",
    "text": "Bayesian parametric models for survival prediction in medical applications\nIwan Paolucci, Yuan-Mao Lin, Jessica Albuquerque Marques Silva, Kristy K. Brock & Bruno C. Odisio\nBMC Medical Research Methodology volume 23, Article number: 250 (2023)\nThis research article, published in BMC Medical Research Methodology, introduces and evaluates Bayesian parametric survival models for predicting patient outcomes in medical applications. The authors, Paolucci et al., highlight the advantages of Bayesian models, such as their ability to provide uncertainty measures, require less hyperparameter tuning, and offer a natural mechanism for model updating using Bayes’ rule without needing original training data due to privacy concerns. The study compares these Bayesian models against conventional survival prediction methods like Cox Proportional Hazards and Random Survival Forests, demonstrating comparable performance while exhibiting less overfitting. The article details the mathematical background of these models, their implementation, and presents results from experiments on various public medical datasets to support their utility in personalized medicine.\n\nProblem\nSurvival analysis is crucial in medical research for predicting patient outcomes, such as time until death or disease recurrence. Traditional models like Cox Proportional Hazards (CoxPH) and Random Survival Forests (RSF) have limitations, including assumptions about hazard ratios and challenges with interpretability. Bayesian parametric models offer a promising alternative by providing uncertainty measures, requiring less hyperparameter tuning, and allowing for model updates without needing original training data, which is beneficial for privacy concerns.\nA major gap in current predictive models for medical applications is the lack of a measure of uncertainty associated with predictions, which is crucial for physicians making high-stakes clinical decisions, especially when treatments have differing side effect profiles or costs.\nThe study tackles practical and technical challenges inherent in medical machine learning. Medical datasets are often limited in size due to the subclassification of diseases, making models highly prone to overfitting. Many existing machine learning algorithms also require extensive hyperparameter tuning to implement regularization and prevent this overfitting.\n\n\nMethodology\nThe authors introduce Bayesian parametric survival models, which are based on the assumption that survival times follow a specific statistical distribution (e.g., Exponential, Weibull, Lognormal). These models use Bayesian inference to estimate the parameters of the chosen distribution, allowing for the incorporation of prior knowledge and the quantification of uncertainty in predictions.\n\nBayesian Parametric Survival Models (BPS)\n\nImplemented using the PyMC library in Python.\nModels include Exponential and Weibull distributions.\nParameters are estimated using Linear combinations and neural networks to predict the parameters of the distributions.\n\nTraining\n\nPyMC framework\nBayes Rule for updating (posterior as prior)\n\nEvaluation\n\nPublic Datasets: AIDS Clinical Trials Group (ACTG), German Breast Cancer Study (GBCS), Veteran lung cancer (Veteran), Worcester Heart Attack Study (WHAS), and primary biliary cirrhosis (PBC)\nExperiment 1: Comparison of Bayesian models with CoxPH, RSF, and DeepSurv\nExperiment 2: Model updating vs retraining from scratch\nExperiment 3: Impact of dataset size on model performance\nMetrics: Concordance Index (C-index), Integrated Brier Score (IBS)\nComparison with Cox Proportional Hazards, Random Survival Forests, and DeepSurv\n\n\n\n\nResults\nThe results demonstrate that Bayesian parametric survival models perform comparably to traditional methods like CoxPH and RSF, with some advantages in terms of reduced overfitting and the ability to provide uncertainty estimates. The models also showed robustness when updating with new data, maintaining performance without needing to retrain from scratch.\n\nPerformance comparison\n\nBPS models performed well, no consistent best model\nBPS Wb NN superior for PBC dataset\n\nOverfitting\n\nBPS and CoxPH & Weibull based models overfit least\nDeepSurv & RSF showed more overfitting\n\nUncertainty estimation beneficial for clinical decision-making\n\n\n\nConclusion\n\nBPS models are competitive, robust, and well-suited for medical applications.\nKey advantages: less overfitting, uncertainty quantification, reduced hyperparameter tuning.\nEfficient model updating without original data (preserves privacy).\nLimitations: datasets used in the study are relatively small, computation time not compared."
  }
]