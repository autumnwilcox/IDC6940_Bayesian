---
title: "Bayesian Logistic Regression for Predicting Diabetes Risk"
subtitle: |
  NHANES 2013–2014 Analysis  
  
  **Namita Mishra & Autumn Wilcox**\
  December 13, 2025
format:
  revealjs:
    theme: simple
    slide-number: true
    transition: fade
    incremental: false
    scrollable: true
    toc: false
    code-overflow: wrap
    code-line-numbers: false
    css: styles.css
execute:
  echo: true
  warning: false
  message: false
  eval: false
bibliography: references.bib
editor: visual
---

## Introduction & Aim

::: panel-tabset

### Introduction

- Diabetes is a major U.S. public health concern.
- Key risk factors: **age**, **BMI**, **sex**, **race/ethnicity**.
- Classical logistic regression struggles with:
  - missing data  
  - quasi-separation  
  - small effective sample sizes
- Bayesian logistic regression helps by:
  - stabilizing estimates  
  - incorporating prior information  
  - quantifying full uncertainty
- Commonly applied in diagnostic modeling and NHANES research.

### Aim

- Predict **doctor-diagnosed diabetes** in U.S. adults (NHANES 2013–2014).
- Evaluate **age, BMI, sex, and race/ethnicity** as predictors.
- Incorporate **NHANES complex survey design**.
- Handle missing data using **MICE**.
- Compare **Bayesian** vs **survey-weighted** logistic regression.
- Assess **model stability**, **interpretability**, and **predictive performance**.

:::

---

## Frequentist Limitations (Motivation)

::: panel-tabset

### Why MLE Struggles
- Sensitive to **missing data**  
- Unstable when predictors or outcomes are **sparse**  
- Vulnerable to **quasi-separation** (infinite or extreme coefficients)  
- Depends on **large-sample assumptions** that may not hold in subgroups  

### Missing Data
- Missing BMI/diabetes values reduce effective sample size  
- Complete-case analysis can introduce **bias** and increase **variance**  

### Quasi-Separation
- Occurs when a predictor perfectly (or nearly perfectly) predicts the outcome  
- Causes non-finite or extremely large MLE estimates  

### Confounding
- Limited covariates → residual confounding remains  
- MLE provides only **point estimates**, not full uncertainty  

### Why Bayesian Helps
- Priors stabilize unstable coefficients  
- Posterior distributions quantify **full uncertainty**  
- Performs better with **missingness**, **small samples**, and **separation**  

:::

---

## Bayesian Logistic Regression

::: panel-tabset

### Overview
- Combines **priors + data** → posterior distributions  
- Parameters treated as **random variables**  
- Produces **credible intervals** and probability-based interpretation  
- Handles **uncertainty** and **separation** better than MLE  

### Model Structure
- **Outcome:** diabetes_dx (binary)  
- **Predictors:** age_c, bmi_c, sex, race  
- Logit model:  
  *logit(P(diabetes = 1)) = α + β_age + β_bmi + β_sex + β_race*

### Prior Specification
- **Coefficients:** Normal(0, 2.5) — weakly informative, recommended for logistic regression [@gelman2008]  
- **Intercept:** Student-t(3, 0, 10) — robust, heavy-tailed prior  

### Posterior Predictions
- Posterior draws → predicted diabetes probabilities  
- Captures **parameter + predictive uncertainty**  
- Supports **individual** and **population-level** inference  

### Model Evaluation & Diagnostics
- Sampling via **Stan’s NUTS algorithm** [@stan2020]  
- Convergence checks: R-hat ≈ 1, ESS values, trace plots, autocorrelation  
- Posterior predictive checks to assess model fit  

### Advantages of Bayesian Approach
- Stable under **quasi-separation**  
- Direct probability statements (**credible intervals**)  
- Priors provide **regularization**  
- Full uncertainty propagation  
- Flexible prediction and validation tools  

:::

---

## Study Data

::: panel-tabset

### NHANES Files
- DEMO_H — demographics  
- BMX_H — BMI & anthropometrics  
- DIQ_H — diabetes questionnaire  
Data from NHANES 2013–2014 [@nhanes2013].

### Merge Rules
- Key ID: **SEQN**  
- Files merged on SEQN with harmonized variable types  

### Adult Cohort Definition
- Adults **≥ 20 years**  
- Final analytic sample: **5,769**  
- Outcome: **doctor-diagnosed diabetes (DIQ010)**  

### Weighted Sample Characteristics
- Weighted mean age: **~47.5 yrs**  
- Weighted diabetes prevalence: **~8.9%**  
- Design effect & effective sample size computed  

### Variables & Coding
- **age_c** — standardized age  
- **bmi_c** — standardized BMI  
- **sex** — Male / Female  
- **race** — 5 categories (NH White reference)  

### Survey Design Variables
- **SDMVPSU** — primary sampling units  
- **SDMVSTRA** — strata  
- **WTMEC2YR** — exam weights  

### Final Analytic Sample
- Cleaned dataset with design variables retained  
- Ready for:  
  - complete-case analysis  
  - MICE imputation  
  - Bayesian modeling  

:::

---

## Missingness

::: panel-tabset
### Missingness Overview

-   \~7% overall missingness.

-   92.7% of rows complete.

-   Minimal structural missingness.

### Key Missing Variables

-   BMI: \~4.3% missing

-   diabetes_dx: \~3.1% missing

### MAR Definition

-   Missing At Random (MAR): Missingness depends on other observed variables (e.g., exam attendance), not on the missing value itself.

### Missingness Visualization

![](figs/missingness.png){fig-align="center" width="70%"}

:::

---

## Implementing Design-Based Logistic Regression

::: panel-tabset

### Complete-Case Summary

-   Complete-case sample: n = 5,348.

-   Survey-weighted logistic regression performed using quasibinomial.

### Key Findings

-   Age (per SD ↑): \~3× higher odds of diabetes

-   BMI (per SD ↑): \~1.9× higher odds

-   Female: lower odds vs. Male

-   Certain racial/ethnic groups show higher odds vs NH White

### Survey-Weighted Coefficient Plot

![](figs/svy_or.png){fig-align="center" width="70%"}
:::

---

## Implementing Bayesian Logistic Regression

::: panel-tabset

### Data (Adults)

* Raw imported: **10,175 obs**

* Survey-weighted (filtered = adult): **5,769 (~7%)**

* Complete case analysis: **5,348** (with no missing values)

* **Statistics (Data = Adults)**

* **Bayesian logistic regression is fit on a completed dataset.**

  * In this analysis, missing values were addressed using MICE before model fitting.

* Imputed dataset (adult_imp1): **5,592**

  * **(Raw data → MICE →** imputed dataset **)**

### MICE

**Multivariate Imputation by Chained Equations**

* **Iteratively imputes incomplete variables** using regression models.
* MICE was used to handle missing data using predictive mean matching for continuous variables and logistic models for binary variables [@vanbuuren2011].
* **Rubin’s rule finds pooled estimates** from across posterior distributions to yield the full posterior:
  **Posterior = parameters + missing data uncertainty**

### Bayesian Logistic Regression Model

Model is fitted to each imputed dataset in **R** using brms **(Stan backend)**, **2000 iterations**.

**Bayesian Regression Formula**

***diabetes_dx | weights(wt_norm) ~ age_c + bmi_c + sex + race***

**Dataset**: pooled imputed dataset 1
**Bernoulli family**
**Chains = 4, Iterations = 2000, Seed = 123**
**Priors:**

* **normal(0, 2.5)** for coefficients
* **student_t(3, 0, 10)** for the intercept

### Prior

* **Intercept prior: student_t(3, 0, 10)** (@vandeschot2013)

  * **df = 3 (degrees of freedom):** heavy tails — allow for occasional extreme values.
  * **location = 0:** centered at 0, so the prior expects the parameter to be near 0.
  * **scale = 10:** controls how spread out the distribution is (larger scale → wider spread).
* **Prior for regression coefficients: Normal(0, 2.5)**, a weakly informative default recommended for logistic regression (Gelman, 2008).

  * Weakly informative with **mean = 0, standard deviation = 2.5**.
  * This prior regularizes extreme coefficient estimates, improving stability under sparse data or quasi-separation.

### Code

```r
library(gt)
priors <- c(
  set_prior("normal(0, 2.5)", class = "b"),
  set_prior("student_t(3, 0, 10)", class = "Intercept")
)

bayes_fit <- brm(
  formula = diabetes_dx | weights(wt_norm) ~ age_c + bmi_c + sex + race,
  data    = adult_imp1,
  family  = bernoulli(link = "logit"),
  prior   = priors,
  chains  = 4, iter = 2000, seed = 123,
  control = list(adapt_delta = 0.95),
  refresh = 0
)
```

* `brm()` in the R package **{brms}** is used to fit complex Bayesian models.

:::

---

## MCMC (Markov Chain Monte Carlo)

::: panel-tabset

### Markov Process

MCMC convergence is the point where information from the prior and the data (likelihood) has been fully combined, and the Gibbs sampler has reached the posterior distribution — the chain is now drawing valid samples from that posterior.

* **Gibbs sampler:** a specific algorithm under the MCMC category that produces a sequence of random vectors of parameters.
* **Burn-in period:** the **first 500–1,000 iterations** (early draws) that are **not representative** of the true posterior and are discarded.
* After the burn-in period, the chain reaches *stationarity*.

**Each sample drawn** depends on the previous sample — they are **not independent**, and random vectors are **sequentially dependent**.

### Code

```r
post <- posterior_summary(bayes_fit, robust = FALSE) %>% as_tibble(rownames = "term")

# Extract convergence diagnostics
diag <- rstan::monitor(as.array(bayes_fit$fit), print = FALSE) %>%
  as_tibble(rownames = "term") %>%
  select(term, Rhat, n_eff = Bulk_ESS)

bayes_results <- post %>%
  left_join(diag, by = "term") %>%
  mutate(
    OR     = exp(Estimate),
    OR_LCL = exp(Q2.5),
    OR_UCL = exp(Q97.5)
  ) %>%
  select(term, Estimate, Est.Error, Q2.5, Q97.5, Rhat, n_eff, OR, OR_LCL, OR_UCL)
```

* `rstan::monitor` in the **RStan** package computes summary statistics and diagnoses the convergence and efficiency of MCMC simulations.

  Provides summaries:

* **Posterior mean** and **standard deviation (SD)**

* **Monte Carlo standard error (MCSE)**

* **Quantiles:** 2.5%, 25%, 50%, 75%, 97.5%

* **Effective sample sizes:** Bulk_ESS and Tail_ESS

* **R-hat (convergence diagnostic)**

### Result

**warmup = 1000**
**total draws = 4000**

* **Bulk ESS** → reliability of mean and median estimates
* **Tail ESS** → reliability of extreme quantiles (e.g., 2.5%, 97.5%)

:::

## Model Diagnostics

::: panel-tabset

### MCMC Convergence

Good convergence shows:

* **No drift across iterations**
* **Well-mixed chains** without any trends and with stable posterior estimates
* **R-hat ≈ 1**
* Effective sample size: **~3500** — the MCMC chain has the same information as **3,500 independent samples**

  * Indicates **good mixing**, **low autocorrelation**, and **highly reliable** posterior estimates

### Trace Plots (Stable Posteriors)

![](figs/mcmc_trace.png){fig-align="center" width="70%"}

### Density Plot

![](figs/prior_only_density.png){fig-align="center" width="70%"}

### Autocorrelation Plot

![](figs/acf_age_bmi.png){fig-align="center" width="70%"}

:::

---

## Posterior Predictive Distribution

::: panel-tabset

### Posterior Draws

Posterior draws = **4000 draws** (4 chains × 1000 iterations) via MCMC, giving a posterior distribution of predictors.

* **Conditional distribution**
* **Full probability distribution** of the parameter given the data

  * **Mean and credible intervals** are summaries of that distribution, not the distribution itself
* **Influenced by the prior (prior subjectivity)**
* **Sample size effect:** prior influence decreases as sample size increases

### Plot

![](figs/ppc_density.png){fig-align="center" width="70%"}
* Y-axis: density (frequency of predicted probabilities)

* X-axis: predicted probability of diabetes (0–1)

* Most individuals have low predicted probabilities (near 0), with a smaller peak near 1.

The model closely reproduces the observed data and captures the distribution of diabetes outcomes accurately.

:::

---

## Regression Results (Posterior Estimates and Distribution)

::: panel-tabset

### Posterior Estimates

![](figs/bayes_fit_summary.png){fig-align="center" width="70%"}

### Posterior Distribution

![](figs/mcmc_areas.png){fig-align="center" width="70%"}

### Interpretation

**Posterior Coefficients**

* **Age (per 1 SD):** strong positive effect (≈ +1.1, 80% CrI ≈ 0.9–1.3)
* **BMI (per 1 SD):** positive association (≈ +0.6, 80% CrI ≈ 0.45–0.75)
* **Female:** slightly protective (≈ –0.2, CrI spans slightly below zero)
* **Mexican American:** small positive effect (≈ +0.25, wide CrI)
* **Other Hispanic:** moderate positive effect (≈ +0.35, wide CrI)
* **Non-Hispanic Black:** clear positive effect (≈ +0.75, CrI ≈ 0.6–0.9)
* **Other/Multi:** small positive effect (≈ +0.3, wide CrI)

:::

---

## Compare Prior and Posterior Predictive Distribution (Age and BMI)

::: panel-tabset

### Plot

![](figs/prior_vs_posterior.png){fig-align="center" width="70%"}

### Interpretation 

**Prior and posterior distributions** for the coefficient estimates of *Age (per 1 SD)* and *BMI (per 1 SD)*:

* **Prior distribution:** wide spread (high variance), indicating a high degree of initial uncertainty
* **Posterior distribution:** narrower and taller
* **Data significantly updated the initial beliefs**

:::

---

## Posterior Predictive Checks

::: panel-tabset

### Proportion of Diabetes

We draw **500 samples** of predicted outcomes for each observation and calculate the proportion of outcomes (Diabetes = 1).

![](figs/ppc_bars.png){fig-align="center" width="70%"}

**Fig:** Bar plot comparing counts/frequencies of each category (0 vs 1) in the observed data (`y`) and predictive samples (`y-rep`).

**Bars for observed data fall within the range of simulated data:** the **model predicts the overall diabetes prevalence well**.

### Posterior Distribution of Diabetes

![](figs/posterior_prevalence_hist.png){fig-align="center" width="70%"}

Histogram shows the posterior distribution of the population proportion of diabetes.
**x-axis:** proportion of diabetics (1s)
**y-axis:** frequency of posterior draws

* Highest bar ≈ 0.11 (11%), **symmetric**, centered at ~0.11
* Spread: **95% credible interval ≈ 9%–13%** diabetes prevalence

### Outcome Mean

![](figs/ppc_mean.png){fig-align="center" width="70%"}

### Outcome Standard Deviation 

![](figs/ppc_sd.png){fig-align="center" width="70%"}

:::

---

## Bayesian vs Survey-Weighted NHANES (Diabetes Prevalence)

![](figs/pop_vs_post_prev.png){fig-align="center" width="70%"}

![](figs/pop_vs_post_prev.png){fig-align="center" width="70%"}

* Model mean diabetes prevalence = **10.95%** (95% CI: **8.5%–12.8%**)
* NHANES diabetes prevalence = **8.9%** (SE = 0.0048)

**Slightly higher model mean**, but the credible interval overlaps with NHANES, indicating **reasonable calibration**.

**NHANES lies near the lower end of the posterior distribution but still within a plausible range — the model is well-calibrated.**

---

## Observed vs Predicted in Individuals

::: panel-tabset

### Average Diabetes

![](figs/ppc_scatter_avg.png){fig-align="center" width="center"}

* Each point = one individual
* **x-axis = observed value** (0 or 1)
* **y-axis = average predicted posterior probability** for that individual across simulated datasets
* **Points near (0, 0) or (1, 1) indicate good prediction**

### Mean BMI

* Each point = individual’s observed BMI vs. model-predicted mean
* **Error bars = 95% credible intervals**

**Model predictions generally align with the observed data.**

:::

---

## Compare Models

::: panel-tabset

### Overview

(1) **Survey-weighted maximum likelihood estimation (MLE)**
(2) **Bayesian regression**

```r
svy_tbl   <- svy_or   %>% mutate(Model = "Survey-weighted MLE")
bayes_tbl <- bayes_or %>% mutate(Model = "Bayesian")
```

![](figs/odds_ratio_comparison.png){fig-align="center" width="70%"}

* Both models consistently identified Age and BMI as strong predictors of diabetes.

  * **Bayesian model:** complements the frequentist approach by providing stable, interpretable estimates, quantifying uncertainty, and producing population-level prevalence.
  * **Survey-weighted model:** used normalized weights but does not fully account for stratification, clustering, or design-based variance adjustments.

### Interpretation

* **Minimal shrinkage and similar uncertainty** → reliable and robust model
* **Posterior estimates are slightly pulled toward the prior mean** (often toward zero or more moderate values) compared to classical estimates
* **The prior was weakly informative, exerting only a small influence**
* **Observed data were strong enough to dominate the prior** → Bayesian results remain very similar to classical (survey-weighted)

:::

---

## Pairwise Correlation

::: panel-tabset

### Plot

![](figs/mcmc_pairs.png){fig-align="center" width="70%"}

### Interpretation

* No strong linear relationships between **Age, BMI, and Sex**; each independently contributes to predicting diabetes.
* **Diagonal histograms** are smooth and unimodal, confirming **stable convergence** and well-behaved posterior samples.
* Mild negative correlation between **Age** and **Sex (Female):** as the effect of Age increases, the effect of being female slightly decreases.

**Weak correlations indicate both predictors provide distinct information for the diabetes outcome.**

:::

---

## R Square

![](figs/bayes_R2.png){fig-align="center" width="70%"}

**Interpretation**

* Approximately **13%** of the variability in diabetes status is explained, with credible uncertainty bounds indicating modest explanatory power.
* Predictors (Age, BMI, Sex, etc.) capture some—but not all—factors influencing diabetes risk.
* Other factors (genetics, lifestyle, environment) likely contribute as well.

---

## Assumptions for Bayesian Logistic Regression

* Binary outcome
* Independent observations
* Linear relationship in the logit
* No perfect collinearity
* Priors properly chosen and informative enough
* Posterior is proper and convergent
* No complete separation
* Good predictive checks

---

## Translational Research

::: panel-tabset

### Internal Validation - Individual Participant 

This example evaluates the posterior predictive distribution for a **real participant from the dataset** (“Participant 1”), who has **lower-risk characteristics**.

- **Median predicted probability:** ≈ 0.25  
- **95% Credible Interval:** ≈ 0.20–0.31  

This illustrates how the model estimates personalized risk for an individual with **moderate demographic and clinical risk factors**.

### Participant Data

```r
participant1_data <- adult[1, ]
phat1 <- posterior_linpred(bayes_fit, newdata = participant1_data, transform = TRUE)

post_pred_df <- data.frame(pred = phat1)
ci_95_participant1 <- quantile(phat1, c(0.025, 0.975))

ggplot(post_pred_df, aes(x = pred)) +
  geom_density(color='darkblue', fill='lightblue') +
  geom_vline(xintercept = ci_95_participant1[1], color='red', linetype='dashed') +
  geom_vline(xintercept = ci_95_participant1[2], color='red', linetype='dashed') +
  xlab('Probability of being diabetic (Outcome = 1)') +
  ggtitle('Posterior Predictive Distribution 95% Credible Interval') +
  theme_bw()
```

```r
new_participant <- data.frame(
  age_c = 40,
  bmi_c = 25,
  sex = "Female",
  race = "Mexican American"
)
```

### External Validation — Representative High-Risk Profile

In contrast to Participant 1, this example evaluates a **hypothetical individual** with a **high-risk profile**:

- **age_c = 40**  
- **bmi_c = 25**  
- **sex = "Female"**  
- **race = "Mexican American"**

The posterior predictive distribution for this profile shows **extremely high predicted probability** of diabetes (with most posterior draws near **1.0**).

This demonstrates how the same Bayesian model yields **very different risk predictions** for individuals with **different demographic and clinical characteristics**, and highlights the importance of individualized risk assessment.

![](figs/posterior_pred_dist.png){fig-align="center" width="70%"}

**X-axis:** predicted probability of diabetes (Outcome = 1)
**Y-axis:** density of predicted probabilities
**Blue curve:** posterior predictive distribution
**Shaded area:** high probability values near 1 indicate increased diabetes risk
**Red dashed line:** upper bound of the 95% credible interval

**Peak predicted probability: ≈ 1**

:::

---

## Reverse Prediction

::: panel-tabset

### Predicting BMI

Predicting BMI value for a given individual (diabetic, age = 40, female, Mexican American).
**logit(Diabetes**ᵢ**) = β₀ + β_Ageᵢ + β_BMIᵢ + β_Sexᵢ + β_Raceᵢ**

![](figs/inverse_prediction_bmi.png){fig-align="center" width="70%"}

**X-axis:** BMI values (centered or raw, depending on the model)
**Y-axis:** predicted probability of diabetes for a representative individual
**Blue curve:** predicted probability of diabetes across the BMI range; consistently high (≈1)
**Red horizontal line:** target probability of diabetes = 0.3
**Red vertical line:** targeted BMI where predicted probability is closest to the target (≈18)

### Practical Implications

“**At what BMI does this individual have a 30% chance of being diabetic?**”

*(Diabetic, age = 40, female, Mexican American)*
**This is inverse prediction — the BMI needed to reach a 30% diabetes risk.**

* For this demographic profile, even a relatively low BMI (~18) is associated with a 30% diabetes risk.
* Indicates that other factors (age, sex, race) contribute strongly to predicted diabetes risk.

:::

---

## Limitations

* **Cross-sectional** NHANES design limits causal inference.
* Multiple **imputation** assumes MAR; MNAR may bias results.
* Residual **confounding** from unmeasured factors (diet, activity, SES, genetics).
* Bayesian estimates may be influenced by **prior choices**.
* **BMI measured once**; may not capture long-term exposure.
* **Self-reported** variables may introduce recall or reporting bias.
* **Interaction effects** (e.g., age × BMI) not tested.

---

## Key Findings

- **Age and BMI are the strongest predictors** of doctor-diagnosed diabetes, with consistent effects across both survey-weighted and Bayesian models.
- **The Bayesian model provides stable, well-calibrated estimates**, with credible intervals that closely match the classical model’s uncertainty while avoiding quasi-separation issues.
- **Posterior predictive checks confirm good model fit**, accurately reproducing observed diabetes prevalence and individual-level predictions.
- **Population-level and individualized risk estimates align**, demonstrating that the model can support both public health interpretation and personalized risk assessment.

---

## Conclusion

Bayesian logistic regression effectively models uncertainty.
MICE improved data completeness and reliability.
Posterior predictions provide interpretable probabilities of diabetes risk.
Framework is adaptable to other outcomes (e.g., hypertension, obesity).

---

## References
::: {#refs}
:::

---

## Thank You

**Advisor**  
Dr. Achraf Cohen  

**Presented by**  
Namita Mishra  
Autumn S. Wilcox  

**University of West Florida**

### Questions?
