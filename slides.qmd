---
title: "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013–2014)"
subtitle: "Capstone Presentation"
author:
  - "Namita Mishra"
  - "Autumn Wilcox"
advisor: "Dr. Ashraf Cohen"
date: '`r Sys.Date()`'
format:
  revealjs:
    theme: simple
    slide-number: true
    transition: slide
bibliography: references.bib
execute:
  warning: false
  message: false
  echo: false
---

```{r}
#| include: false
# Global libs & theme for all slides
library(ggplot2)   # ggplot()
library(dplyr)     # %>% and verbs (mutate, count, etc.)
library(viridis)   # colorblind-friendly palettes

theme_set(theme_minimal(base_size = 13))
custom_colors <- viridis(3, option = "D")
```

## Our Question

- Can Bayesian logistic regression provide **more stable** and **transparent** inference than classical MLE for diabetes-related outcomes in **NHANES 2013–2014**?

---

## Data Pipeline (Reproducible)

- Source: NHANES 2013–2014  
- Files: **BMX_H**, **DEMO_H**, **DIQ_H**  
- Preprocessing:in **`R/data_prep.R`** → creates **`merged_2013_2014.rds`**

```{r}
if (!file.exists("data/merged_2013_2014.rds")) {
  source("R/data_prep.R")
}
merged_data <- readRDS("data/merged_2013_2014.rds")
dim(merged_data)
```

---

## Predictors

- BMI category (`BMDBMIC`)
- Age (`RIDAGEYR`)
- Sex (`RIAGENDR`)
- Race/Ethnicity (`RIDRETH1`)

## Survey Design

- weights (`WTMEC2YR`)
- PSU (`SDMVPSU`) 
- strata (`SDMVSTRA`)

## Outcome

- `DIQ240` — a proxy indicator of diabetes status.

---

## Quick EDA: Age Table

```{r}
knitr::kable(head(merged_data[, c("SEQN", "BMDBMIC", "RIDAGEYR")]))
```

## Quick EDA: Age Distribution 

```{r}
ggplot(merged_data, aes(x = RIDAGEYR)) +
  geom_histogram(binwidth = 5, boundary = 0, closed = "left",
                 fill = custom_colors[1], color = "white") +
  labs(title = "Age Distribution (NHANES 2013–2014)",
       x = "Age (years)", y = "Count")
```

## Quick EDA: BMI Category Codes

```{r}
merged_data %>%
  mutate(BMDBMIC = factor(BMDBMIC, exclude = NULL)) %>%
  count(BMDBMIC) %>%
  ggplot(aes(x = BMDBMIC, y = n)) +
  geom_col(fill = custom_colors[2], color = "black") +
  labs(title = "Counts by BMDBMIC (BMI Category Code)",
       x = "BMDBMIC (code; NA common for adults)", y = "Count")
```

## Survey Design 

```{r}
library(survey)
nhanes_design <- svydesign(
  id = ~SDMVPSU,
  strata = ~SDMVSTRA,
  weights = ~WTMEC2YR,
  nest = TRUE,
  data = merged_data
)
svymean(~RIDAGEYR, nhanes_design, na.rm = TRUE)
```

## Methods

We applied four approaches:

1. **MLE Logistic Regression** – unstable due to quasi-separation.
2. **Firth Regression** – finite estimates with tiny complete-case sample.
3. **MICE + MLR** – stable across ~9,813 cases but poor fit.
4. **Bayesian Logistic Regression** – priors regularize, stable posteriors, uncertainty fully captured.

---

## Modeling

```{r}
library(brms)

priors <- c(
  set_prior("normal(0,2.5)", class="b"),
  set_prior("normal(0,5)", class="Intercept")
)

bayes_fit <- brm(
  DIQ240 ~ BMDBMIC + RIDAGEYR + RIAGENDR + RIDRETH1,
  data = merged_data,
  family = bernoulli(link="logit"),
  prior = priors,
  chains = 4, iter = 2000, seed = 123,
  control = list(adapt_delta = 0.95)
)
```

## Results

- **MLE:** collapsed; unstable estimates.
- **Firth:** finite but based on N = 14 complete cases.
- **MICE + MLR:** stable across ~9,813; Hosmer-Lemeshow misfit (p < 0.001).
- **Bayesian:** stable, interpretable ORs with credible intervals; best handling of missingness + separation.

---

## Discussion

- **Best approach:** MICE + Bayesian logistic regression
- **Priors:** weakly-informative + literature-informed (Ali 2024)
- **Limitations:** subjective priors, computation time, proxy outcome (`DIQ240`)

---

## References
::: {.refs}
:::