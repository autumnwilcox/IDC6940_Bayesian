---
title: "Bayesian Logistic Regression for Predicting Diabetes Risk"
subtitle: |
  NHANES 2013–2014 Analysis  
  
  **Namita Mishra & Autumn Wilcox**\
  December 13, 2025
format:
  revealjs:
    theme: simple
    slide-number: true
    transition: fade
    incremental: false
    scrollable: true
    toc: false
    code-overflow: wrap
    code-line-numbers: false
    css: styles.css
execute:
  echo: true
  warning: false
  message: false
  eval: false
bibliography: references.bib
editor: visual
---

## Introduction & Aim

::: panel-tabset
### Introduction

-   Diabetes is a major U.S. public health burden.

-   Key predictors include age, BMI, sex, and race/ethnicity.

-   Traditional (frequentist) logistic regression can become unstable with missing data, quasi-separation, or small effective sample sizes.

-   Bayesian logistic regression offers a solution by incorporating prior information, stabilizing estimates, and fully quantifying uncertainty.

-   This framework is widely used in diagnostic modeling, metabolic risk assessment, and NHANES analyses.

### Aim

-   Predict doctor-diagnosed diabetes in U.S. adults using NHANES 2013–2014 data.

-   Evaluate age, BMI, sex, and race/ethnicity as predictors.

-   Account for complex NHANES survey design.

-   Address missing data using MICE.

-   Compare Bayesian logistic regression with classical survey-weighted logistic regression.

-   Assess model stability, interpretability, and predictive performance.
:::

---

## Frequentist Limitations (Motivation)

::: panel-tabset
### Why MLE struggles

-   Sensitive to missing data.

-   Unstable estimates when outcome or predictors are sparse.

-   Quasi-separation causes infinite or extremely large coefficients.

-   Relies on large-sample assumptions that may not hold for subgroup analyses.

### Missing Data

-   Missing BMI and diabetes variables reduce effective sample size.

-   Complete-case analysis can bias results and increase variance.

### Quasi-separation

-   Occurs when a predictor perfectly or almost perfectly predicts the outcome.

-   Leads to unstable or non-finite MLE estimates.

### Confounding

-   Residual confounding persists with limited covariates.

-   MLE provides only point estimates without full uncertainty propagation.

### Why Bayesian is needed

-   Priors regularize unstable coefficients.

-   Posterior distributions summarize full uncertainty.

-   Better performance with missingness, separation, and small effective sample sizes.
:::

---

## Bayesian Logistic Regression

::: panel-tabset
### Overview

-   Combines priors + data → posterior distributions.

-   Treats parameters as random variables.

-   Produces credible intervals and probability-based interpretation.

-   Handles uncertainty and separation more effectively than MLE.

### Model Structure

-   Outcome: diabetes_dx (binary).

-   Predictors: age_c, bmi_c, sex, race/ethnicity.

-   Logit model:

    logit(P(diabetes = 1)) = α + β_age + β_bmi + β_sex + β_race.

### Prior Specification

-   Coefficients: Normal(0, 2.5) → weakly informative.

-   Intercept: Student-t(3, 0, 10) → robust, heavy-tailed.

### Posterior Predictions

-   Posterior draws produce predicted diabetes probabilities.

-   Captures parameter + predictive uncertainty.

-   Supports individual-level and population-level inference.

### Model Evaluation & Diagnostics

-   Stan’s NUTS sampler.

-   Convergence checks: R-hat ≈ 1, ESS values, trace plots, autocorrelation.

-   Posterior predictive checks for model fit.

### Advantages of Bayesian approach

-   Stable estimates under separation.

-   Direct probability statements (credible intervals).

-   Regularization via priors. • Full uncertainty propagation.

-   Flexible prediction and validation tools.
:::

---

## Study Data

::: panel-tabset
### NHANES Files

-   DEMO_H (demographics)

-   BMX_H (anthropometrics, BMI)

-   DIQ_H (diabetes questionnaire)

### Merge Rules

-   Key variable: SEQN (unique participant ID).

-   Merged datasets using SEQN with harmonized variable types.

### Adult Cohort Definition

-   Restricted to adults ≥20 years.

-   Final analytic sample: 5,769 adults.

-   Outcome: doctor-diagnosed diabetes (DIQ010).

### Weighted Sample Characteristics

-   Weighted mean age: \~47.5 years.

-   Weighted diabetes prevalence: \~8.9%.

-   Design effect and effective sample size calculated.

### Variables & Coding

-   age_c: standardized age

-   bmi_c: standardized BMI

-   sex: Male/Female

-   race: 5 categories (NH White reference)

### Survey Design Variables

-   SDMVPSU – primary sampling units

-   SDMVSTRA – strata

-   WTMEC2YR – exam weights

### Final Analytic Sample

-   Cleaned dataset with survey design variables preserved.

-   Ready for complete-case analysis, MICE imputation, and Bayesian modeling.
:::

---

## Missingness

::: panel-tabset
### Missingness Overview

-   \~7% overall missingness.

-   92.7% of rows complete.

-   Minimal structural missingness.

### Key Missing Variables

-   BMI: \~4.3% missing

-   diabetes_dx: \~3.1% missing

### MAR Definition

-   Missing At Random (MAR): Missingness depends on other observed variables (e.g., exam attendance), not on the missing value itself.

### Missingness Visualization

![](figs/missingness.png){fig-align="center" width="85%"}

:::

---

## Implementing Design-Based Logistic Regression

::: panel-tabset

### Complete-Case Summary

-   Complete-case sample: n = 5,348.

-   Survey-weighted logistic regression performed using quasibinomial.

### Key Findings

-   Age (per SD ↑): \~3× higher odds of diabetes

-   BMI (per SD ↑): \~1.9× higher odds

-   Female: lower odds vs. Male

-   Certain racial/ethnic groups show higher odds vs NH White

### Survey-Weighted Coefficient Plot

![](figs/svy_or.png){fig-align="center" width="85%"}
:::

---

## Implementing Bayesian Logistic Regression

::: panel-tabset

### Data (Adults)

* Raw imported: **10,175 obs**

* Survey-weighted (filtered = adult): **5,769 (~7%)**

* Complete case analysis: **5,348** (with no missing values)

* **Statistics (Data = Adults)**

* **Bayesian models assume complete data.**

* Imputed dataset (adult_imp1): **5,592**

  * **(Raw data → MICE →** imputed dataset **)**

### MICE

**Multivariate Imputation by Chained Equations**

* **Iteratively imputes incomplete variables** using regression models.
* MICE runs **5 iterations for each of the 5 imputed datasets** to ensure stability and convergence and provides multiple realistic versions of missing data.
* Uses the **PMM method** for continuous variables **(Predictive Mean Matching)** to fill missing values with the observed values closest to predicted values.
* Uses **logistic regression for binary variables.**
* **Rubin’s rule finds pooled estimates** from across posterior distributions to yield the full posterior:
  **Posterior = parameters + missing data uncertainty**

### Bayesian Logistic Regression Model

Model is fitted to each imputed dataset in **R** using brms **(Stan backend)**, **2000 iterations**.

**Bayesian Regression Formula**

***diabetes_dx | weights(wt_norm) ~ age_c + bmi_c + sex + race***

**Dataset**: pooled imputed dataset 1
**Bernoulli family**
**Chains = 4, Iterations = 2000, Seed = 123**
**Priors:**

* **normal(0, 2.5)** for coefficients
* **student_t(3, 0, 10)** for the intercept

### Prior

* **Intercept prior: student_t(3, 0, 10)** — @VanDeSchoot2013

  * **df = 3 (degrees of freedom):** heavy tails — allow for occasional extreme values.
  * **location = 0:** centered at 0, so the prior expects the parameter to be near 0.
  * **scale = 10:** controls how spread out the distribution is (larger scale → wider spread).
* **Prior for regression coefficient: normal(0, 2.5)** — @VanDeSchoot2021

  * Weakly informative with **mean = 0, standard deviation = 2.5**.
  * Probability of very large or very small parameter values is low and **stabilizes the model**.

### Code

```r
library(gt)
priors <- c(
  set_prior("normal(0, 2.5)", class = "b"),
  set_prior("student_t(3, 0, 10)", class = "Intercept")
)

bayes_fit <- brm(
  formula = diabetes_dx | weights(wt_norm) ~ age_c + bmi_c + sex + race,
  data    = adult_imp1,
  family  = bernoulli(link = "logit"),
  prior   = priors,
  chains  = 4, iter = 2000, seed = 123,
  control = list(adapt_delta = 0.95),
  refresh = 0
)
```

* `brm()` in the R package **{brms}** is used to fit complex Bayesian models.

:::

---

## MCMC (Markov Chain Monte Carlo)

::: panel-tabset

### Markov Process

MCMC convergence is the point where information from the prior and the data (likelihood) has been fully combined, and the Gibbs sampler has reached the posterior distribution — the chain is now drawing valid samples from that posterior.

* **Gibbs sampler:** a specific algorithm under the MCMC category that produces a sequence of random vectors of parameters.
* **Burn-in period:** the **first 500–1,000 iterations** (early draws) that are **not representative** of the true posterior and are discarded.
* After the burn-in period, the chain reaches *stationarity*.

**Each sample drawn** depends on the previous sample — they are **not independent**, and random vectors are **sequentially dependent**.

### Code

```r
post <- posterior_summary(bayes_fit, robust = FALSE) %>% as_tibble(rownames = "term")

# Extract convergence diagnostics
diag <- rstan::monitor(as.array(bayes_fit$fit), print = FALSE) %>%
  as_tibble(rownames = "term") %>%
  select(term, Rhat, n_eff = Bulk_ESS)

bayes_results <- post %>%
  left_join(diag, by = "term") %>%
  mutate(
    OR     = exp(Estimate),
    OR_LCL = exp(Q2.5),
    OR_UCL = exp(Q97.5)
  ) %>%
  select(term, Estimate, Est.Error, Q2.5, Q97.5, Rhat, n_eff, OR, OR_LCL, OR_UCL)
```

* `rstan::monitor` in the **RStan** package computes summary statistics and diagnoses the convergence and efficiency of MCMC simulations.

  Provides summaries:

* **Posterior mean** and **standard deviation (SD)**

* **Monte Carlo standard error (MCSE)**

* **Quantiles:** 2.5%, 25%, 50%, 75%, 97.5%

* **Effective sample sizes:** Bulk_ESS and Tail_ESS

* **R-hat (convergence diagnostic)**

### Result

**warmup = 1000**
**total draws = 4000**

* **Bulk ESS** → reliability of mean and median estimates
* **Tail ESS** → reliability of extreme quantiles (e.g., 2.5%, 97.5%)

:::

## Model Diagnostics

::: panel-tabset

### MCMC Convergence

Good convergence shows:

* **No drift across iterations**
* **Well-mixed chains** without any trends and with stable posterior estimates
* **R-hat ≈ 1**
* Effective sample size: **~3500** — the MCMC chain has the same information as **3,500 independent samples**

  * Indicates **good mixing**, **low autocorrelation**, and **highly reliable** posterior estimates

### Trace Plots (Stable Posteriors)

![](figs/mcmc_trace.png){fig-align="center" width="85%"}

### Density Plot

![](figs/prior_only_density.png){fig-align="center" width="85%"}

### Autocorrelation Plot

![](figs/acf_age_bmi.png){fig-align="center" width="85%"}

:::

---

## Posterior Predictive Distribution

::: panel-tabset

### Posterior Draws

Posterior draws = **4000 draws** (4 chains × 1000 iterations) via MCMC, giving a posterior distribution of predictors.

* **Conditional distribution**
* **Full probability distribution** of the parameter given the data

  * **Mean and credible intervals** are summaries of that distribution, not the distribution itself
* **Influenced by the prior (prior subjectivity)**
* **Sample size effect:** prior influence decreases as sample size increases

### Plot

![](figs/ppc_density.png){fig-align="center" width="85%"}
* Y-axis: density (frequency of predicted probabilities)

* X-axis: predicted probability of diabetes (0–1)

* Most individuals have low predicted probabilities (near 0), with a smaller peak near 1.

The model closely reproduces the observed data and captures the distribution of diabetes outcomes accurately.

:::

---

## Regression Results (Posterior Estimates and Distribution)

::: panel-tabset

### Posterior Estimates

![](figs/bayes_fit_summary.png){fig-align="center" width="85%"}

### Posterior Distribution

![](figs/mcmc_areas.png){fig-align="center" width="85%"}

### Interpretation

**Posterior Coefficients**

* **Age (per 1 SD):** strong positive effect (≈ +1.1, 80% CrI ≈ 0.9–1.3)
* **BMI (per 1 SD):** positive association (≈ +0.6, 80% CrI ≈ 0.45–0.75)
* **Female:** slightly protective (≈ –0.2, CrI spans slightly below zero)
* **Mexican American:** small positive effect (≈ +0.25, wide CrI)
* **Other Hispanic:** moderate positive effect (≈ +0.35, wide CrI)
* **Non-Hispanic Black:** clear positive effect (≈ +0.75, CrI ≈ 0.6–0.9)
* **Other/Multi:** small positive effect (≈ +0.3, wide CrI)

:::

---

## Compare Prior and Posterior Predictive Distribution (Age and BMI)

::: panel-tabset

### Plot

![](figs/prior_vs_posterior.png){fig-align="center" width="85%"}

### Interpretation 

**Prior and posterior distributions** for the coefficient estimates of *Age (per 1 SD)* and *BMI (per 1 SD)*:

* **Prior distribution:** wide spread (high variance), indicating a high degree of initial uncertainty
* **Posterior distribution:** narrower and taller
* **Data significantly updated the initial beliefs**

:::

---

## Posterior Predictive Checks

::: panel-tabset

### Proportion of Diabetes

We draw **500 samples** of predicted outcomes for each observation and calculate the proportion of outcomes (Diabetes = 1).

![](figs/ppc_bars.png){fig-align="center" width="85%"}

**Fig:** Bar plot comparing counts/frequencies of each category (0 vs 1) in the observed data (`y`) and predictive samples (`y-rep`).

**Bars for observed data fall within the range of simulated data:** the **model predicts the overall diabetes prevalence well**.

### Posterior Distribution of Diabetes

![](figs/posterior_prevalence_hist.png){fig-align="center" width="85%"}

Histogram shows the posterior distribution of the population proportion of diabetes.
**x-axis:** proportion of diabetics (1s)
**y-axis:** frequency of posterior draws

* Highest bar ≈ 0.11 (11%), **symmetric**, centered at ~0.11
* Spread: **95% credible interval ≈ 9%–13%** diabetes prevalence

### Outcome Mean

![](figs/ppc_mean.png){fig-align="center" width="85%"}

### Outcome Standard Deviation 

![](figs/ppc_sd.png){fig-align="center" width="85%"}

:::

---

## Bayesian vs Survey-Weighted NHANES (Diabetes Prevalence)

![](figs/pop_vs_post_prev.png){fig-align="center" width="85%"}

![](figs/pop_vs_post_prev.png){fig-align="center" width="85%"}

* Model mean diabetes prevalence = **10.95%** (95% CI: **8.5%–12.8%**)
* NHANES diabetes prevalence = **8.9%** (SE = 0.0048)

**Slightly higher model mean**, but the credible interval overlaps with NHANES, indicating **reasonable calibration**.

**NHANES lies near the lower end of the posterior distribution but still within a plausible range — the model is well-calibrated.**

---

## Observed vs Predicted in Individuals

::: panel-tabset

### Average Diabetes

![](figs/ppc_scatter_avg.png){fig-align="center" width="center"}

* Each point = one individual
* **x-axis = observed value** (0 or 1)
* **y-axis = average predicted posterior probability** for that individual across simulated datasets
* **Points near (0, 0) or (1, 1) indicate good prediction**

### Mean BMI

* Each point = individual’s observed BMI vs. model-predicted mean
* **Error bars = 95% credible intervals**

**Model predictions generally align with the observed data.**

:::

---

## Compare Models

::: panel-tabset

### Overview

(1) **Survey-weighted maximum likelihood estimation (MLE)**
(2) **Bayesian regression**

```r
svy_tbl   <- svy_or   %>% mutate(Model = "Survey-weighted MLE")
bayes_tbl <- bayes_or %>% mutate(Model = "Bayesian")
```

![](figs/odds_ratio_comparison.png){fig-align="center" width="85%"}

* Both models consistently identified Age and BMI as strong predictors of diabetes.

  * **Bayesian model:** complements the frequentist approach by providing stable, interpretable estimates, quantifying uncertainty, and producing population-level prevalence.
  * **Survey-weighted model:** used normalized weights but does not fully account for stratification, clustering, or design-based variance adjustments.

### Interpretation

* **Minimal shrinkage and similar uncertainty** → reliable and robust model
* **Posterior estimates are slightly pulled toward the prior mean** (often toward zero or more moderate values) compared to classical estimates
* **The prior was weakly informative, exerting only a small influence**
* **Observed data were strong enough to dominate the prior** → Bayesian results remain very similar to classical (survey-weighted)

:::

---

## Pairwise Correlation

::: panel-tabset

### Plot

![](figs/mcmc_pairs.png){fig-align="center" width="85%"}

### Interpretation

* No strong linear relationships between **Age, BMI, and Sex**; each independently contributes to predicting diabetes.
* **Diagonal histograms** are smooth and unimodal, confirming **stable convergence** and well-behaved posterior samples.
* Mild negative correlation between **Age** and **Sex (Female):** as the effect of Age increases, the effect of being female slightly decreases.

**Weak correlations indicate both predictors provide distinct information for the diabetes outcome.**

:::

---

## R Square

![](figs/bayes_R2.png){fig-align="center" width="85%"}

**Interpretation**

* Approximately **13%** of the variability in diabetes status is explained, with credible uncertainty bounds indicating modest explanatory power.
* Predictors (Age, BMI, Sex, etc.) capture some—but not all—factors influencing diabetes risk.
* Other factors (genetics, lifestyle, environment) likely contribute as well.

---

## Assumptions for Bayesian Logistic Regression

* Binary outcome
* Independent observations
* Linear relationship in the logit
* No perfect collinearity
* Priors properly chosen and informative enough
* Posterior is proper and convergent
* No complete separation
* Good predictive checks

---

## Translational Research

::: panel-tabset

### Internal Validation

**Personalized Risk Estimation**

**Threshold > 30%**
**BMI is modifiable** — interventions can lower risk (Sex & Race = constant).
Used **posterior_linpred(transform = TRUE)**.

![](figs/posterior_density_participant1.png){fig-align="center" width="85%"}

**Median predicted probability ≈ 0.25**, 95% CrI = **0.20–0.31** (~1 in 4 chance of diabetes).

### Participant Data

```r
participant1_data <- adult[1, ]
phat1 <- posterior_linpred(bayes_fit, newdata = participant1_data, transform = TRUE)

post_pred_df <- data.frame(pred = phat1)
ci_95_participant1 <- quantile(phat1, c(0.025, 0.975))

ggplot(post_pred_df, aes(x = pred)) +
  geom_density(color='darkblue', fill='lightblue') +
  geom_vline(xintercept = ci_95_participant1[1], color='red', linetype='dashed') +
  geom_vline(xintercept = ci_95_participant1[2], color='red', linetype='dashed') +
  xlab('Probability of being diabetic (Outcome = 1)') +
  ggtitle('Posterior Predictive Distribution 95% Credible Interval') +
  theme_bw()
```

```r
new_participant <- data.frame(
  age_c = 40,
  bmi_c = 25,
  sex = "Female",
  race = "Mexican American"
)
```

### External Validation

Computed the posterior predicted probability with 95% credible intervals for:
**age_c = 40, bmi_c = 25, sex = "Female", race = "Mexican American"**

![](figs/posterior_pred_dist.png){fig-align="center" width="85%"}

**X-axis:** predicted probability of diabetes (Outcome = 1)
**Y-axis:** density of predicted probabilities
**Blue curve:** posterior predictive distribution
**Shaded area:** high probability values near 1 indicate increased diabetes risk
**Red dashed line:** upper bound of the 95% credible interval

**Peak predicted probability: ≈ 1**

:::

---

## Reverse Prediction

::: panel-tabset

### Predicting BMI

Predicting BMI value for a given individual (diabetic, age = 40, female, Mexican American).
**logit(Diabetes**ᵢ**) = β₀ + β_Ageᵢ + β_BMIᵢ + β_Sexᵢ + β_Raceᵢ**

![](figs/inverse_prediction_bmi.png){fig-align="center" width="85%"}

**X-axis:** BMI values (centered or raw, depending on the model)
**Y-axis:** predicted probability of diabetes for a representative individual
**Blue curve:** predicted probability of diabetes across the BMI range; consistently high (≈1)
**Red horizontal line:** target probability of diabetes = 0.3
**Red vertical line:** targeted BMI where predicted probability is closest to the target (≈18)

### Practical Implications

“**At what BMI does this individual have a 30% chance of being diabetic?**”

*(Diabetic, age = 40, female, Mexican American)*
**This is inverse prediction — the BMI needed to reach a 30% diabetes risk.**

* For this demographic profile, even a relatively low BMI (~18) is associated with a 30% diabetes risk.
* Indicates that other factors (age, sex, race) contribute strongly to predicted diabetes risk.

:::

---

## Limitations

* **Cross-sectional** NHANES design limits causal inference.
* Multiple **imputation** assumes MAR; MNAR may bias results.
* Residual **confounding** from unmeasured factors (diet, activity, SES, genetics).
* Bayesian estimates may be influenced by **prior choices**.
* **BMI measured once**; may not capture long-term exposure.
* **Self-reported** variables may introduce recall or reporting bias.
* **Interaction effects** (e.g., age × BMI) not tested.

---

## Conclusion

Bayesian logistic regression effectively models uncertainty.
MICE improved data completeness and reliability.
Posterior predictions provide interpretable probabilities of diabetes risk.
Framework is adaptable to other outcomes (e.g., hypertension, obesity).

---

## Thank You

**Advisor**  
Dr. Achraf Cohen  

**Presented by**  
Namita Mishra  
Autumn S. Wilcox  

**University of West Florida**

### Questions?
